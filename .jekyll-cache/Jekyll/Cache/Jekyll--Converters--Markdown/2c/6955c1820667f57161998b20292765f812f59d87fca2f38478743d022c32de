I"€6<h1 id="n-step-methods">N-step Methods</h1>
<ul>
  <li>N-step Methods : Further our understanding of TD methods</li>
  <li>we know so farï¼š TD(0)</li>
  <li>we will learn :
    <ul>
      <li>Î» = 0 gives us TD(0), Î» = 1 gives us monte Carlo</li>
    </ul>
  </li>
  <li>any other Î» is a trade-off(åè°ƒ) between the two</li>
</ul>

<p><a href="https://postimg.cc/n9hf3sZq"><img src="https://i.postimg.cc/gjvckRTt/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>
<ul>
  <li>1-Step TDì˜ stepì„ ì¦ê°€ì‹œì¼œ ë‚˜ê°€ë©´ì„œ nê¹Œì§€ ë³´ê²Œ ëœë‹¤ë©´ n-step TDë¡œ ì¼ë°˜í™” í•  ìˆ˜ ìˆë‹¤.</li>
  <li>ë§Œì•½ stepì´ ë¬´í•œëŒ€ì— ê°€ê¹ê²Œ ë˜ë©´ MCì™€ ë™ì¼í•˜ê²Œ ë  ê²ƒì´ë‹¤.</li>
  <li>2-Step TDì—ì„œ ì—…ë°ì´íŠ¸ ë°©ì‹ì€ ì²™ë²ˆì§¸ ë³´ìƒê³¼ ë‘ë²ˆì§¸ ë³´ìƒ ê·¸ë¦¬ê³  ë¶€ì „ì§¸ ìƒíƒœì—ì„œì˜ Value functionì˜ í•©ìœ¼ë¡œ ì—…ë°ì´íŠ¸ê°€ ëœë‹¤.</li>
</ul>

<p><a href="https://postimg.cc/62Yp6xpL"><img src="https://i.postimg.cc/J0WDq1wg/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li>n step TDì—ì„œì˜ Value í•¨ìˆ˜ëŠ” N-step ì—ì„œ ì–»ì€ ì´ ë³´ìƒì—ì„œ ê¸°ì¡´ Value í•¨ìˆ˜ê°’ê³¼ ì°¨ì´ë¥¼ ì•ŒíŒŒë§Œí¼ ê°€ì¤‘ì¹˜í•˜ì—¬ ë”í•¨ìœ¼ë¡œì„œ ì—…ë°ì´íŠ¸ê°€ ë˜ê²Œ ë©ë‹ˆë‹¤.</li>
</ul>

<p><a href="https://postimg.cc/RJtC76TG"><img src="https://i.postimg.cc/prG9tK5N/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li>n ì´ ëª‡ì¼ë•Œê°€ ê°€ì¥ ìµœê³ ì˜ ê²°ê³¼ê°’ëŠ˜ ë‚˜íƒ€ë‚¼ê¹Œ? ìœ„ì— ê·¸ë˜í”„ê°€ ì‹¤í—˜ì— ëŒ€í•œ ê²°ê³¼ ê°’ì…ë‹ˆë‹¤.</li>
  <li>ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸í•˜ëŠ” ì˜¨ë¼ì¸ ë°©ì‹, ì—í”¼ì†Œë“œ ì™„ë£Œí›„ ì—…ë°ì´íŠ¸í•˜ëŠ” ì˜¤í”„ë¼ì¸ ë°©ì‹ì— ëŒ€í•œ ê²°ê³¼ëŠ” ë¹„ìŠ·í•˜ê²Œ ë‚˜ì˜¨ë‹¤.</li>
  <li>nì´ ì»¤ì§ˆìˆ˜ë¡ ì—ëŸ¬ê°€ ì»¤ì§€ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.</li>
  <li>3~5 step ì´ ì¢‹ì€ê±° ê°™ë‹¤.</li>
</ul>

<p><a href="https://postimg.cc/8J2fgVKK"><img src="https://i.postimg.cc/MHT7R6yS/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li>n-step ì—ì„œ ë³´ìƒê°’ì„ ë‹¤ë¥¸ ê°’ìœ¼ë¡œ í‰ê· ì„ ë‚¼ ìˆ˜ ìˆë‹¤.</li>
  <li>í•˜ì§€ë§Œ 2~4 stepì—ì„œì˜ í‰ê· ì„ ë‚´ì–´ë³´ë©´ ìœ„ì™€ ê°™ì€ ì‹ì´ ë  ê²ƒì´ë‹¤.</li>
  <li>ì´ ë‘ê°€ì§€ë¥¼ ê²°í•©í•˜ì—¬ íš¨ìœ¨ì ìœ¼ë¡œ ë§Œë“¤ìˆ˜ ìˆì„ê¹Œ?</li>
</ul>

<p><a href="https://postimg.cc/WFwhDfsS"><img src="https://i.postimg.cc/FR2cB28t/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li>ë‹µì€ ê°€ëŠ¥í•˜ë‹¤</li>
  <li>ì—¬ê¸°ì„œ ëŒë‹¤ ë³´ìƒì€ ëª¨ë“  n-step ê¹Œì§€ì˜ ê°€ì¤‘í‰ê· ëœ ë³´ìƒì´ë‹¤.</li>
  <li>ê¸°ì¡´ì— n-stepì—ì„œ ì‚¬ìš©í•œ ë³´ìƒì€ ì´í•©ì´ì˜€ë‹¤.</li>
  <li>ì´ë ‡ê²Œ í‰ê· ì„ ì´ìš©í•˜ëŠ” ë°©ì‹ì€ ì˜¤ë¥˜ë¥¼ ë” ë‚®ì¶œ ìˆ˜ ìˆë‹¤.</li>
  <li>ëŒë‹¤ì˜ ì´í•©ì´ 1 ë˜ë„ë¡ í•˜ê¸° ìœ„í•´ (1-lamda)ê³„ìˆ˜ë¡œ ë…¸ë©€ë¼ì´ì¦ˆë¥¼ í•˜ì—¬ 0ë¶€í„° 1ê¹Œì§€ì˜ ê°’ì„ ê°–ë„ë¡ í•©ë‹ˆë‹¤.</li>
  <li><strong>ëŒë‹¤ëŠ” n-stepì´ ì»¤ì§ˆ ìˆ˜ë¡ ë³´ìƒì— ëŒ€í•œ ê°’ì„ ê°ì†Œì‹œí‚¤ê²Œ í•©ë‹ˆë‹¤.</strong></li>
  <li>ë§ˆì§€ë§‰ì— ê³µì‹ì´ TD-Lamda ì˜ Value í•¨ìˆ˜ì…ë‹ˆë‹¤.</li>
</ul>

<p><a href="https://postimg.cc/HcVqsMx6"><img src="https://i.postimg.cc/KzNxfryh/4564.png" width="700px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li>Step ì‹œê°„ì´ íë¥¼ìˆ˜ë¡ weightê°€ ì§€ìˆ˜í˜•íƒœë¡œ ê°ì†Œí•˜ëŠ” ê²ƒì„ ë³¼ìˆ˜ ìˆë‹¤.</li>
  <li>ê·¸ë¦¬ê³  ì´ë“¤ì˜ ì´í•©ì€ 1ì´ ëœë‹¤.</li>
  <li><strong>ì§€ìˆ˜í˜•íƒœì˜ ê°€ì¤‘ì¹˜</strong> ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì•Œê³ ë¦¬ì¦˜ ì—°ìƒì— íš¨ìœ¨ì„±ì„ ì£¼ê³  ë©”ëª¨ë¦¬ë¥¼ ëœ ì‚¬ìš©í•˜ê²Œ ë˜ëŠ” ì´ì ì´ ìˆë‹¤.</li>
</ul>

<p><a href="https://postimg.cc/t1Xp3DRJ"><img src="https://i.postimg.cc/1zFXZYLG/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li>Value í•¨ìˆ˜ê°€ ì—…ë°ì´íŠ¸ í•˜ëŠ” ê³¼ì •</li>
  <li>ë™ì¼í•˜ê²Œ n-stepê¹Œì§€ ë„ë‹¿ë¼ê³  ë‚˜ì„œ ì–»ì€ ë³´ìƒë“¤ì„ lamdaë¥¼ ì´ìš©í•˜ì—¬ ë³´ìƒê°’ì„ ì—…ë°ì´íŠ¸ í•˜ê²Œ ëœë‹¤.</li>
  <li>ì •ë°©í–¥ì˜ ê´€ì ì—ì„œ ë³´ë©´ ì´ë¡ ì ‘ì´ë‹¤.</li>
  <li>ë°˜ëŒ€ë°©í–¥ ê´€ì ì—ì„œ ë³´ë©´ ì»´í“¨í„° ì ì´ë‹¤.</li>
  <li>ë°˜ëŒ€ ë°©í–¥ìœ¼ë¡œ ë³´ë©´ TD(Lamda) ì•Œê³ ë¦¬ì¦˜ì€ ì˜¨ë¼ì¸ ë°©ì‹ê³¼ ê°™ì´ ë§¤ stepë§ˆë‹¤ ì—…ë°ì´íŠ¸ê°€ ëœë‹¤.</li>
</ul>

<p><a href="https://postimg.cc/HJJTxn6s"><img src="https://i.postimg.cc/5tgysFvv/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li>ìœ„ì—ì„œ ë²ˆê°œê°€ ë°œìƒí•œ ì´ìœ ëŠ” ìì£¼ ë°œìƒí•œ ê²ƒì´ ì˜í–¥ì´ í°ì§€ ìµœê·¼ì— ë°œìƒí•œ ê²ƒì´ ì˜í–¥ì´ í°ì§€ë¥¼ ê²°í•©í•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.</li>
  <li>í•˜ë‚˜ì˜ íŠ¹ì •í•œ stateë¥¼ ë°©ë¬¸í•˜ëŠ” íšŸìˆ˜ì— ë”°ë¼ì„œ Eligibility Tracesí•´ë³´ë©´ ìœ„ì—ì™€ ê°™ì´ ë‚˜ì˜¨ë‹¤.</li>
</ul>

<p><a href="https://postimg.cc/HVRqzmHV"><img src="https://i.postimg.cc/pLLP2V6Y/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li>ì´ë¥¼ ì ìš©í•´ë³´ë©´ ê° state ë§ˆë‹¤ ì—…ë°ì´íŠ¸ê°€ ë°œìƒ ë ë•Œ TDì—ëŸ¬ì˜ ë¹„ìœ¨ ë§Œí¼ ì—…ë°ì´íŠ¸ë¥¼ ê°€ì¤‘ì ìš©í•˜ëŠ” í•œë‹¤.</li>
  <li>ì´ê²ƒì€ ì—í”¼ì†Œë“œì˜ ê¸¸ì´ë³´ë‹¤ ì§§ì€ ê¸°ì–µì„ í•˜ëŠ” ë‹¨ê¸° ë©”ëª¨ë¦¬ ê°™ì€ ì—­í• ì„ í•œë‹¤.</li>
</ul>

<p><a href="https://postimg.cc/xkKfyj4P"><img src="https://i.postimg.cc/pdSmvpQ3/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li>lamdaëŠ” ì–¼ë§ˆë‚˜ ë¹¨ë¦¬ ê°’ì„ ê°ì†Œì‹œí‚¤ëŠ”ê°€ë¥¼ ì˜ë¯¸í•œë‹¤.</li>
  <li>lamdaì˜ ê°’ì´ 0ì´ ë˜ë©´ ì™„ì „ ê°€íŒŒë¥´ê²Œ decayê°€ ë°œìƒí•  ê²ƒì´ë‹¤.</li>
  <li>ê²°êµ­ í˜„ì¬ stateì˜ value í•¨ìˆ˜ë§Œ ì—…ë°ì´íŠ¸ê°€ ë˜ë©° ë‹ˆëŠ” TD(0)ê°€ ë™ì¼í•œ ë°©ì‹ì´ ëœë‹¤.</li>
</ul>

<p><a href="https://postimg.cc/239WMT0B"><img src="https://i.postimg.cc/wTgktGJF/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li>ë°˜ëŒ€ë¡œ lamdaê°€ 1ì´ ë˜ë©´ ì—í”¼ì†Œë“œë¥¼ ëª¨ë‘ ì»¤ë²„í•˜ê²Œ ëœë‹¤.</li>
  <li>MCì™€ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ì˜¤í”„ë¼ì¸ ì—…ë°ì´íŠ¸ê°€ ëœë‹¤.</li>
</ul>

<p><a href="https://postimg.cc/t1XPLgVB"><img src="https://i.postimg.cc/GpDQT83n/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li>value function can be described recursively(bellmanâ€™s equation)</li>
</ul>

<p><a href="https://postimg.cc/tY4gQVrZ"><img src="https://i.postimg.cc/MKyfB7VD/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li>we can estimate V by taking the sample mean of Gâ€™s</li>
</ul>

<p><a href="https://postimg.cc/CR0mxSqq"><img src="https://i.postimg.cc/kg6Zw5Mf/Capture.png" width="400px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li>TD(0) is to combine these 2 things to estimate G itself(all we know for sure is r)</li>
</ul>

<p><a href="https://postimg.cc/K10dCJ9w"><img src="https://i.postimg.cc/XvW4GP9j/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li>All we know for sure is R</li>
  <li>itâ€™s plausible(æœ‰é“ç†çš„) to ask: what if we use more râ€™s and less of V?</li>
</ul>

<p><a href="https://postimg.cc/3dC0TvQ2"><img src="https://i.postimg.cc/3JzX2XLf/Capture.png" width="500px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li>Monte Carlo is just the extreme of this, where we donâ€™t use V at all</li>
</ul>

<p><a href="https://postimg.cc/WFRL7SbZ"><img src="https://i.postimg.cc/vBTYbSJ0/Capture.png" width="500px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li>Letâ€™s superscript G to tell us how many Râ€™s we are using</li>
</ul>

<p><a href="https://postimg.cc/kB92MvCf"><img src="https://i.postimg.cc/7YJ0VKW8/Capture.png" width="500px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li>gives us a discrete transition from TD(0) to Monte Carlo</li>
  <li><strong>TD(0) - wait 1 step to update V</strong></li>
  <li>Monte Carlo - Wait until end of episode to update V</li>
  <li>N-step -wait N step to update V
    <ul>
      <li>why? we need to collect all the Râ€™s</li>
    </ul>
  </li>
</ul>

<p><a href="https://postimg.cc/6TpNwM19"><img src="https://i.postimg.cc/SQWQ656z/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li>For the actual update(which is in terms of G already). no change is needed</li>
  <li>the only change is to G itself</li>
</ul>

<p><a href="https://postimg.cc/Y4WkTR8p"><img src="https://i.postimg.cc/g0MJXBZ6/Capture.png" width="500px" title="source: imgur.com" /><a></a></a></p>

<h2 id="control">Control</h2>
<ul>
  <li>Use Q instead of V</li>
  <li>Ex. SARSA</li>
</ul>

<p><a href="https://postimg.cc/23r42K87"><img src="https://i.postimg.cc/HW7BWqqR/Capture.png" width="500px" title="source: imgur.com" /><a></a></a></p>

<blockquote>
  <p>Code</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># https://deeplearningcourses.com/c/deep-reinforcement-learning-in-python
# https://www.udemy.com/deep-reinforcement-learning-in-python
</span><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">print_function</span><span class="p">,</span> <span class="n">division</span>
<span class="kn">from</span> <span class="nn">builtins</span> <span class="kn">import</span> <span class="nb">range</span>
<span class="c1"># Note: you may need to update your version of future
# sudo pip install -U future
#
# Note: gym changed from version 0.7.3 to 0.8.0
# MountainCar episode length is capped at 200 in later versions.
# This means your agent can't learn as much in the earlier episodes
# since they are no longer as long.   
#
# Adapt Q-Learning script to use N-step method instead
</span>
<span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">gym</span> <span class="kn">import</span> <span class="n">wrappers</span>
<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span>

<span class="c1"># code we already wrote
</span><span class="kn">import</span> <span class="nn">q_learning</span>
<span class="kn">from</span> <span class="nn">q_learning</span> <span class="kn">import</span> <span class="n">plot_cost_to_go</span><span class="p">,</span> <span class="n">FeatureTransformer</span><span class="p">,</span> <span class="n">Model</span><span class="p">,</span> <span class="n">plot_running_avg</span>

</code></pre></div></div>

<blockquote>
  <p>SGDRegressor</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SGDRegressor</span><span class="p">:</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="mf">1e-2</span>

  <span class="k">def</span> <span class="nf">partial_fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
      <span class="n">D</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">D</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">D</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr</span><span class="o">*</span><span class="p">(</span><span class="n">Y</span> <span class="o">-</span> <span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">))</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">)</span>
</code></pre></div></div>
<p>Reference:</p>

<p><a href="https://www.udemy.com/course/artificial-intelligence-reinforcement-learning-in-python/">Artificial Intelligence Reinforcement Learning</a></p>

<p><a href="https://www.udemy.com/deep-reinforcement-learning-in-python/">Advance AI : Deep-Reinforcement Learning</a></p>

<p><a href="https://www.udemy.com/cutting-edge-artificial-intelligence/learn/lecture/14650508#overview">Cutting-Edge Deep-Reinforcement Learning</a></p>
:ET