I"‡%<h2 id="1-what-is-reinforcement-learning">1. What is Reinforcement Learning?</h2>

<p><a href="https://postimg.cc/qNRLDkZb"><img src="https://i.postimg.cc/130jtX0Q/14512412123124123.png" width="700px" title="source: imgur.com" /></a></p>

<ul>
  <li>Humans/ AIs alike never sense the entire world/universe at once</li>
  <li>We have sensors which feed signals to our brain from the environment</li>
  <li>We donâ€™t even know everything thatâ€™s going on in a room</li>
  <li>Thus the Sensors limit the amount of information we get</li>
  <li>The measurements we get from these sensors(e.g. sight, sound touch) make up a â€œstateâ€</li>
  <li>weâ€™ll only discuss finite state spaces</li>
  <li>state spaces with an infinite number of states are possible too</li>
</ul>

<p><a href="https://postimg.cc/y3jG9Lkk"><img src="https://i.postimg.cc/hhRRkwnb/444444.png" width="700px" title="source: imgur.com" /></a></p>

<ul>
  <li>whatâ€™s the # of states?
    <ul>
      <li>if we simplify the problem so that we can keep adding xâ€™s and oâ€™s even after a player gets 3 in a row</li>
      <li>Each Location on the board has 3 possibilities: empty, X, O</li>
      <li>9 Locations on the Board</li>
      <li>
        <h1 id="states--3-x-3--x3---39-">states = 3 x 3 â€¦ x3 = $ 3^9 $</h1>
      </li>
    </ul>
  </li>
</ul>

<h2 id="1-supervisedunsupervised-interfaces">1. Supervised/Unsupervised Interfaces</h2>
<ul>
  <li>as we know about ML Theory and Function
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SupervisedModel</span><span class="p">:</span>
<span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
<span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
</code></pre></div>    </div>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">UnsupervisedModel</span><span class="p">:</span>
<span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="n">x</span><span class="p">):</span> <span class="c1">#(e.g cluster assignment)
</span></code></pre></div>    </div>
  </li>
  <li>common theme is â€œtraining dataâ€</li>
  <li>input data: X(N x D Matrix)</li>
  <li>Tragets: Y (N x 1 vector)</li>
  <li>â€œall data is the sameâ€</li>
  <li>Format doesnâ€™t change whether youâ€™re in biology, finance, economics, etc.</li>
  <li>fits neatly into one library: scikit-Learn</li>
  <li>â€œsimplisticâ€, but still useful:
    <ul>
      <li>Face Detection</li>
      <li>Speech Recognition
<script async="" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script></li>
    </ul>
  </li>
</ul>
<div align="center" style="margin: 1em 0;">
<ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-5150894678574694" data-ad-slot="9221331439" data-ad-format="auto" data-full-width-responsive="true"></ins>
     </div>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

<h2 id="2-reinforcement-learning">2. Reinforcement Learning</h2>
<p><a href="https://postimg.cc/0r96nzLv"><img src="https://i.postimg.cc/7P2gx795/12312413.png" width="700px" title="source: imgur.com" /></a></p>

<ul>
  <li>Not just a static table of data</li>
  <li>An agent interacts with the world(environment)</li>
  <li>Can be simulated or real(E.g Vacuum robot)</li>
  <li>Data comes from sensors
    <ul>
      <li>Cameras, Microphones, GPS, accelerometer</li>
    </ul>
  </li>
  <li>Continuous stream of data
    <ul>
      <li>Consider past and future</li>
    </ul>
  </li>
  <li>RL agent is â€œthingâ€ with a lifetime
    <ul>
      <li>At each step, decide what to do</li>
    </ul>
  </li>
  <li>An (un)supervised model is just a static object -&gt; input -&gt; output</li>
</ul>

<h2 id="3-isnt-it-still-supervised-learning">3. isnâ€™t it still supervised learning?</h2>
<ul>
  <li>X can represent the state iâ€™m in, Y represent the Target (ideal action to perform in that state)
    <ul>
      <li>state = sensor recording from self-driving car</li>
      <li>state = Video Game ScreenShot</li>
      <li>state = Chess Board Positions</li>
    </ul>
  </li>
  <li>Yes it is, but for example, consider GO: $ N = 8 x 10^100 $</li>
  <li>ImageNet, the image Classification benchmark, has $ N=10^6 $ images
    <ul>
      <li>Go is 94 orders of magnitude larger</li>
      <li>Takes ~1 day with good hardware</li>
    </ul>
  </li>
  <li>1 order of magnitude Larger -&gt; 10 days</li>
  <li>2 order of magnitude Larger -&gt; 100 days</li>
</ul>

<h2 id="4-rewards">4. Rewards</h2>
<p><a href="https://postimg.cc/8JD4k1sS"><img src="https://i.postimg.cc/W3qHnzvh/123231.png" width="700px" title="source: imgur.com" /></a></p>

<ul>
  <li>sometimes youâ€™ll see reference to psychology; RL has been used to model animal behavior</li>
  <li>RL agentâ€™s goal is in the future
    <ul>
      <li>in contrast, a supervised model simply tried to get good accuracy / minimized cost on current input</li>
    </ul>
  </li>
  <li>Feedback Signals(Rewards) come from the environment (i.e the agent <em>experiences</em> them)</li>
</ul>

<h2 id="5-rewards-vs-targets">5. Rewards vs Targets</h2>
<p><a href="https://postimg.cc/1f2p5pzS"><img src="https://i.postimg.cc/x1Tg55DX/14141314123.png" width="700px" title="source: imgur.com" /></a></p>
<ul>
  <li>you might think of supervised Targets/labels as something like rewards. but these handmade labels are coded by humans - they do not come from environment</li>
  <li>Supervised inputs/targets are just database tables</li>
  <li>Supervised models <strong>instantly</strong> know if it is wrong/right, because inputs + targets are provided simultaneously</li>
  <li>RL is dynamic - if an agent solves a maze, it only know its decisions were correct if it eventually solves the maze</li>
</ul>

<h2 id="6-on-unusual-or-unexpected-strategies-of-rl">6. On Unusual or Unexpected Strategies of RL</h2>
<p><a href="https://postimg.cc/F1cfg2rM"><img src="https://i.postimg.cc/59Kw1VZ2/125212425.png" width="700px" title="source: imgur.com" /></a></p>
<ul>
  <li>Goal of AlphaGO is to win Go, and the goal of a video game agent is high score/live as long as possible</li>
  <li>what is the goal of an animal/human?</li>
  <li>Evolutionary psychologists believe in the â€œselfish Geneâ€ Theory
    <ul>
      <li><em>Rechard Dawkins - The Selfish Gene</em></li>
    </ul>
  </li>
  <li>Genes simply want to make more of themselves</li>
  <li>We humans(conscious living beings) are totally unaware of this</li>
  <li>we canâ€™t ask our genes how they feel</li>
  <li>we are simply a vessel for our genesâ€™ proliferation(ê¸‰ì¦)</li>
  <li>is consciousness just an illusion?</li>
  <li>Disconnect between what we think we want vs â€œtrue goalâ€
<script async="" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script></li>
</ul>
<div align="center" style="margin: 1em 0;">
<ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-5150894678574694" data-ad-slot="9221331439" data-ad-format="auto" data-full-width-responsive="true"></ins>
     </div>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

<ul>
  <li>Like Alphago, weâ€™ve found roundabout and unlikely ways of achieving our Goal</li>
  <li>The action taken doesnâ€™t necessarily have to have an obvious / explicit relationship to the Goal</li>
  <li>we might desire riches/money -but why? Maybe natural selection or leads to better health and social status. there are no laws physics which govern riches and gene replication</li>
  <li>itâ€™s a novel solution to the problem</li>
  <li>AI can also find such strange or unusual ways to achieve a goal
<a href="https://postimg.cc/xkw6hWWw"><img src="https://i.postimg.cc/500DYJp2/2152314215234.png" width="700px" title="source: imgur.com" /></a></li>
  <li>we can replace â€œgetting richâ€ with any trait(íŠ¹ì„±) we want
    <ul>
      <li>being healthy and strong</li>
      <li>Having strong analytical skills</li>
    </ul>
  </li>
  <li>Thatâ€™s a sociologist(ì‚¬íšŒí•™ì)â€™s job</li>
  <li>Our interest lies in the fact that there are multiple novel strangies of achieving the same goal(gene replication)</li>
  <li>What is considered a good strategy can fluctuate</li>
  <li>Ex. Sugar:
    <ul>
      <li>our brain runs on sugar, it gives us energy</li>
      <li>today, it causes disease and death</li>
    </ul>
  </li>
  <li>Thus, a Strategy that seems good right now may not be globally optimal</li>
</ul>

<h2 id="7-speed-of-learning-and-adaption">7. Speed of Learning and Adaption</h2>
<ul>
  <li>Animals gain new traits via evolution/mutation/natural selection
    <ul>
      <li>this is slow</li>
      <li>each newborn, even given an advantageous trait, still must learn from scratch</li>
    </ul>
  </li>
  <li>AI can train via simulation
    <ul>
      <li>it can spawn new offspring instantly</li>
      <li>obtain hundreds / thousands of years of experience in the blink of an eye</li>
    </ul>
  </li>
</ul>

<p>Reference
<a href="https://www.udemy.com/cutting-edge-artificial-intelligence/learn/lecture/14650508#overview">Cutting-Edge Deep-Reinforcement Learning</a></p>
:ET