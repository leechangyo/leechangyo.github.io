I"Ô8<h1 id="approximation-methods">Approximation Methods</h1>
<ul>
  <li>Major Disadvantage of methods weâ€™ve learned</li>
  <li>V - Need to estimate ISI values</li>
  <li>Q - Need to estimate ISI x IAI values</li>
  <li>ISI and IAI can be very large</li>
  <li>solution : Approximation</li>
  <li>Recall: neural networks are universal function approximator</li>
  <li>First, we do feature extraction: convert state s to <strong>feature vector x</strong>
<a href="https://postimg.cc/VSsNgQMT"><img src="https://i.postimg.cc/N0mrgQ0g/414141412312.png" width="300px" title="source: imgur.com" /><a></a></a></li>
  <li>we want a function, parameterized by theta, that accurately approximates V
<a href="https://postimg.cc/yJBDGp3q"><img src="https://i.postimg.cc/MGf1vhWG/123131.png" width="300px" title="source: imgur.com" /><a></a></a></li>
</ul>

<h2 id="1-linear-approximation">1. Linear Approximation</h2>
<ul>
  <li>Function approximation methods requires us to use models that are differentiable</li>
  <li>Canâ€™t use something like decision tree or k-nearest neighbour</li>
  <li>in sequal(æŸ¥è¯¢è¯­è¨€) to this section, we will use deep learning methods, which are differentiable</li>
  <li>Neural networks donâ€™t require us to engineer features beforehand, but it can help</li>
  <li>FOr Approximation methods, we will need linear regression and gradient descent</li>
</ul>

<h2 id="2-section-outline">2. Section outline</h2>
<ul>
  <li>MC prediction</li>
  <li>TD(0) prediction</li>
  <li>SARSA</li>
</ul>

<h2 id="3-sanityæ˜æ™º-checking">3. Sanity(æ˜æ™º) Checking</h2>
<ul>
  <li>we can always sanity check our work by comparing to non-approximated versions</li>
  <li>we expect approximation to be close, but perhaps not exactly equal</li>
  <li>Obstacle we may encounter: our code is implemented perfectly, but our model is bad</li>
  <li>Linear models are not very expressive</li>
  <li>if we extract a poor set of features, model wonâ€™t learn value function well</li>
  <li>will need to put in manual work to do feature engineering</li>
</ul>

<h2 id="4-linear-model">4. Linear Model</h2>
<ul>
  <li>Supervised learning aka,<strong>Function Approximation</strong></li>
  <li>The function we are trying to approximate is V(s) or Q(s,a)</li>
  <li>Rewards are real numbers</li>
  <li>So returns, which are sums of rewards are real number</li>
  <li>2 Supervised learning techniques
    <ul>
      <li>Classification</li>
      <li>regression</li>
    </ul>
  </li>
  <li>we are doing regression</li>
</ul>

<h2 id="5-error">5. Error</h2>
<ul>
  <li>Supervised Learning methods have cost Functions</li>
  <li>Regression means squared error is the appropriate choice</li>
  <li>Squared difference between V and estimate of V
<a href="https://postimg.cc/m1G258rz"><img src="https://i.postimg.cc/Gt489fwQ/121231231231.png" width="300px" title="source: imgur.com" /><a></a></a></li>
  <li>Replace V with its definition</li>
  <li>But we donâ€™t know this expected value
<a href="https://postimg.cc/s1rvggVY"><img src="https://i.postimg.cc/9X4T1DKn/124123124123.png" width="300px" title="source: imgur.com" /><a></a></a></li>
  <li>as we learned from MC(Monte Carlo), we can replace the expected value with its sample means
<a href="https://postimg.cc/0MrZ60G4"><img src="https://i.postimg.cc/DZPNMCS7/433.png" width="300px" title="source: imgur.com" /><a></a></a></li>
  <li>Alternatively, we can treat each G_(i,s) as a training sample</li>
  <li>And try to minimize all the <strong>individual squared differences</strong> simultaneously(just like linear regression)
<a href="https://postimg.cc/m1kDWtsC"><img src="https://i.postimg.cc/4ybht92L/41241312321.png" width="300px" title="source: imgur.com" /><a>
<a href="https://postimg.cc/XZKcpDyx"><img src="https://i.postimg.cc/vmkP2J8R/412313124123.png" width="300px" title="source: imgur.com" /><a></a></a></a></a></li>
</ul>

<h2 id="6-stochastic-gradient-descent">6. stochastic Gradient Descent</h2>
<ul>
  <li>we can now do stochastic(éšæœºçš„) gradient decent</li>
  <li>Move in direction of gradient of error wrt only one sample at a time</li>
</ul>

<h3 id="gradient-descent-ì„¤ëª…">Gradient Descent ì„¤ëª…</h3>
<ul>
  <li>Neural Networkì˜ loss functionì˜ í˜„ weightì˜ ê¸°ìš¸ê¸°(gradient)ë¥¼ êµ¬í•˜ê³  Lossë¥¼ ì¤„ì´ëŠ” ë°©í–¥ìœ¼ë¡œ ì—…ë°ì´íŠ¸ ë‚˜ê°€ëŠ” ë°©ë²•
<a href="https://postimg.cc/XGVhJ5XG"><img src="https://i.postimg.cc/0QmqVdsf/124141312312.png" width="500px" title="source: imgur.com" /><a></a></a></li>
  <li>Loss function (ERROR) ëŠ” í˜„ì¬ ê°€ì¤‘ì¹˜(weight)ì—ì„œ í‹€ë¦° ì •ë„ë¥¼ ì•Œë ¤ì£¼ëŠ” í•¨ìˆ˜ì´ë‹¤.</li>
  <li>ì¦‰, í˜„ì¬ ë„¤íŠ¸ì›Œí¬ì˜ weightì—ì„œ ë‚´ê°€ ê°€ì§„ ë°ì´í„°ë¥¼ ì§‘ì–´ ë„£ìœ¼ë©´ ì—ëŸ¬ê°€ ìƒê¸¸ ê²ƒì´ë‹¤. ê±°ê¸°ì— ë¯¸ë¶„ì„ í•˜ë©´ ì—ëŸ¬ë¥¼ ì¤„ì´ëŠ” ë°©í–¥ì„ ì•Œ ìˆ˜ ìˆë‹¤.</li>
  <li>ê·¸ ë°©í–¥ìœ¼ë¡œ ì •í•´ì§„ Learning rateë¥¼ í†µí•´ì„œ weightë¥¼ ì´ë™ì‹œí‚¨ë‹¤. ì´ê²ƒì„ ë°˜ë³µí•´ì„œ í•™ìŠµí•˜ëŠ” ê²ƒì´ë‹¤.</li>
  <li><strong>ì¦‰ Gradient DescentëŠ” í˜„ì¬ ë„¤íŠ¸ì›Œí¬ì˜ Weightì— ì–´ë–¤ ë°ì´í„°ë¥¼ ë„£ì„ ë–„ ì—ëŸ¬ê°’ì„ ë¯¸ë¶„í•´ì„œ ì—ëŸ¬ì˜ ë°©í–¥(Gradient)ì„ ì•Œì• ì²´ê³  ê·¸ ë°©í–¥ìœ¼ë¡œ ì •í•´ì§„ Learning rateë¥¼ í†µí•´ì„œ weightì„ ì´ë™ì‹œí‚¨ë‹¤(Descent).</strong>
<a href="https://postimg.cc/kR47V4fs"><img src="https://i.postimg.cc/fy7yrSdG/141412412312312.png" width="500px" title="source: imgur.com" /><a></a></a></li>
  <li>Gradient Descentì˜ ë‹¨ì ì€ ê°€ì§„ ë°ì´í„°ë¥¼ ë‹¤ ë„£ìœ¼ë©´ ì „ì²´ ì—ëŸ¬ê°€ ê³„ì‚°ì´ ë  ê²ƒì´ë‹¤.</li>
  <li>ê·¸ë ‡ê¸° ë•Œë¬¸ì— ìµœì ê°’ì„ ì°¾ì•„ ë‚˜ê°€ê¸° ìœ„í•´ì„œ í•œì¹¸ ì „ì§„í•  ë•Œë§ˆë‹¤ ëª¨ë“  ë°ì´í„° ì…‹ì„ ë„£ì–´ì£¼ì–´ì•¼ í•´ì•¼í•˜ê¸° ë•Œë¬¸ì— í•™ìŠµì´ êµ‰ì¥íˆ ì˜¤ë˜ ê±¸ë¦¬ëŠ” ë¬¸ì œê°€ ë°œìƒí•˜ê²Œ ëœë‹¤.</li>
  <li>ê·¸ëŸ¼ Gradient Descent ë§ê³  ë” ë¹ ë¥¸ Optimizer ëŠ” ì—†ì„ê¹Œ? ê·¸ê²ƒì´ ë°”ë¡œ <strong>stochastic Gradient Descent</strong> ì´ë‹¤.</li>
</ul>

<h3 id="stochastic-gradient-descent">Stochastic Gradient Descent</h3>
<ul>
  <li>Stochastic Gradient Descent(SGD)ëŠ” ì¡°ê¸ˆë§Œ íì–´ë³´ê³ (Mini Batch)ë¹ ë¥´ê²Œ í•™ìŠµ í•˜ëŠ” ê²ƒì´ë‹¤.
<a href="https://postimg.cc/qhZ26tpM"><img src="https://i.postimg.cc/d3J6XdXd/41241231312312.png" width="500px" title="source: imgur.com" /><a></a></a></li>
  <li>ìµœì ê°’ì„ ì°¾ì•„ ê³¼ì •ì€ ì´ì™€ ê°™ë‹¤
<a href="https://postimg.cc/D8YyVqBz"><img src="https://i.postimg.cc/vZG1Vhn5/111.png" width="500px" title="source: imgur.com" /><a></a></a></li>
  <li>GDì˜ ê²½ìš° í•­ìƒ ì „ì²´ ë°ì´í„° ì…‹ì„ ê°€ì§€ê³  Learning rateë¡œ ìµœì ì˜ ê°’ì„ ì°¾ì•„ê°€ëŠ” ëª¨ìŠµì„ ë¶ˆ ìˆ˜ ìˆë‹¤.</li>
  <li>SGD ê°™ì€ ê²½ìš° Mini-batch ì‚¬ì´ì¦ˆ ë§Œí¼ ì¡°ê¸ˆì”© ëŒë ¤ì„œ ìµœì ì˜ ê°’ìœ¼ë¡œ ì°¾ì•„ë‚˜ê°„ë‹¤.</li>
</ul>

<h2 id="7-gradient-descent">7. Gradient Descent</h2>
<p><a href="https://postimg.cc/Hrs5z4js"><img src="https://i.postimg.cc/bw1Tzmhb/4121312312312.png" width="500px" title="source: imgur.com" /><a></a></a></p>
<ul>
  <li>we would work for all models, not just linear models
<a href="https://postimg.cc/r0L516r7"><img src="https://i.postimg.cc/C1RJVSLM/222.png" width="500px" title="source: imgur.com" /><a></a></a></li>
</ul>

<h3 id="gradient-descent-for-linear-models">Gradient Descent for Linear Models</h3>
<p><a href="https://postimg.cc/B8JqHMvM"><img src="https://i.postimg.cc/CKqzy2pT/412312312312321312.png" width="500px" title="source: imgur.com" /><a></a></a></p>

<h2 id="8-relationship-to-montel-carlo">8. Relationship to Montel Carlo</h2>
<ul>
  <li>Recall when we did not parameterized V(s) - as if V(s) itself was a parameter
<a href="https://postimg.cc/H8YZt8MG"><img src="https://i.postimg.cc/XqFR4w7v/1241231312.png" width="300px" title="source: imgur.com" /><a></a></a></li>
  <li>this is the exact same equation we had before for Mote Carlo</li>
  <li>therefore, what we were doing was an instance of <strong>gradient descent</strong></li>
</ul>

<h2 id="9-feature-engineering">9. Feature Engineering</h2>
<ul>
  <li>Recall: neural networks can in some sense find good nonlinear transformations / features of the raw data</li>
  <li>since we only study linear methods in the past section, we need to find features manually</li>
  <li>Feature Engineering ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì„ ì‘ë™í•˜ê¸° ìœ„í•´ ë°ì´í„°ì— ëŒ€í•œ ë„ë©”ì¸ ì§€ì‹ì„ í™œìš©í•˜ì—¬ íŠ¹ì§•(Feature)ì„ ë§Œë“¤ì–´ ë‚´ëŠ” ê²ƒ</li>
  <li>ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ ìœ„í•œ ë°ì´í„° í…Œì´ë¸”ì˜ column(íŠ¹ì§•)ì„ ìƒì„±í•˜ê±°ë‚˜ ì„ íƒí•˜ëŠ” ì‘ì—…ì„ ì˜ë¯¸</li>
  <li>ì¦‰ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë†’ì´ê¸° ìœ„í•´ ì´ˆê¸° ì£¼ì–´ì§„ ë°ì´í„°ë¡œë¶€í„° íŠ¹ì§•ì„ ê°€ê³µí•˜ê³  ìƒì„±í•˜ëŠ” ì „ì²´ ê³¼ì •</li>
  <li>Feature Engineering ëª¨ë¸ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì´ í¬ê¸° ë•Œë¬¸ì— ë¨¸ì‹ ëŸ¬ë‹ ì‘ìš©ì— ìˆì–´ì„œ êµ‰ì¥íˆ ì¤‘ìš”í•œ ë‹¨ê³„</li>
  <li>Feature Engineering ì•„ë˜ì™€ ê°™ì€ ë‹¨ê³„ì— êµ¬ì„±ë˜ì–´ ìˆë‹¤.
    <ol>
      <li>Project scoping(Define Problem)</li>
      <li>Data Collection</li>
      <li>EDA</li>
      <li>Data Preprocessing</li>
      <li><strong>Feature Engineering</strong></li>
      <li>Modeling</li>
      <li>Evaluation</li>
      <li>Project Delivery / Insights</li>
    </ol>
  </li>
  <li>Feature Eingeeringì€ ëª¨ë¸ì— ì…ë ¥í•˜ê¸° ì „ ë‹¨ê³„ì— ë°ì´í„°ì˜ íŠ¹ì„±ì„ ì˜ ë°˜ì˜í•˜ê³  ì„±ëŠ¥ì„ ë†’ì¼ ìˆ˜ ìˆë„ë¡ íŠ¹ì§•ì„ ìƒì„±í•˜ê³  ê°€ê³µí•˜ëŠ” ë‹¨ê³„ì´ë‹¤.</li>
  <li>Feature Engineeringì— êµ¬ì„±ì€ ì•„ë˜ì™€ ê°™ë‹¤.
    <h3 id="ë°©ë²•ì ì¸-ì¸¡ë©´">ë°©ë²•ì ì¸ ì¸¡ë©´</h3>
    <ol>
      <li>Feature Selection(íŠ¹ì§• ì„ íƒ)</li>
    </ol>
    <ul>
      <li>íŠ¹ì§• ë­í‚¹(Feature Ranking) ë˜ëŠ” íŠ¹ì§• ì¤‘ìš”ë„ (Feature Importance)ë¼ê³ ë„ ë¶ˆë¦°ë‹¤.</li>
      <li>ë¶ˆë¥˜ ëª¨ë¸ ì¤‘ Decision Tree ê°™ì€ ê²½ìš° íŠ¸ë¦¬ì˜ ìƒë‹¨ì— ìˆì„ ìˆ˜ë¡ ì¤‘ìš”ë„ê°€ ë†’ìœ¼ë¯€ë¡œ ì´ë¥¼ ë°˜ì˜í•˜ì—¬ íŠ¹ì§• ë³„ë¡œ ì¤‘ìš”ë„ë¥¼ ë§¤ê¸´ë‹¤.</li>
      <li>íšŒê·€ ëª¨ë¸ì˜ ê²½ìš° Forward Selection ê³¼ Backward Elimination ê°™ì€ ì•Œê³ ë¦¬ì¦˜ì„ í†µí•´ íŠ¹ì§•ì„ ì„ íƒí•œë‹¤.
    2. Dimension Reduction(ì°¨ì› ê°ì†Œ)</li>
      <li>Dimension Reductionì€ Feature extraction(íŠ¹ì§• ì¶”ì¶œ)ì´ë¼ëŠ” ë§ë¡œë„ ë¶ˆë¦°ë‹¤.</li>
      <li>ì°¨ì› ì¶•ì†ŒëŠ” ë°ì´í„°ì˜ ì••ì¶•ì´ë‚˜ ì¡ìŒì„ ì œê±°í•˜ëŠ” íš¨ê³¼ë„ ìˆì§€ë§Œ, <strong>ê´€ì¸¡ ë°ì´í„°ë¥¼ ì˜ ì„¤ëª…í•  ìˆ˜ ìˆëŠ” ì ì¬ ê³µê°„(Latent space)ì„ ì°¾ëŠ” ê²ƒì´ë‹¤.</strong></li>
      <li>ê°€ì¥ ëŒ€í‘œì ì¸ ì•Œê³ ë¦¬ì¦˜ì€ PCA(Principle Component Analysis)</li>
      <li>PCAëŠ” ê° Feature(ë³€ìˆ˜)ë¥¼ í•˜ë‚˜ì˜ ì¶•ìœ¼ë¡œ íˆ¬ì˜ì‹œì¼°ì„ ë•Œ ë¶„ì‚°ì´ ê°€ì¥ í° ì¶•ì„ ì²«ë²ˆì§¸ ì£¼ì„±ë¶„ìœ¼ë¡œ ì„ íƒí•˜ê³  ê·¸ë‹¤ìŒ í° ì¶•ì„ ë‘ë²ˆì§¸ ì£¼ì„±ë¶„ìœ¼ë¡œ ì„ íƒí•˜ê³  ë°ì´í„°ë¥¼ ì„ í˜• ë³€í™˜í•˜ì—¬ ë‹¤ì°¨ì›ì„ ì¶•ì†Œí•˜ëŠ” ë°©ë²•ì´ë‹¤.
        <h3 id="domainë¶„ì•¼-ì „ë¬¸ì„±-ì¸¡ë©´">Domain(ë¶„ì•¼) ì „ë¬¸ì„± ì¸¡ë©´</h3>
        <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>3. Feature Generation(íŠ¹ì§• ìƒì„±) or Feature Construction(íŠ¹ì§• êµ¬ì¶•)
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
  <li>ì´ ë°©ë²•ì„ ì£¼ë¡œ Feature Engineering ì´ë¼ê³  ë§í•œë‹¤.</li>
  <li>ì´ˆê¸°ì— ì£¼ì–´ì§„ ë°ì´í„°ë¡œ ë¶€í„° ëª¨ë¸ë§ ì„±ëŠ¥ì„ ë†’ì´ëŠ” ìƒˆë¡œìš´ íŠ¹ì§•ì„ ë§Œë“œëŠ” ê³¼ì •ì´ë‹¤.</li>
  <li>ì´ë•Œ ë°ì´í„°ì— ëŒ€í•œ Domain(ë¶„ì•¼) ì „ë¬¸ì„±ì„ ë°”íƒ•ìœ¼ë¡œ ë°ì´í„°ë¥¼ í•©ì¹˜ê±°ë‚˜ ìª¼ê°œëŠ” ë“±ì„ ê±°ì³ Featureì„ ë§Œë“¤ê²Œ ëœë‹¤.</li>
  <li>ê°„ë‹¨í•œ ì˜ˆë¡œ ì‹œê°„ ë°ì´í„°ë¥¼ AM/PMìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì´ë‹¤.</li>
  <li>ì´ ì‘ì—…ì€ í•œë²ˆ í•´ì„œ ëë‚˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ëŠì„ ì—†ì´ ëª¨ë¸ë§ ì„±ëŠ¥ì„ ë†’ì´ëŠ” ëª©ì ìœ¼ë¡œ ë°˜ë³µí•´ì„œ ì‘ì—…í•  ìˆ˜ ìˆëŠ” ë¶€ë¶„ì´ê¸° ë•Œë¬¸ì— ì „ë¬¸ì„±ê³¼ ê²½í—˜ì— ë”°ë¼ ë¹„ìš©ê³¼ ì‹œê°„ì„ ì¤„ì¼ ìˆ˜ ìˆëŠ” ë¶€ë¶„ì´ë‹¤.</li>
</ul>

<h3 id="mapping-s---x">Mapping s -&gt; x</h3>
<ul>
  <li>states can be thought of as categorical variable
    <ul>
      <li>(0,0) -&gt; category 1</li>
      <li>(0,1) -&gt; category 2</li>
      <li>etc</li>
    </ul>
  </li>
  <li>how do we treat categorical variable? <strong>One-Hot encoding</strong></li>
  <li>what if we do one-hot encoding?
<a href="https://postimg.cc/VSv9RyDN"><img src="https://i.postimg.cc/0Ndt9PS7/4141231232132.png" width="300px" title="source: imgur.com" /><a></a></a></li>
  <li>s=(0,0) -&gt; x=[1,0,0,..]</li>
  <li>s=(0,1) -&gt; x=[0,1,0,..]</li>
  <li>D is the cardinality(åŸºæ•°) of S
    <ul>
      <li>ì§‘í•©ì˜ cardinalityëŠ” ì§‘í•©ì˜ ì›ì†Œì˜ ê°œìˆ˜ë¥¼ ë§í•œë‹¤.</li>
      <li>ë‘ ì§‘í•© A,Bê°€ cardinal numberê°€ ê°™ë‹¤ê³  í•¨ì€ ì¼ëŒ€ì¼ ëŒ€ì‘ f: Aâ€“&gt; Bê°€ ì¡´ì¬í•˜ëŠ” ê²ƒì´ë‹¤.</li>
      <li>ë”°ë¼ì„œ ë‘ ì§‘í•© A,B cardinal numberê°€ ê°™ë‹¤ë©´ ì›ì†Œì˜ ê°œìˆ˜ê°€ ê°™ì€ê²ƒì´ë‹¤.</li>
    </ul>
  </li>
  <li>Problem with one-hot encoding: this requires the same number of parameters as measuring V(s) directly</li>
  <li>i.e. V is a dict with ISI keys and ISI values</li>
  <li>if we do one-hot encoding, each Component $Î¸_i$ actually represents V(s=i) itself
<a href="https://postimg.cc/ygFTJmLh"><img src="https://i.postimg.cc/ydGpbn9G/1412312412313.png" width="300px" title="source: imgur.com" /><a></a></a></li>
</ul>

<h3 id="one-hot-encoding">One-Hot Encoding</h3>
<ul>
  <li>one positive aspect to using one-hot encoding</li>
  <li>suppose our code is not working(our V isnâ€™t accurate)</li>
  <li>we can have perfectly good code/no bugs, but still might not yield good results because feature are bad</li>
  <li>Change feature transformer to use one-hot encoding</li>
  <li>if it starts working, that confirm our features are bat(since itâ€™s the same as a non-approximate method)</li>
</ul>

<h3 id="alternative-to-one-hot">Alternative to One-Hot</h3>
<ul>
  <li>in case of GridWorld, each(i,j) represents a position in 2-D space</li>
  <li>itâ€™s more like 2 real numbers than a category</li>
  <li>Vector x can be (i,j) itself</li>
  <li>could scale it so mean = 0 and var = 1</li>
  <li>this x is the â€œraw dataâ€</li>
  <li>problem
    <ul>
      <li>model is only linear</li>
      <li>for fixed j, V(i) is just a line
<a href="https://postimg.cc/SjZpjCHb"><img src="https://i.postimg.cc/CL0MP4N5/41312312.png" width="300px" title="source: imgur.com" /><a></a></a></li>
    </ul>
  </li>
</ul>

<h3 id="polynomials">Polynomials</h3>
<ul>
  <li>Recall from linear regression class: we can make new features by creating polynomials</li>
  <li>Recall from calculus: infinite taylor expansion can approximate any function</li>
  <li>Ex. second order terms
    <ul>
      <li>$x_1$^2</li>
      <li>$x_2$^2</li>
      <li>$x_1$$x_2$</li>
    </ul>
  </li>
  <li>donâ€™t overfit</li>
</ul>

<h2 id="10-feature-engineering">10. Feature Engineering</h2>
:ET