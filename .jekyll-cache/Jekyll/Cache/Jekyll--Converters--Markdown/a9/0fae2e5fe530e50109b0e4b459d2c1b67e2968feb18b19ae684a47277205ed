I"§+<h1 id="approximation-methods">Approximation Methods</h1>
<ul>
  <li>Major Disadvantage of methods weâ€™ve learned</li>
  <li>V - Need to estimate ISI values</li>
  <li>Q - Need to estimate ISI x IAI values</li>
  <li>ISI and IAI can be very large</li>
  <li>solution : Approximation</li>
  <li>Recall: neural networks are universal function approximator</li>
  <li>First, we do feature extraction: convert state s to <strong>feature vector x</strong>
<a href="https://postimg.cc/VSsNgQMT"><img src="https://i.postimg.cc/N0mrgQ0g/414141412312.png" width="300px" title="source: imgur.com" /><a></a></a></li>
  <li>we want a function, parameterized by theta, that accurately approximates V
<a href="https://postimg.cc/yJBDGp3q"><img src="https://i.postimg.cc/MGf1vhWG/123131.png" width="300px" title="source: imgur.com" /><a></a></a></li>
</ul>

<h2 id="1-linear-approximation">1. Linear Approximation</h2>
<ul>
  <li>Function approximation methods requires us to use models that are differentiable</li>
  <li>Canâ€™t use something like decision tree or k-nearest neighbour</li>
  <li>in sequal(æŸ¥è¯¢è¯­è¨€) to this section, we will use deep learning methods, which are differentiable</li>
  <li>Neural networks donâ€™t require us to engineer features beforehand, but it can help</li>
  <li>FOr Approximation methods, we will need linear regression and gradient descent</li>
</ul>

<h2 id="2-section-outline">2. Section outline</h2>
<ul>
  <li>MC prediction</li>
  <li>TD(0) prediction</li>
  <li>SARSA</li>
</ul>

<h2 id="3-sanityæ˜æ™º-checking">3. Sanity(æ˜æ™º) Checking</h2>
<ul>
  <li>we can always sanity check our work by comparing to non-approximated versions</li>
  <li>we expect approximation to be close, but perhaps not exactly equal</li>
  <li>Obstacle we may encounter: our code is implemented perfectly, but our model is bad</li>
  <li>Linear models are not very expressive</li>
  <li>if we extract a poor set of features, model wonâ€™t learn value function well</li>
  <li>will need to put in manual work to do feature engineering</li>
</ul>

<h2 id="4-linear-model">4. Linear Model</h2>
<ul>
  <li>Supervised learning aka,<strong>Function Approximation</strong></li>
  <li>The function we are trying to approximate is V(s) or Q(s,a)</li>
  <li>Rewards are real numbers</li>
  <li>So returns, which are sums of rewards are real number</li>
  <li>2 Supervised learning techniques
    <ul>
      <li>Classification</li>
      <li>regression</li>
    </ul>
  </li>
  <li>we are doing regression</li>
</ul>

<h2 id="5-error">5. Error</h2>
<ul>
  <li>Supervised Learning methods have cost Functions</li>
  <li>Regression means squared error is the appropriate choice</li>
  <li>Squared difference between V and estimate of V
<a href="https://postimg.cc/m1G258rz"><img src="https://i.postimg.cc/Gt489fwQ/121231231231.png" width="300px" title="source: imgur.com" /><a></a></a></li>
  <li>Replace V with its definition</li>
  <li>But we donâ€™t know this expected value
<a href="https://postimg.cc/s1rvggVY"><img src="https://i.postimg.cc/9X4T1DKn/124123124123.png" width="300px" title="source: imgur.com" /><a></a></a></li>
  <li>as we learned from MC(Monte Carlo), we can replace the expected value with its sample means
<a href="https://postimg.cc/0MrZ60G4"><img src="https://i.postimg.cc/DZPNMCS7/433.png" width="300px" title="source: imgur.com" /><a></a></a></li>
  <li>Alternatively, we can treat each G_(i,s) as a training sample</li>
  <li>And try to minimize all the <strong>individual squared differences</strong> simultaneously(just like linear regression)
<a href="https://postimg.cc/m1kDWtsC"><img src="https://i.postimg.cc/4ybht92L/41241312321.png" width="300px" title="source: imgur.com" /><a>
<a href="https://postimg.cc/XZKcpDyx"><img src="https://i.postimg.cc/vmkP2J8R/412313124123.png" width="300px" title="source: imgur.com" /><a></a></a></a></a></li>
</ul>

<h2 id="6-stochastic-gradient-descent">6. stochastic Gradient Descent</h2>
<ul>
  <li>we can now do stochastic(éšæœºçš„) gradient decent</li>
  <li>Move in direction of gradient of error wrt only one sample at a time</li>
</ul>

<h3 id="gradient-descent-ì„¤ëª…">Gradient Descent ì„¤ëª…</h3>
<ul>
  <li>Neural Networkì˜ loss functionì˜ í˜„ weightì˜ ê¸°ìš¸ê¸°(gradient)ë¥¼ êµ¬í•˜ê³  Lossë¥¼ ì¤„ì´ëŠ” ë°©í–¥ìœ¼ë¡œ ì—…ë°ì´íŠ¸ ë‚˜ê°€ëŠ” ë°©ë²•
<a href="https://postimg.cc/XGVhJ5XG"><img src="https://i.postimg.cc/0QmqVdsf/124141312312.png" width="500px" title="source: imgur.com" /><a></a></a></li>
  <li>Loss function (ERROR) ëŠ” í˜„ì¬ ê°€ì¤‘ì¹˜(weight)ì—ì„œ í‹€ë¦° ì •ë„ë¥¼ ì•Œë ¤ì£¼ëŠ” í•¨ìˆ˜ì´ë‹¤.</li>
  <li>ì¦‰, í˜„ì¬ ë„¤íŠ¸ì›Œí¬ì˜ weightì—ì„œ ë‚´ê°€ ê°€ì§„ ë°ì´í„°ë¥¼ ì§‘ì–´ ë„£ìœ¼ë©´ ì—ëŸ¬ê°€ ìƒê¸¸ ê²ƒì´ë‹¤. ê±°ê¸°ì— ë¯¸ë¶„ì„ í•˜ë©´ ì—ëŸ¬ë¥¼ ì¤„ì´ëŠ” ë°©í–¥ì„ ì•Œ ìˆ˜ ìˆë‹¤.</li>
  <li>ê·¸ ë°©í–¥ìœ¼ë¡œ ì •í•´ì§„ Learning rateë¥¼ í†µí•´ì„œ weightë¥¼ ì´ë™ì‹œí‚¨ë‹¤. ì´ê²ƒì„ ë°˜ë³µí•´ì„œ í•™ìŠµí•˜ëŠ” ê²ƒì´ë‹¤.</li>
  <li><strong>ì¦‰ Gradient DescentëŠ” í˜„ì¬ ë„¤íŠ¸ì›Œí¬ì˜ Weightì— ì–´ë–¤ ë°ì´í„°ë¥¼ ë„£ì„ ë–„ ì—ëŸ¬ê°’ì„ ë¯¸ë¶„í•´ì„œ ì—ëŸ¬ì˜ ë°©í–¥(Gradient)ì„ ì•Œì• ì²´ê³  ê·¸ ë°©í–¥ìœ¼ë¡œ ì •í•´ì§„ Learning rateë¥¼ í†µí•´ì„œ weightì„ ì´ë™ì‹œí‚¨ë‹¤(Descent).</strong>
<a href="https://postimg.cc/kR47V4fs"><img src="https://i.postimg.cc/fy7yrSdG/141412412312312.png" width="500px" title="source: imgur.com" /><a></a></a></li>
  <li>Gradient Descentì˜ ë‹¨ì ì€ ê°€ì§„ ë°ì´í„°ë¥¼ ë‹¤ ë„£ìœ¼ë©´ ì „ì²´ ì—ëŸ¬ê°€ ê³„ì‚°ì´ ë  ê²ƒì´ë‹¤.</li>
  <li>ê·¸ë ‡ê¸° ë•Œë¬¸ì— ìµœì ê°’ì„ ì°¾ì•„ ë‚˜ê°€ê¸° ìœ„í•´ì„œ í•œì¹¸ ì „ì§„í•  ë•Œë§ˆë‹¤ ëª¨ë“  ë°ì´í„° ì…‹ì„ ë„£ì–´ì£¼ì–´ì•¼ í•´ì•¼í•˜ê¸° ë•Œë¬¸ì— í•™ìŠµì´ êµ‰ì¥íˆ ì˜¤ë˜ ê±¸ë¦¬ëŠ” ë¬¸ì œê°€ ë°œìƒí•˜ê²Œ ëœë‹¤.</li>
  <li>ê·¸ëŸ¼ Gradient Descent ë§ê³  ë” ë¹ ë¥¸ Optimizer ëŠ” ì—†ì„ê¹Œ? ê·¸ê²ƒì´ ë°”ë¡œ <strong>stochastic Gradient Descent</strong> ì´ë‹¤.</li>
</ul>

<h3 id="stochastic-gradient-descent">Stochastic Gradient Descent</h3>
<ul>
  <li>Stochastic Gradient Descent(SGD)ëŠ” ì¡°ê¸ˆë§Œ íì–´ë³´ê³ (Mini Batch)ë¹ ë¥´ê²Œ í•™ìŠµ í•˜ëŠ” ê²ƒì´ë‹¤.
<a href="https://postimg.cc/qhZ26tpM"><img src="https://i.postimg.cc/d3J6XdXd/41241231312312.png" width="500px" title="source: imgur.com" /><a></a></a></li>
  <li>ìµœì ê°’ì„ ì°¾ì•„ ê³¼ì •ì€ ì´ì™€ ê°™ë‹¤
<a href="https://postimg.cc/D8YyVqBz"><img src="https://i.postimg.cc/vZG1Vhn5/111.png" width="500px" title="source: imgur.com" /><a></a></a></li>
  <li>GDì˜ ê²½ìš° í•­ìƒ ì „ì²´ ë°ì´í„° ì…‹ì„ ê°€ì§€ê³  Learning rateë¡œ ìµœì ì˜ ê°’ì„ ì°¾ì•„ê°€ëŠ” ëª¨ìŠµì„ ë¶ˆ ìˆ˜ ìˆë‹¤.</li>
  <li>SGD ê°™ì€ ê²½ìš° Mini-batch ì‚¬ì´ì¦ˆ ë§Œí¼ ì¡°ê¸ˆì”© ëŒë ¤ì„œ ìµœì ì˜ ê°’ìœ¼ë¡œ ì°¾ì•„ë‚˜ê°„ë‹¤.</li>
</ul>

<h2 id="7-gradient-descent">7. Gradient Descent</h2>
<p><a href="https://postimg.cc/Hrs5z4js"><img src="https://i.postimg.cc/bw1Tzmhb/4121312312312.png" width="500px" title="source: imgur.com" /><a></a></a></p>
<ul>
  <li>we would work for all models, not just linear models
<a href="https://postimg.cc/r0L516r7"><img src="https://i.postimg.cc/C1RJVSLM/222.png" width="500px" title="source: imgur.com" /><a></a></a></li>
</ul>

<h3 id="gradient-descent-for-linear-models">Gradient Descent for Linear Models</h3>
<p><a href="https://postimg.cc/B8JqHMvM"><img src="https://i.postimg.cc/CKqzy2pT/412312312312321312.png" width="500px" title="source: imgur.com" /><a></a></a></p>

<h2 id="8-relationship-to-montel-carlo">8. Relationship to Montel Carlo</h2>
<ul>
  <li>Recall when we did not parameterized V(s) - as if V(s) itself was a parameter
<a href="https://postimg.cc/H8YZt8MG"><img src="https://i.postimg.cc/XqFR4w7v/1241231312.png" width="300px" title="source: imgur.com" /><a></a></a></li>
  <li>this is the exact same equation we had before for Mote Carlo</li>
  <li>therefore, what we were doing was an instance of <strong>gradient descent</strong></li>
</ul>

<h2 id="9-feature-engineering">9. Feature Engineering</h2>
<ul>
  <li>Recall: neural networks can in some sense find good nonlinear transformations / features of the raw data</li>
  <li>since we only study linear methods in the past section, we need to find features manually</li>
  <li>Feature Engineering ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì„ ì‘ë™í•˜ê¸° ìœ„í•´ ë°ì´í„°ì— ëŒ€í•œ ë„ë©”ì¸ ì§€ì‹ì„ í™œìš©í•˜ì—¬ íŠ¹ì§•(Feature)ì„ ë§Œë“¤ì–´ ë‚´ëŠ” ê²ƒ</li>
  <li>ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ ìœ„í•œ ë°ì´í„° í…Œì´ë¸”ì˜ column(íŠ¹ì§•)ì„ ìƒì„±í•˜ê±°ë‚˜ ì„ íƒí•˜ëŠ” ì‘ì—…ì„ ì˜ë¯¸</li>
  <li>ì¦‰ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë†’ì´ê¸° ìœ„í•´ ì´ˆê¸° ì£¼ì–´ì§„ ë°ì´í„°ë¡œë¶€í„° íŠ¹ì§•ì„ ê°€ê³µí•˜ê³  ìƒì„±í•˜ëŠ” ì „ì²´ ê³¼ì •</li>
  <li>Feature Engineering ëª¨ë¸ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì´ í¬ê¸° ë•Œë¬¸ì— ë¨¸ì‹ ëŸ¬ë‹ ì‘ìš©ì— ìˆì–´ì„œ êµ‰ì¥íˆ ì¤‘ìš”í•œ ë‹¨ê³„</li>
  <li>Feature Engineering ì•„ë˜ì™€ ê°™ì€ ë‹¨ê³„ì— êµ¬ì„±ë˜ì–´ ìˆë‹¤.
    <ol>
      <li>Project scoping(Define Problem)</li>
      <li>Data Collection</li>
      <li>EDA</li>
      <li>Data Preprocessing</li>
      <li><strong>Feature Engineering</strong></li>
      <li>Modeling</li>
      <li>Evaluation</li>
      <li>Project Delivery / Insights</li>
    </ol>
  </li>
  <li>Feature Eingeeringì€ ëª¨ë¸ì— ì…ë ¥í•˜ê¸° ì „ ë‹¨ê³„ì— ë°ì´í„°ì˜ íŠ¹ì„±ì„ ì˜ ë°˜ì˜í•˜ê³  ì„±ëŠ¥ì„ ë†’ì¼ ìˆ˜ ìˆë„ë¡ íŠ¹ì§•ì„ ìƒì„±í•˜ê³  ê°€ê³µí•˜ëŠ” ë‹¨ê³„ì´ë‹¤.</li>
  <li>Feature Engineeringì— êµ¬ì„±ì€ ì•„ë˜ì™€ ê°™ë‹¤.
    <h3 id="ë°©ë²•ì ì¸-ì¸¡ë©´">ë°©ë²•ì ì¸ ì¸¡ë©´</h3>
    <ol>
      <li>Feature Selection(íŠ¹ì§• ì„ íƒ)</li>
    </ol>
    <ul>
      <li>íŠ¹ì§• ë­í‚¹(Feature Ranking) ë˜ëŠ” íŠ¹ì§• ì¤‘ìš”ë„ (Feature Importance)ë¼ê³ ë„ ë¶ˆë¦°ë‹¤.</li>
      <li>ë¶ˆë¥˜ ëª¨ë¸ ì¤‘ Decision Tree ê°™ì€ ê²½ìš° íŠ¸ë¦¬ì˜ ìƒë‹¨ì— ìˆì„ ìˆ˜ë¡ ì¤‘ìš”ë„ê°€ ë†’ìœ¼ë¯€ë¡œ ì´ë¥¼ ë°˜ì˜í•˜ì—¬ íŠ¹ì§• ë³„ë¡œ ì¤‘ìš”ë„ë¥¼ ë§¤ê¸´ë‹¤.</li>
      <li>íšŒê·€ ëª¨ë¸ì˜ ê²½ìš° Forward Selection ê³¼ Backward Elimination ê°™ì€ ì•Œê³ ë¦¬ì¦˜ì„ í†µí•´ íŠ¹ì§•ì„ ì„ íƒí•œë‹¤.
    2. Dimension Reduction(ì°¨ì› ê°ì†Œ)</li>
      <li>Dimension Reductionì€ Feature extraction(íŠ¹ì§• ì¶”ì¶œ)ì´ë¼ëŠ” ë§ë¡œë„ ë¶ˆë¦°ë‹¤.</li>
      <li>ì°¨ì› ì¶•ì†ŒëŠ” ë°ì´í„°ì˜ ì••ì¶•ì´ë‚˜ ì¡ìŒì„ ì œê±°í•˜ëŠ” íš¨ê³¼ë„ ìˆì§€ë§Œ, <strong>ê´€ì¸¡ ë°ì´í„°ë¥¼ ì˜ ì„¤ëª…í•  ìˆ˜ ìˆëŠ” ì ì¬ ê³µê°„(Latent space)ì„ ì°¾ëŠ” ê²ƒì´ë‹¤.</strong></li>
      <li>ê°€ì¥ ëŒ€í‘œì ì¸ ì•Œê³ ë¦¬ì¦˜ì€ PCA(Principle Component Analysis)</li>
      <li>PCAëŠ” ê° Feature(ë³€ìˆ˜)ë¥¼ í•˜ë‚˜ì˜ ì¶•ìœ¼ë¡œ íˆ¬ì˜ì‹œì¼°ì„ ë•Œ ë¶„ì‚°ì´ ê°€ì¥ í° ì¶•ì„ ì²«ë²ˆì§¸ ì£¼ì„±ë¶„ìœ¼ë¡œ ì„ íƒí•˜ê³  ê·¸ë‹¤ìŒ í° ì¶•ì„ ë‘ë²ˆì§¸ ì£¼ì„±ë¶„ìœ¼ë¡œ ì„ íƒí•˜ê³  ë°ì´í„°ë¥¼ ì„ í˜• ë³€í™˜í•˜ì—¬ ë‹¤ì°¨ì›ì„ ì¶•ì†Œí•˜ëŠ” ë°©ë²•ì´ë‹¤.
        <h3 id="domainë¶„ì•¼-ì „ë¬¸ì„±-ì¸¡ë©´">Domain(ë¶„ì•¼) ì „ë¬¸ì„± ì¸¡ë©´</h3>
        <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>3. Feature Generation(íŠ¹ì§• ìƒì„±) or Feature Construction(íŠ¹ì§• êµ¬ì¶•)
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
  <li>ì´ ë°©ë²•ì„ ì£¼ë¡œ Feature Engineering ì´ë¼ê³  ë§í•œë‹¤.</li>
  <li>ì´ˆê¸°ì— ì£¼ì–´ì§„ ë°ì´í„°ë¡œ ë¶€í„° ëª¨ë¸ë§ ì„±ëŠ¥ì„ ë†’ì´ëŠ” ìƒˆë¡œìš´ íŠ¹ì§•ì„ ë§Œë“œëŠ” ê³¼ì •ì´ë‹¤.</li>
  <li>ì´ë•Œ ë°ì´í„°ì— ëŒ€í•œ Domain(ë¶„ì•¼)</li>
</ul>
:ET