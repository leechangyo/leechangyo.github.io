I"π<h1 id="1-golden-section-search">1. GOLDEN SECTION SEARCH</h1>
<p><a href="https://postimg.cc/30wMDX9T"><img src="https://i.postimg.cc/wMX9ZFm7/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>
<ul>
  <li>The search methods we discuss in this and the next section allow us to determine the minimizer of a function <em>f</em> : R ‚Äî&gt; R over a closed interval, say [$a_o$, $b_o$]</li>
  <li>The only property that we assume of the objective function <em>f</em> is that it is unimodal(Îã® ÌïúÍ∞ÄÏßÄÏùò Î™®Îç∏)
    <ul>
      <li>which means that <em>f</em> has only one local minimizer</li>
    </ul>
  </li>
  <li>The methods we discuss are based on evaluating the objective function at different points in the interval [$a_o$, $b_o$]</li>
  <li>We choose these points in such a way that an approximation to the minimizer of / may be achieved in as few evaluations as possible.</li>
  <li><strong>Our goal is to progressively narrow the range until the minimizer is ‚Äúboxed in‚Äù with sufficient accuracy.</strong></li>
</ul>

<p><a href="https://postimg.cc/LhjgyzHT"><img src="https://i.postimg.cc/ZYMP5c21/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li>Consider a unimodal function / of one variable and the interval [$a_o$, $b_o$].
    <ul>
      <li>If we evaluate / at only one intermediate point of the interval, we cannot narrow the range within which we know the minimizer is located.</li>
      <li>We have to evaluate <em>f</em> at two intermediate points</li>
      <li>We choose the intermediate points in such a way that the reduction in the range is symmetric, in the sense that:</li>
    </ul>
  </li>
</ul>

<p><a href="https://postimg.cc/sBZgMrxc"><img src="https://i.postimg.cc/MG9MrZ8h/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>
<ul>
  <li>We then evaluate <em>f</em> at the intermediate points</li>
</ul>

<p><a href="https://postimg.cc/zy2jF9N6"><img src="https://i.postimg.cc/25rKrDKk/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>
<ul>
  <li>
    <p>If f($a_1$) &lt; f($a_2$), then the minimizer
must lie in the range [$a_0$, $b_1$]. If, on the other hand,f($a_1$) &gt;= f($a_2$), then the minimizer
is located in the range [$a_1$, $b_0$]</p>
  </li>
  <li>Starting with the reduced range of uncertainty we can repeat the process and similarly find two new points, say $a_2$ and $b_2$ using the same value of p &lt; 1/2</li>
  <li>However, we would like to minimize the number of the objective function evaluations while reducing the width of the uncertainty interval.</li>
  <li>Suppose, for example, that f($a_1$) &lt; f($b_1$), as in Figure 7.3.</li>
</ul>

<p><a href="https://postimg.cc/hhSRwdTn"><img src="https://i.postimg.cc/y6R6DhND/Capture.png" width="300px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li>Because $a_1$ is already in the uncertainty interval and f ( $a_1$ ) is already known, we can make $a_1$
coincide with $b_2$</li>
  <li>Thus, only one new evaluation of <em>f</em> at $a_2$ would be necessary</li>
</ul>

<p><a href="https://postimg.cc/t7VWKsjr"><img src="https://i.postimg.cc/1RvBNFYZ/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li>To find the value of <em>p</em> that results in only one new evaluation of <em>f</em></li>
  <li>Without loss of generality, imagine that the original range [$a_0$, $b_0$] is of unit length.</li>
</ul>

<p><a href="https://postimg.cc/871S8fqM"><img src="https://i.postimg.cc/Sx9NMWdT/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>

<p><a href="https://postimg.cc/PPyW4Bhh"><img src="https://i.postimg.cc/Bn9mq0V8/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li>Thus, dividing a range in the ratio of p to 1 ‚Äî p has the effect that the ratio of the shorter segment to the longer equals the ratio of the longer to the sum of the two.</li>
  <li>This rule was referred to by ancient Greek geometers as the <em>Golden Section.</em></li>
  <li>Using this Golden Section rule means that at every stage of the uncertainty range reduction (except the first one), the objective function <em>f</em> need only be evaluated at one new point.</li>
  <li>The uncertainty range is reduced by the ratio 1 ‚Äî p = 0.61803 at every stage.</li>
  <li>Hence, N steps of reduction using the Golden Section method reduces the range by the factor</li>
</ul>

<p><a href="https://postimg.cc/cK0MB0ts"><img src="https://i.postimg.cc/6QGjwQ9G/Capture.png" width="300px" title="source: imgur.com" /><a></a></a></p>

<h2 id="example">Example</h2>
<p><a href="https://postimg.cc/xkhhG2p3"><img src="https://i.postimg.cc/nzhxJVvg/Capture.png" width="700px" title="source: imgur.com" /><a>
<a href="https://postimg.cc/FfNg2ndX"><img src="https://i.postimg.cc/1zNCVkyX/Capture.png" width="700px" title="source: imgur.com" /><a>
<a href="https://postimg.cc/DW3nF1nT"><img src="https://i.postimg.cc/tgYT3zcY/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></a></a></a></a></p>

<h2 id="example-1">Example</h2>

<h2 id="second-order-necessary-conditionsonc">Second order necessary condition(SONC)</h2>

<h2 id="sonc-for-interior-case">SONC for interior case</h2>
<p><a href="https://postimg.cc/xJbZd4Rp"><img src="https://i.postimg.cc/V67PKybw/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>

<h2 id="the-necessary-conditions-are-not-sufficient">The necessary conditions are not sufficient</h2>
<p><a href="https://postimg.cc/w34krVQ7"><img src="https://i.postimg.cc/7YwQftCM/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>

<h2 id="second-order-sufficient-condition-sosc">Second-order sufficient condition (SOSC)</h2>
<p><a href="https://postimg.cc/kBBdrTrZ"><img src="https://i.postimg.cc/TYc2DstR/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>

<h2 id="example-2">Example</h2>

<p><a href="https://postimg.cc/N9ZRsgGS"><img src="https://i.postimg.cc/BbnpC6rS/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>

<h2 id="remarks-on-roles-of-optimalityÊúÄ‰ºòÊÄß-conditions">Remarks on roles of optimality(ÊúÄ‰ºòÊÄß) conditions</h2>
<p><a href="https://postimg.cc/JtqBYcZS"><img src="https://i.postimg.cc/J4wcnpn7/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>

<h1 id="reference">Reference</h1>

<p><a href="https://web.stanford.edu/class/ee364a/lectures.html"> Optimization method - Standford University </a></p>

<p>https://www.youtube.com/results?search_query=convex+function</p>
:ET