I"2`<h1 id="cartpole-with-bins">Cartpole with bins</h1>
<p><a href="https://imgur.com/7l1pWjn"><img src="https://i.imgur.com/7l1pWjn.png" width="400px" title="source:imgur.com" /><a></a></a></p>
<ul>
  <li>Add some complexity: Q-Learning using the tabular method</li>
  <li>why?
    <ul>
      <li>it’s not necessary to do the most complicated thing possible</li>
      <li>simple adjustments allow us to use what we know, and it works well</li>
    </ul>
  </li>
  <li>CartPole state space is continuous / infinite</li>
  <li>But some states are more likely than others</li>
  <li>From the documentation, we can infer that some are impossible, since if we go past a certain angle/position, we lose</li>
  <li>very high velocity is likely impossible as well</li>
  <li>Cut up relevant part of the state space into boxes</li>
  <li>this image is a 3-D box (a 4-D box would look weird)</li>
  <li>now we have a discrete state space</li>
  <li>we just name these 0,1 etc</li>
  <li>now we can use the tabular method</li>
</ul>

<h2 id="hidden-complexities">Hidden Complexities</h2>
<ul>
  <li>how do we choose upper and lower limits of the box?
    <ul>
      <li>Naïve approach: just try different numbers until it works</li>
      <li>More complex: play some episode, plot histogram to see what limit are</li>
    </ul>
  </li>
  <li>what if we land in a state outside the box?
    <ul>
      <li>Extend the edge boxes to infinity
        <ul>
          <li>ex) box0 - box1 - box2 - box3</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="implementation">implementation</h2>
<ul>
  <li>Convert state into a bin</li>
  <li>Must be a unique number, so we can index a dictionary or array</li>
  <li>Not easy, so take our time to test out a few ideas</li>
</ul>

<h2 id="overwriting-rewards">Overwriting rewards</h2>
<ul>
  <li>Default reward is +1 for every step</li>
  <li>Doesn’t work too well ( but makes perfect sense)</li>
  <li>Better: give a large negative reward (like -300) if the pole falls</li>
  <li>incentivizes the agent to not reach that point</li>
  <li>is modifying the default rewards a good idea or not?</li>
  <li>in the real world, if we are building an agent to solve a novel task, we would be defining the rewards anyway</li>
  <li>the programmer must define an intelligent reward structure</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># https://deeplearningcourses.com/c/deep-reinforcement-learning-in-python
# https://www.udemy.com/deep-reinforcement-learning-in-python
</span><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">print_function</span><span class="p">,</span> <span class="n">division</span>
<span class="kn">from</span> <span class="nn">builtins</span> <span class="kn">import</span> <span class="nb">range</span>
<span class="c1"># Note: you may need to update your version of future
#       if builtins is not defined
# sudo pip install -U future
</span>
<span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">gym</span> <span class="kn">import</span> <span class="n">wrappers</span>
<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span>


<span class="c1"># turns list of integers into an int
# Ex.
# build_state([1,2,3,4,5]) -&gt; 12345
</span><span class="k">def</span> <span class="nf">build_state</span><span class="p">(</span><span class="n">features</span><span class="p">):</span>
  <span class="c1">#print(int("".join(map(lambda feature: str(int(feature)), features))))
</span>
  <span class="k">return</span> <span class="nb">int</span><span class="p">(</span><span class="s">""</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">feature</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">feature</span><span class="p">)),</span> <span class="n">features</span><span class="p">)))</span>

<span class="k">def</span> <span class="nf">to_bin</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">bins</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">digitize</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="p">[</span><span class="n">value</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="n">bins</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="c1"># it takese in a value and then array of possible bins and figures out which bin the value belongs in
</span>

<span class="k">class</span> <span class="nc">FeatureTransformer</span><span class="p">:</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># Note: to make this better you could look at how often each bin was
</span>    <span class="c1"># actually used while running the script.
</span>    <span class="c1"># It's not clear from the high/low values nor sample() what values
</span>    <span class="c1"># we really expect to get.
</span>    <span class="c1"># nine size because it given 10 bins
</span>    <span class="bp">self</span><span class="o">.</span><span class="n">cart_position_bins</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">2.4</span><span class="p">,</span> <span class="mf">2.4</span><span class="p">,</span> <span class="mi">9</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">cart_velocity_bins</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">9</span><span class="p">)</span> <span class="c1"># (-inf, inf) (I did not check that these were good values)
</span>    <span class="bp">self</span><span class="o">.</span><span class="n">pole_angle_bins</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mi">9</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">pole_velocity_bins</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">3.5</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">,</span> <span class="mi">9</span><span class="p">)</span> <span class="c1"># (-inf, inf) (I did not check that these were good values)
</span>
  <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">observation</span><span class="p">):</span>
    <span class="c1"># returns an int
</span>    <span class="n">cart_pos</span><span class="p">,</span> <span class="n">cart_vel</span><span class="p">,</span> <span class="n">pole_angle</span><span class="p">,</span> <span class="n">pole_vel</span> <span class="o">=</span> <span class="n">observation</span>
    <span class="k">return</span> <span class="n">build_state</span><span class="p">([</span>
      <span class="n">to_bin</span><span class="p">(</span><span class="n">cart_pos</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cart_position_bins</span><span class="p">),</span>
      <span class="n">to_bin</span><span class="p">(</span><span class="n">cart_vel</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cart_velocity_bins</span><span class="p">),</span>
      <span class="n">to_bin</span><span class="p">(</span><span class="n">pole_angle</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pole_angle_bins</span><span class="p">),</span>
      <span class="n">to_bin</span><span class="p">(</span><span class="n">pole_vel</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pole_velocity_bins</span><span class="p">),</span>
    <span class="p">])</span>


<span class="k">class</span> <span class="nc">Model</span><span class="p">:</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">feature_transformer</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">env</span> <span class="o">=</span> <span class="n">env</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">feature_transformer</span> <span class="o">=</span> <span class="n">feature_transformer</span>

    <span class="n">num_states</span> <span class="o">=</span> <span class="mi">10</span><span class="o">**</span><span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="c1"># 10000 num states
</span>    <span class="n">num_actions</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span>
    <span class="c1"># 2
</span>    <span class="bp">self</span><span class="o">.</span><span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">low</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">num_states</span><span class="p">,</span> <span class="n">num_actions</span><span class="p">))</span>
    <span class="c1">#2개 인덱스의 10000개 난수 생성
</span>
  <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">feature_transformer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">[</span><span class="n">x</span><span class="p">]</span>

  <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">G</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">feature_transformer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">[</span><span class="n">x</span><span class="p">,</span><span class="n">a</span><span class="p">]</span> <span class="o">+=</span> <span class="mf">1e-2</span><span class="o">*</span><span class="p">(</span><span class="n">G</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">[</span><span class="n">x</span><span class="p">,</span><span class="n">a</span><span class="p">])</span>
    <span class="c1"># this takens in as state action and target return
</span>
  <span class="k">def</span> <span class="nf">sample_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">eps</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">eps</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">play_one</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">gamma</span><span class="p">):</span>
  <span class="n">observation</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
  <span class="n">done</span> <span class="o">=</span> <span class="bp">False</span>
  <span class="n">totalreward</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="n">iters</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span> <span class="ow">and</span> <span class="n">iters</span> <span class="o">&lt;</span> <span class="mi">10000</span><span class="p">:</span>
    <span class="n">action</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">sample_action</span><span class="p">(</span><span class="n">observation</span><span class="p">,</span> <span class="n">eps</span><span class="p">)</span>
    <span class="n">prev_observation</span> <span class="o">=</span> <span class="n">observation</span>
    <span class="n">observation</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>

    <span class="n">totalreward</span> <span class="o">+=</span> <span class="n">reward</span>

    <span class="k">if</span> <span class="n">done</span> <span class="ow">and</span> <span class="n">iters</span> <span class="o">&lt;</span> <span class="mi">199</span><span class="p">:</span>
      <span class="n">reward</span> <span class="o">=</span> <span class="o">-</span><span class="mi">300</span>

    <span class="c1"># update the model
</span>    <span class="n">G</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">+</span> <span class="n">gamma</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="nb">max</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">observation</span><span class="p">))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">prev_observation</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">G</span><span class="p">)</span>

    <span class="n">iters</span> <span class="o">+=</span> <span class="mi">1</span>

  <span class="k">return</span> <span class="n">totalreward</span>


<span class="k">def</span> <span class="nf">plot_running_avg</span><span class="p">(</span><span class="n">totalrewards</span><span class="p">):</span>
  <span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">totalrewards</span><span class="p">)</span>
  <span class="n">running_avg</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
    <span class="n">running_avg</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">totalrewards</span><span class="p">[</span><span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">t</span><span class="o">-</span><span class="mi">100</span><span class="p">):(</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">running_avg</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Running Average"</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>


<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">'__main__'</span><span class="p">:</span>
  <span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s">'CartPole-v0'</span><span class="p">)</span>
  <span class="n">ft</span> <span class="o">=</span> <span class="n">FeatureTransformer</span><span class="p">()</span>
  <span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">ft</span><span class="p">)</span>
  <span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.9</span>

  <span class="k">if</span> <span class="s">'monitor'</span> <span class="ow">in</span> <span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">:</span>
    <span class="n">filename</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">basename</span><span class="p">(</span><span class="n">__file__</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">'.'</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">monitor_dir</span> <span class="o">=</span> <span class="s">'./'</span> <span class="o">+</span> <span class="n">filename</span> <span class="o">+</span> <span class="s">'_'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">())</span>
    <span class="n">env</span> <span class="o">=</span> <span class="n">wrappers</span><span class="o">.</span><span class="n">Monitor</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">monitor_dir</span><span class="p">)</span>

  <span class="n">N</span> <span class="o">=</span> <span class="mi">10000</span>
  <span class="n">totalrewards</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="mf">1.0</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">totalreward</span> <span class="o">=</span> <span class="n">play_one</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span>
    <span class="n">totalrewards</span><span class="p">[</span><span class="n">n</span><span class="p">]</span> <span class="o">=</span> <span class="n">totalreward</span>
    <span class="k">if</span> <span class="n">n</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
      <span class="k">print</span><span class="p">(</span><span class="s">"episode:"</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="s">"total reward:"</span><span class="p">,</span> <span class="n">totalreward</span><span class="p">,</span> <span class="s">"eps:"</span><span class="p">,</span> <span class="n">eps</span><span class="p">)</span>
  <span class="k">print</span><span class="p">(</span><span class="s">"avg reward for last 100 episodes:"</span><span class="p">,</span> <span class="n">totalrewards</span><span class="p">[</span><span class="o">-</span><span class="mi">100</span><span class="p">:]</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
  <span class="k">print</span><span class="p">(</span><span class="s">"total steps:"</span><span class="p">,</span> <span class="n">totalrewards</span><span class="o">.</span><span class="nb">sum</span><span class="p">())</span>

  <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">totalrewards</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Rewards"</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

  <span class="n">plot_running_avg</span><span class="p">(</span><span class="n">totalrewards</span><span class="p">)</span>
</code></pre></div></div>

<p>```
Reference:</p>

<p><a href="https://www.udemy.com/course/artificial-intelligence-reinforcement-learning-in-python/">Artificial Intelligence Reinforcement Learning</a></p>

<p><a href="https://www.udemy.com/deep-reinforcement-learning-in-python/">Advance AI : Deep-Reinforcement Learning</a></p>

<p><a href="https://www.udemy.com/cutting-edge-artificial-intelligence/learn/lecture/14650508#overview">Cutting-Edge Deep-Reinforcement Learning</a></p>
:ET