I"õ<h1 id="markov-decision-processes">Markov Decision Processes</h1>
<ul>
  <li>this section: formalize some RL concepts we already know about</li>
  <li>Agent, Environment, action, state, reward, episode</li>
  <li>Formal Framework: Markov Decision Processes(MDPs)</li>
</ul>

<h2 id="1-typical-game-by-solving-mdps-is-gridword">1. Typical Game by solving MDPs is GridWord</h2>
<p><a href="https://postimg.cc/VdS9ym01"><img src="https://i.postimg.cc/FspTLK9f/54123.png" width="500px" title="source: imgur.com" /></a></p>
<ul>
  <li>possible actions:</li>
  <li>up,down,left,right</li>
  <li>(1,1) -&gt; wall,canâ€™t go here</li>
  <li>(0,3) -&gt; Terminal(+1 Reward)</li>
  <li>(1,3) -&gt; Terminal(-1 Reward)</li>
  <li>12 Positions(w x h = 3 x 4 = 12)</li>
  <li>11 states (where the robot is)</li>
  <li>4 actions</li>
</ul>

<h2 id="2-markov-property">2. Markov Property</h2>
<ul>
  <li>Given a sequence:
 <a href="https://postimg.cc/fJKVgPBZ"><img src="https://i.postimg.cc/2Spn3RWq/4121233.png" width="500px" title="source: imgur.com" /></a></li>
  <li>Generally, this canâ€™t be simplified:
 <a href="https://postimg.cc/jw0qj6ZZ"><img src="https://i.postimg.cc/sX1MwwSd/41233.png" width="500px" title="source: imgur.com" /></a></li>
  <li>First-order Markov:
 <a href="https://postimg.cc/bZ4tfBS5"><img src="https://i.postimg.cc/zfGSKmzq/2.png" width="500px" title="source: imgur.com" /></a></li>
  <li>second-order Markov:
 <a href="https://postimg.cc/bDgrrCdp"><img src="https://i.postimg.cc/cCynDPSg/44.png" width="500px" title="source: imgur.com" /></a></li>
  <li>Simple Example
```
Consider the sentence : â€œLetâ€™s do a simple exampleâ€
Given:â€letâ€™s do a simpleâ€
Predict the new word? Easy</li>
</ul>

<p>Given: â€œsimpleâ€
Predict the next word? not as easy</p>

<p>Given: â€œaâ€
predict the next word? very difficult</p>

<p>is the Markov Property limiting? Not necessarily
```</p>

<h2 id="3-markov-property-in-rl">3. Markov Property in RL</h2>
<ul>
  <li>{S(t), A(t)} produces 2 things -&gt; {S(t+1),R(t+1)}</li>
  <li>Markov Property:
<a href="https://postimg.cc/bDgrrCdp"><img src="https://i.postimg.cc/cCynDPSg/44.png" width="500px" title="source: imgur.com" /></a></li>
  <li>Convenience notation
<a href="https://postimg.cc/LJrhqgTQ"><img src="https://i.postimg.cc/g00hTv6p/44.png" width="500px" title="source: imgur.com" /></a></li>
  <li>joint on sâ€™ and r, conditioned on 2 other variables</li>
  <li>different from â€œusualâ€ Markov: 1 RV(Random Variable) Conditioned on 1 other RV</li>
</ul>

<h2 id="4-other-conditional-distributions">4. Other Conditional distributions</h2>
<ul>
  <li>can be found using rules of probability</li>
  <li>For pretty much all cases weâ€™ll consider these will be deterministic</li>
  <li>i.e. states will always give us the same reward</li>
  <li>Action will always bring us to same next state</li>
  <li>But, these distributions are part of the core theory of RL
<a href="https://postimg.cc/BL5dTRGP"><img src="https://i.postimg.cc/W3xTvPW8/44.png" width="500px" title="source: imgur.com" /></a></li>
</ul>

<h2 id="5-is-the-markov-assumption-limiting">5. Is the Markov Assumption limiting?</h2>
<ul>
  <li>Not necessarily</li>
  <li>Recent application: DeepMind used concatenation(ä¸€ç³»åˆ—ç›¸å…³è”çš„äº‹ç‰©) of 4 most recent frames to represent state when playing Atari Games</li>
  <li>State can be made up of anything from anytime past to current</li>
  <li>Typically think of state right now = something we measure right now</li>
  <li>Also, donâ€™t need to use raw data(state can be features transformed from raw data)</li>
  <li>Any input from agentâ€™s sensors can be used to form state</li>
</ul>

<h2 id="6-markov-decision-processesmdps">6. Markov Decision Processes(MDPs)</h2>
<ul>
  <li>Any RL task with a set of States, actions, and rewards, that follows the Markov Property, is a MDP</li>
  <li>MDP is defined as the collection of
    <ul>
      <li>set of states</li>
      <li>set of actions</li>
      <li>set of rewards</li>
      <li>State-Transition probability, Reward probability(as defined jointly(è¿å¸¦åœ°) earlier)</li>
      <li>Discount factor</li>
    </ul>
  </li>
  <li>Often written as a 5 tuples</li>
</ul>

<h2 id="7-policy">7. Policy</h2>
<ul>
  <li>One more piece to complete the puzzle - the policy(denoted by Ï€)</li>
  <li>Technically Ï€ is not part of the MDP itself, but it, along with the value function, form the solution</li>
  <li>Left out until now because itâ€™s a weird symbol</li>
  <li>thereâ€™s no â€œequationâ€ for it</li>
  <li>how do we write epsilon-greedy as an equation? itâ€™s more like an algorithm</li>
  <li>the only exception is the <em>Optimal policy</em>, which can be defined in terms of the value function</li>
  <li>think of Ï€ as shorthand for the algorithm the agent is using to navigate the environment</li>
  <li>(Epsilon GreedyëŠ” ì£¼ì‚¬ìœ„ í•œë²ˆ ë˜ì ¸ì„œ ê²°ì •í•˜ê³  ê·¸ action ë‹¤ìŒë²ˆì— ì‚¬ìš©í•˜ëŠ” ê²ƒì´ off Policy)</li>
  <li>(Q-Learningì€ ë‹¤ìŒ Stepì—ì„œ ì‹¤ì œë¡œ ì‚¬ìš©í•  actionê³¼ ìƒê´€ ì—†ì´ Max Q ì·¨í•˜ê¸° ë•Œë¬¸ì— off policy)</li>
</ul>
:ET