I"E<h1 id="n-step-methods">N-step Methods</h1>
<ul>
  <li>N-step Methods : Further our understanding of TD methods</li>
  <li>we know so far： TD(0)</li>
  <li>we will learn :
    <ul>
      <li>λ = 0 gives us TD(0), λ = 1 gives us monte Carlo</li>
    </ul>
  </li>
  <li>any other λ is a trade-off(协调) between the two</li>
</ul>

<p><a href="https://postimg.cc/n9hf3sZq"><img src="https://i.postimg.cc/gjvckRTt/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>
<ul>
  <li>1-Step TD의 step을 증가시켜 나가면서 n까지 보게 된다면 n-step TD로 일반화 할 수 있다.</li>
  <li>만약 step이 무한대에 가깝게 되면 MC와 동일하게 될 것이다.</li>
  <li>2-Step TD에서 업데이트 방식은 척번째 보상과 두번째 보상 그리고 부전째 상태에서의 Value function의 합으로 업데이트가 된다.</li>
</ul>

<p><a href="https://postimg.cc/62Yp6xpL"><img src="https://i.postimg.cc/J0WDq1wg/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li>n step TD에서의 Value 함수는 N-step 에서 얻은 총 보상에서 기존 Value 함수값과 차이를 알파만큼 가중치하여 더함으로서 업데이트가 되게 됩니다.</li>
</ul>

<p><a href="https://postimg.cc/RJtC76TG"><img src="https://i.postimg.cc/prG9tK5N/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>

<p>```
Reference:</p>

<p><a href="https://www.udemy.com/course/artificial-intelligence-reinforcement-learning-in-python/">Artificial Intelligence Reinforcement Learning</a></p>

<p><a href="https://www.udemy.com/deep-reinforcement-learning-in-python/">Advance AI : Deep-Reinforcement Learning</a></p>

<p><a href="https://www.udemy.com/cutting-edge-artificial-intelligence/learn/lecture/14650508#overview">Cutting-Edge Deep-Reinforcement Learning</a></p>
:ET