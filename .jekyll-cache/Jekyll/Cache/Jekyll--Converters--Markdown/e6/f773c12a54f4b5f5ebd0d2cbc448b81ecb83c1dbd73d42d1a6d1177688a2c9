I"õ<h1 id="markov-decision-processes">Markov Decision Processes</h1>
<ul>
  <li>this section: formalize some RL concepts we already know about</li>
  <li>Agent, Environment, action, state, reward, episode</li>
  <li>Formal Framework: Markov Decision Processes(MDPs)</li>
</ul>

<h2 id="1-typical-game-by-solving-mdps-is-gridword">1. Typical Game by solving MDPs is GridWord</h2>
<p><a href="https://postimg.cc/VdS9ym01"><img src="https://i.postimg.cc/FspTLK9f/54123.png" width="500px" title="source: imgur.com" /></a></p>
<ul>
  <li>possible actions:</li>
  <li>up,down,left,right</li>
  <li>(1,1) -&gt; wall,canâ€™t go here</li>
  <li>(0,3) -&gt; Terminal(+1 Reward)</li>
  <li>(1,3) -&gt; Terminal(-1 Reward)</li>
  <li>12 Positions(w x h = 3 x 4 = 12)</li>
  <li>11 states (where the robot is)</li>
  <li>4 actions</li>
</ul>

<h2 id="2-markov-property">2. Markov Property</h2>
<ul>
  <li>Given a sequence:
 <a href="https://postimg.cc/fJKVgPBZ"><img src="https://i.postimg.cc/2Spn3RWq/4121233.png" width="500px" title="source: imgur.com" /></a></li>
  <li>Generally, this canâ€™t be simplified:
 <a href="https://postimg.cc/jw0qj6ZZ"><img src="https://i.postimg.cc/sX1MwwSd/41233.png" width="500px" title="source: imgur.com" /></a></li>
  <li>First-order Markov:
 <a href="https://postimg.cc/bZ4tfBS5"><img src="https://i.postimg.cc/zfGSKmzq/2.png" width="500px" title="source: imgur.com" /></a></li>
  <li>second-order Markov:
 <a href="https://postimg.cc/bDgrrCdp"><img src="https://i.postimg.cc/cCynDPSg/44.png" width="500px" title="source: imgur.com" /></a></li>
  <li>Simple Example
```
Consider the sentence : â€œLetâ€™s do a simple exampleâ€
Given:â€letâ€™s do a simpleâ€
Predict the new word? Easy</li>
</ul>

<p>Given: â€œsimpleâ€
Predict the next word? not as easy</p>

<p>Given: â€œaâ€
predict the next word? very difficult</p>

<p>is the Markov Property limiting? Not necessarily
```</p>

<h2 id="3-markov-property-in-rl">3. Markov Property in RL</h2>
<ul>
  <li>{S(t), A(t)} produces 2 things -&gt; {S(t+1),R(t+1)}</li>
  <li>Markov Property:
<a href="https://postimg.cc/bDgrrCdp"><img src="https://i.postimg.cc/cCynDPSg/44.png" width="500px" title="source: imgur.com" /></a></li>
  <li>Convenience notation
<a href="https://postimg.cc/LJrhqgTQ"><img src="https://i.postimg.cc/g00hTv6p/44.png" width="500px" title="source: imgur.com" /></a></li>
  <li>joint on sâ€™ and r, conditioned on 2 other variables</li>
  <li>different from â€œusualâ€ Markov: 1 RV(Random Variable) Conditioned on 1 other RV</li>
</ul>

<h2 id="4-other-conditional-distributions">4. Other Conditional distributions</h2>
<ul>
  <li>can be found using rules of probability</li>
  <li>For pretty much all cases weâ€™ll consider these will be deterministic</li>
  <li>i.e. states will always give us the same reward</li>
  <li>Action will always bring us to same next state</li>
  <li>But, these distributions are part of the core theory of RL
<a href="https://postimg.cc/BL5dTRGP"><img src="https://i.postimg.cc/W3xTvPW8/44.png" width="500px" title="source: imgur.com" /></a></li>
</ul>

<h2 id="5-is-the-markov-assumption-limiting">5. Is the Markov Assumption limiting?</h2>
<ul>
  <li>Not necessarily</li>
  <li>Recent application: DeepMind used concatenation(ä¸€ç³»åˆ—ç›¸å…³è”çš„äº‹ç‰©) of 4 most recent frames</li>
</ul>
:ET