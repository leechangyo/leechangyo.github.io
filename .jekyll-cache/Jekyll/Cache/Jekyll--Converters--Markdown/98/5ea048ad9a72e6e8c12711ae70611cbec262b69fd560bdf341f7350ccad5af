I"w<h1 id="plugging-in-a-neural-network">Plugging in a neural network</h1>
<ul>
  <li>what might happen if we plugged in a neural network instead of a linear model?</li>
  <li>we have seen how we can plug in a tensorflow model into q_learning.py script(Tensorflow_warmup)</li>
  <li>put them together</li>
  <li>try removing RBF layer since presumably(很可能) a deep neural network will automatically learn features</li>
</ul>

<h2 id="catastrophic灾难性的-forgetting">catastrophic(灾难性的) Forgetting</h2>
<ul>
  <li>lots of attention recently, in relation to “transfer learning”</li>
  <li>train AI on one game, keep some weights, train on another game</li>
  <li>showed the neural net can be trained such that the AI still performs well on the first game</li>
  <li>so it didn’t just learn the 2nd game and forget the 1st game</li>
  <li>this is noteworthy(值得注意的) because it’s not how neural networks work by default</li>
  <li>more typically we would expect the neural net to forget how to play the 1st game</li>
  <li>Doesn’t only apply across different tasks</li>
  <li>Stochastic/ batch gradient descent -&gt; cost can “jump” around</li>
  <li>seems more pronounced on highly nonlinear regression problems</li>
  <li><strong>we’d like the data in cost function to represent true distribution of data</strong></li>
  <li>even using all training data simultaneously is only an approximation of the “true” data</li>
  <li>E.g. clinical trial for a drug, we sample 1000 subjects. we hope they are representative of the population as a whole</li>
  <li>So when we use batch/Stochastic GD, the approximation becomes even worse</li>
  <li>in RL, it’s even worse because there’s a large bias in the ordering of training samples</li>
  <li>we always proceed from start state to end state</li>
  <li>No Randomization, which is recommended in SGD/BGD</li>
  <li>Also, not a true gradient because the target uses the model to make a prediction</li>
</ul>

<h2 id="try-dropout">Try dropout</h2>
<ul>
  <li>Dropout has been shown to help</li>
  <li>some weights will not be updated for any particular training iteration</li>
  <li>try other types of regularization and architectures</li>
</ul>

<h1 id="open-ai-gym--rbf-network-summary">Open AI gym &amp; RBF Network summary</h1>
<ul>
  <li>Building Gradually more and more complex RL agents, using what we already know</li>
</ul>

<h2 id="openai-gym">OpenAI Gym</h2>
<ul>
  <li>how to load an environment</li>
  <li>inspect its state space, action space</li>
  <li>play an episode</li>
  <li>save a video</li>
</ul>

<h2 id="cartpole">CartPole</h2>
<ul>
  <li>Random search</li>
  <li>Easy to conceptualize, no complicated math</li>
  <li><strong>choose random weights until we find something good</strong></li>
</ul>

<h2 id="tabular-q-learning">Tabular Q-Learning</h2>
<ul>
  <li>Q-Learning with binned states</li>
  <li>Allows us to use tabular method</li>
  <li>Re-acquaint(再使熟悉)</li>
</ul>

<p>```
Reference:</p>

<p><a href="https://www.udemy.com/course/artificial-intelligence-reinforcement-learning-in-python/">Artificial Intelligence Reinforcement Learning</a></p>

<p><a href="https://www.udemy.com/deep-reinforcement-learning-in-python/">Advance AI : Deep-Reinforcement Learning</a></p>

<p><a href="https://www.udemy.com/cutting-edge-artificial-intelligence/learn/lecture/14650508#overview">Cutting-Edge Deep-Reinforcement Learning</a></p>
:ET