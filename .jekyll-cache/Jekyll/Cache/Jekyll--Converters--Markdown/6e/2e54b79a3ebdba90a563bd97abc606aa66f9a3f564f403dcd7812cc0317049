I"s<h1 id="markov-decision-processes">Markov Decision Processes</h1>
<ul>
  <li>this section: formalize some RL concepts we already know about</li>
  <li>Agent, Environment, action, state, reward, episode</li>
  <li>Formal Framework: Markov Decision Processes(MDPs)</li>
</ul>

<h2 id="1-typical-game-by-solving-mdps-is-gridword">1. Typical Game by solving MDPs is GridWord</h2>
<p><a href="https://postimg.cc/VdS9ym01"><img src="https://i.postimg.cc/FspTLK9f/54123.png" width="500px" title="source: imgur.com" /></a></p>
<ul>
  <li>possible actions:</li>
  <li>up,down,left,right</li>
  <li>(1,1) -&gt; wall,can’t go here</li>
  <li>(0,3) -&gt; Terminal(+1 Reward)</li>
  <li>(1,3) -&gt; Terminal(-1 Reward)</li>
  <li>12 Positions(w x h = 3 x 4 = 12)</li>
  <li>11 states (where the robot is)</li>
  <li>4 actions</li>
</ul>

<h2 id="2-markov-property">2. Markov Property</h2>
<ul>
  <li>Given a sequence:
 <a href="https://postimg.cc/fJKVgPBZ"><img src="https://i.postimg.cc/2Spn3RWq/4121233.png" width="500px" title="source: imgur.com" /></a></li>
  <li>Generally, this can’t be simplified:
 <a href="https://postimg.cc/jw0qj6ZZ"><img src="https://i.postimg.cc/sX1MwwSd/41233.png" width="500px" title="source: imgur.com" /></a></li>
  <li>First-order Markov:
 <a href="https://postimg.cc/bZ4tfBS5"><img src="https://i.postimg.cc/zfGSKmzq/2.png" width="500px" title="source: imgur.com" /></a></li>
  <li>second-order Markov:
 <a href="https://postimg.cc/bDgrrCdp"><img src="https://i.postimg.cc/cCynDPSg/44.png" width="500px" title="source: imgur.com" /></a></li>
  <li>Simple Example
```
Consider the sentence : “Let’s do a simple example”
Given:”let’s do a simple”
Predict the new word? Easy</li>
</ul>

<p>Given: “simple”
Predict the next word? not as easy</p>

<p>Given: “a”
predict the next word? very difficult</p>

<p>is the Markov Property limiting? Not necessarily
```</p>

<h2 id="3-markov-property-in-rl">3. Markov Property in RL</h2>
<ul>
  <li>{S(t), A(t)} produces 2 things -&gt; {S(t+1),R(t+1)}</li>
  <li>Markov Property:
<a href="https://postimg.cc/bDgrrCdp"><img src="https://i.postimg.cc/cCynDPSg/44.png" width="500px" title="source: imgur.com" /></a></li>
  <li>Convenience notation
<a href="https://postimg.cc/LJrhqgTQ"><img src="https://i.postimg.cc/g00hTv6p/44.png" width="500px" title="source: imgur.com" /></a></li>
  <li>joint on s’ and r, conditioned on 2 other variables</li>
  <li>different from “usual” Markov: 1 RV(Random Variable) Conditioned on 1 other RV</li>
</ul>

<h2 id="4-other-conditional-distributions">4. Other Conditional distributions</h2>
<ul>
  <li>can be found using rules of probability</li>
  <li>For pretty much all cases we’ll consider these will be deterministic</li>
  <li>i.e. states will always give us the same reward</li>
  <li>Action will always bring us to same next state</li>
  <li>But, these distributions are part of the core theory of RL
<a href="https://postimg.cc/BL5dTRGP"><img src="https://i.postimg.cc/W3xTvPW8/44.png" width="500px" title="source: imgur.com" /></a></li>
</ul>

<h2 id="5-is-the-markov-assumption-limiting">5. Is the Markov Assumption limiting?</h2>
<ul>
  <li>Not necessarily</li>
  <li>Recent application: DeepMind used concatenation(一系列相关联的事物) of 4 most recent frames to represent state when playing Atari Games</li>
  <li>State can be made up of anything from anytime past to current</li>
  <li>Typically think of state right now = something we measure right now</li>
  <li>Also, don’t need to use raw data(state can be features transformed from raw data)</li>
  <li>Any input from agent’s sensors can be used to form state</li>
</ul>

<h2 id="6-markov-decision-processesmdps">6. Markov Decision Processes(MDPs)</h2>
<ul>
  <li>Any RL task with a set of States, actions, and rewards, that follows the Markov Property, is a MDP</li>
  <li>MDP is defined as the collection of
    <ul>
      <li>set of states</li>
      <li>set of actions</li>
      <li>set of rewards</li>
      <li>State-Transition probability, Reward probability(as defined jointly(连带地) earlier)</li>
      <li>Discount factor</li>
    </ul>
  </li>
  <li>Often written as a 5 tuple</li>
</ul>
:ET