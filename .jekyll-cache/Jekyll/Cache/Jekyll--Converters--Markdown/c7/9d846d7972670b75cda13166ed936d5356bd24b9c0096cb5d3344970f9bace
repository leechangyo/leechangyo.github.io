I"‰<h1 id="montel-carlomc-methods-introduction">Montel Carlo(MC) Methods Introduction</h1>
<ul>
  <li>Last Section: Dynamic Programming</li>
  <li>would that work for self-driving cars or video games?</li>
  <li>can i just set the state of the agent?</li>
  <li>‚Äúgod-mode‚Äù capabilities?</li>
  <li><strong>MC Methods learn purely from experience</strong></li>
  <li>Montel Carlo usually refers to any method with a significant random component</li>
  <li>Random Component in RL is the return</li>
  <li>With MC, instead of calculating the true expected value of G, <strong>we calculate its sample mean</strong>
<a href="https://postimg.cc/6yrZL1Rr"><img src="https://i.postimg.cc/c4jcWpLb/22222.png" width="500px" title="source: imgur.com" /></a></li>
  <li>Need to assume episode tasks only</li>
  <li>Episode must terminate before we calculate return</li>
  <li>Not ‚Äúfully‚Äù online since we need to wait for entire episode to finish before updating</li>
  <li>(full online mean is update after every action)</li>
  <li>monte carlo methods is not fully online which mean it is updated after episode to finish</li>
  <li>Should Remind you of multi-armed bandit</li>
  <li><strong>Multi-Armed bandit : average reward after every action</strong></li>
  <li>MDPs: average the return</li>
  <li>One way to think of MC-MDP is every state is a separate multi-armed bandit problem</li>
  <li>Follow the same pattern</li>
  <li>Prediction Problem(Finding Value given policy)</li>
  <li>Control Problem(finding optimal policy)</li>
</ul>

<h2 id="1-monte-carlo-for-prediction-problem">1. Monte Carlo for prediction Problem</h2>
<ul>
  <li>Recall what V(s) is :
 <a href="https://postimg.cc/t1yqhpWb"><img src="https://i.postimg.cc/hGQXRGGX/4124131233.png" width="500px" title="source: imgur.com" /></a></li>
  <li>Any expected value can be approximated like
<a href="https://postimg.cc/rzYYWjxF"><img src="https://i.postimg.cc/dtqMbS0d/41241231321312.png" width="500px" title="source: imgur.com" /></a></li>
  <li>‚Äúi‚Äù is episode, ‚Äút‚Äù is steps</li>
</ul>

<h3 id="how-do-we-generate-g">How do we generate G?</h3>
<ul>
  <li>just play a bunch of episode, log the states and reward sequences
    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>s1 s2 s3 ... sT
r1 r2 r3 ... rT
</code></pre></div>    </div>
  </li>
  <li>
    <p>Calculate G from Definition:
G(t) = r(t+1)+gamma*G(t+1)</p>
  </li>
  <li>very helpful to calculate G by iterating through states in reverse order</li>
  <li>Once we have(s,G) pairs, average them for each s</li>
</ul>

<h3 id="multiple-visits-to-s">Multiple Visits to s</h3>
<ul>
  <li>what if we see the same state more than once in an episode</li>
  <li>E.g. we see s at t=1 and t=3</li>
  <li>which return should we use? G(1) or G(3</li>
  <li>First-visit method :
    <ul>
      <li>Use t = 1 only</li>
    </ul>
  </li>
  <li>Every-visit method :
    <ul>
      <li>Use both t=1 and t=3 as samples</li>
    </ul>
  </li>
  <li>Surprisingly, it has been proven that both lead to same answer</li>
</ul>

<blockquote>
  <p>First-Visit MC Pseudocode</p>
  <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def first_visit_monte_carlo_prediction(œÄ, N):
  V = random initialization
  all_return = {} # default = []
  do N times:
    states, returns = play_episode
    for s, g in zip(states, returns):
      if not seen s in this episode yet:
        all_return[s].append(g)
        V(s) = sample_mean(all_returns[s])
</code></pre></div>  </div>
</blockquote>
:ET