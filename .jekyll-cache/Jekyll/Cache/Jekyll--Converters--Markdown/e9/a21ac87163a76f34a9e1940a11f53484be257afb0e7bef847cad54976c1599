I"œn<h1 id="intro-to-dynamic-programming">intro to Dynamic Programming</h1>
<ul>
  <li>Solutions to MDPs</li>
  <li>Centrepiece of MDP: The bellman Equation
<a href="https://postimg.cc/5XWK8Hhk"><img src="https://i.postimg.cc/zGymZK05/4141313123.png" width="500px" title="source: imgur.com" /></a></li>
  <li>if we look carefully, this can be used to solve for V(s) Directly</li>
  <li>I S I equations, I S I unknowns(linear Problem)</li>
  <li>Many entries(ËøõÂÖ•(ÊåáË°åÂä®)) will be 0, since transitions s -&gt; s‚Äô are sparse</li>
  <li>Instead, we will use <strong>Iterative Policy evaluation</strong></li>
</ul>

<h2 id="1-iterative-policy-evaluation">1. Iterative Policy evaluation</h2>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def iterative_policy_evaluation(œÄ):
  initialize V(s) = 0 for all s ‚àà S
  while true :
    ‚àÜ = 0
    for each s ‚àà S:
      old_v = V(s)
</code></pre></div></div>

<p><a href="https://postimg.cc/cvDD2DKP"><img src="https://i.postimg.cc/cJWVK2d1/4121241314123.png" width="500px" title="source: imgur.com" /></a></p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>     ‚àÜ = max(‚àÜ, I V(s) - old_v I )
     if ‚àÜ &lt; Threshold: break

     return V(s)
</code></pre></div></div>

<ul>
  <li>Technically, it‚Äôs defined where V(s) at iteration k+1 is updated from V(s) at iteration k</li>
  <li>But we can update V(s) ‚Äúin place‚Äù, to use the most recently updated values</li>
  <li>Converges(Ê±áÈõÜ) Faster
<a href="https://postimg.cc/0MJPbKR2"><img src="https://i.postimg.cc/Ssgs1cg8/51212.png" width="500px" title="source: imgur.com" /></a></li>
</ul>

<h3 id="definition">Definition</h3>
<ul>
  <li>What we just did(Finding V(s) given a policy) is called the <strong>Prediction Problem</strong></li>
  <li>Finding the optimal policy is called the <strong>Control Problem</strong></li>
</ul>

<h2 id="2-designing-our-rl-program">2. Designing our RL Program</h2>
<ul>
  <li>Let‚Äôs recap how to do this in supervised learning</li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>class MyModel;
  def fit(x,y):
    # our job
  def predict(x)
    # our job
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># boilerplate
Xtrain, Ytrain, Xtest, Ytest = get_data() # 1. Get Data
Model = MyModel() # 2. Instantiate Model
model.fit(Xtrain,Ytrain) # 3. Train Model
model.score(Xtest, ytest) # 4. Evaluation Model
</code></pre></div></div>

<ul>
  <li>RL Program is not supervised learning but there is still a pattern to be followed</li>
  <li>Same as bandit section: several algorithms, but all have the same interface</li>
  <li>Only the algorithm is different, not the layout</li>
  <li>Applies to all the RL algorithms</li>
  <li>Designing our RL Program, there are t types of problems
    <ol>
      <li>Prediction ProblemÔºö Given a policy, find V(s)</li>
    </ol>
    <ul>
      <li>Goal: Find V(s)
        <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>given: policy
V(s) = initial value
for t in range(max_iterations):
states, actions, rewards = play_game(policy)
update V(s) given (state, actions, rewards)
print useful info (change in V(s) vs time, final V(s), policy)
</code></pre></div>        </div>
        <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>2. Control Problem : Find the optimal policy and the corresponding value function
</code></pre></div>        </div>
      </li>
      <li>Goal : find the optimal Policy</li>
      <li>note : Policy may not be explicitly represented
        <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>initialize value function and Policy
for t in rangeÔºàmax_iterations):
states, actions, rewards = play_game(policy)
update Value function and policy to (states, actions,rewards) using the algorithm
print useful info (change in V(s) vs time, final V(s), final policy)
</code></pre></div>        </div>
        <script async="" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

        <!-- horizon2 -->
        <p><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-5150894678574694" data-ad-slot="5130167867" data-ad-format="auto" data-full-width-responsive="true"></ins>
<script>
 (adsbygoogle = window.adsbygoogle || []).push({});
</script></p>
      </li>
    </ul>
  </li>
</ul>

<h2 id="3-iterative-policy-evaluation">3. Iterative Policy Evaluation</h2>
<ul>
  <li>we will look at 2 different Policies</li>
  <li>First: Completely random(Uniform) policy</li>
  <li>which is relavant?
    <ul>
      <li>œÄ(a I s)</li>
      <li>p(s‚Äô,r I s,a)</li>
    </ul>
  </li>
  <li>Answer : <strong>œÄ(a I s)</strong></li>
  <li>for a uniformly random policy, this will be 1/IA(s)I</li>
  <li>A(s) = set of possible actions from state s</li>
  <li>
    <p>p(s‚Äô,r I s,a) is only relevant when state transitions are random</p>
  </li>
  <li>Second: policy we we‚Äôll look at is Completely deterministic</li>
  <li>from start position, we go Directly to goal</li>
  <li>otherwise, we go Directly to losing state</li>
</ul>

<h2 id="4-policy-improvement">4. Policy Improvement</h2>
<ul>
  <li><strong>THe control Problem</strong></li>
  <li>How to find better policies -&gt; optimal policy</li>
  <li>what we know so far : how to find V/Q given a fixed policy
<a href="https://postimg.cc/1nt01PFQ"><img src="https://i.postimg.cc/RF1gfZVN/521151.png" width="500px" title="source: imgur.com" /></a></li>
  <li>using the current policy, we simply get the state-value function
<a href="https://postimg.cc/Y4SLhRCN"><img src="https://i.postimg.cc/BvTT7wWz/1241413123.png" width="500px" title="source: imgur.com" /></a></li>
  <li>can we change just this one action for s? i.e. choose a != œÄ(s)</li>
  <li>Yes</li>
  <li>we have a finite set of actions, so just go through each one until we get a better Q
<a href="https://postimg.cc/0b8kkQQD"><img src="https://i.postimg.cc/WbZh7qcW/4124131231131.png" width="500px" title="source: imgur.com" /></a></li>
  <li>this looks complicated, but it‚Äôs very simple</li>
  <li>all it‚Äôs saying is, if the policy for state s is to currently go ‚Äúup‚Äù</li>
  <li>just check ‚Äúleft‚Äù, ‚Äúright‚Äù, and ‚Äúdown‚Äù to see if we can get a bigger Q</li>
  <li>if so, change our policy for state <strong>s</strong> to the <strong>new action</strong></li>
  <li>Formally, we‚Äôre finding a new policy œÄ‚Äô, that gives us a givver value than we had before:
<a href="https://postimg.cc/7bSMzxkK"><img src="https://i.postimg.cc/66mHD31x/41231231231231221.png" width="500px" title="source: imgur.com" /></a></li>
  <li>if we have Q:
<a href="https://postimg.cc/t1VDmWKj"><img src="https://i.postimg.cc/g0g5jK4n/4124123123123.png" width="500px" title="source: imgur.com" /></a></li>
  <li>if we have V:
<a href="https://postimg.cc/21g7v3VX"><img src="https://i.postimg.cc/9fQ867VV/4124213123.png" width="500px" title="source: imgur.com" /></a></li>
  <li>Notice : it‚Äôs greedy</li>
  <li>we never consider globally the value function at all states</li>
  <li>only look at current state s</li>
  <li>notice: it uses an imperfect version of $V_œÄ$(s)</li>
  <li>once we change œÄ. $V_œÄ$(s) also changes</li>
  <li>when are we finished changing the policy, it doesn‚Äôt change when we try policy Improvement, V(s) also doesn‚Äôt change</li>
  <li>‚Äù&lt;‚Äù when still improving = when finished
<a href="https://postimg.cc/7bSMzxkK"><img src="https://i.postimg.cc/66mHD31x/41231231231231221.png" width="500px" title="source: imgur.com" /></a></li>
  <li>if we found optimal policy, Value function is always same in state</li>
</ul>

<h2 id="5-policy-iteration">5. Policy Iteration</h2>
<ul>
  <li>this is what use to find the optimal policy</li>
  <li>problem we encountered last lecture: when we change the policy, the value function becomes out of date</li>
  <li>How do we fix the out-of-date value function?</li>
  <li>simply recalculate it given the new policy</li>
  <li>we already know how to find V given œÄ:
    <ul>
      <li>iterative policy Evaluation</li>
    </ul>
  </li>
  <li>High-level: alternate between policy evaluation and policy improvement</li>
  <li>keep doing this until policy doesn‚Äôt change</li>
  <li>Don‚Äôt need to check value function for convergence, because once policy becomes constant so will value</li>
</ul>

<h3 id="step-of-policy-iteration">Step of Policy Iteration</h3>
<ol>
  <li>Randomly initialize V(s) and the policy œÄ(s)</li>
  <li>V(s) = iterative_policy_evaluation(œÄ)</li>
  <li>algorithm</li>
</ol>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>policy_changed = False
for s in all_state:
  old_a = policy(s)
  policy(s) = argmax[a]{sum[s',r]{p(s',r I s, a )[ r+ gamma*V(s')]}}
  if policy(s) != old_a: policy_changed = true
if policy_changed: go back to step 2
</code></pre></div></div>

<h2 id="6-example-windy-gridworld">6. Example (Windy Gridworld)</h2>
<ul>
  <li>recall, we have 2 probability distributions to deal with:
    <ul>
      <li>œÄ(a I s)</li>
      <li>p(s‚Äô,r I s,a)</li>
    </ul>
  </li>
  <li>so far, p(s‚Äô,r I s,a) has been deterministic</li>
  <li>in windy Gridworld, it‚Äôs not</li>
  <li>imagine we are walking on a windy street. we try to walk straight but wind push we back or left.</li>
  <li>same thins in windy Gridworld</li>
  <li>if agent tries to go ‚Äúup‚Äù, it will do so with probability 0.5</li>
  <li>But it can also go ‚Äúleft‚Äù, ‚Äúdown‚Äù, or ‚Äúright‚Äù with probability 0.5/3</li>
</ul>

<blockquote>
  <p>code</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># https://deeplearningcourses.com/c/artificial-intelligence-reinforcement-learning-in-python
# https://www.udemy.com/artificial-intelligence-reinforcement-learning-in-python
</span><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">print_function</span><span class="p">,</span> <span class="n">division</span>
<span class="kn">from</span> <span class="nn">builtins</span> <span class="kn">import</span> <span class="nb">range</span>
<span class="c1"># Note: you may need to update your version of future
# sudo pip install -U future
</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">grid_world</span> <span class="kn">import</span> <span class="n">standard_grid</span><span class="p">,</span> <span class="n">negative_grid</span>
<span class="kn">from</span> <span class="nn">iterative_policy_evaluation</span> <span class="kn">import</span> <span class="n">print_values</span><span class="p">,</span> <span class="n">print_policy</span>

<span class="n">SMALL_ENOUGH</span> <span class="o">=</span> <span class="mf">1e-3</span>
<span class="n">GAMMA</span> <span class="o">=</span> <span class="mf">0.9</span>
<span class="n">ALL_POSSIBLE_ACTIONS</span> <span class="o">=</span> <span class="p">(</span><span class="s">'U'</span><span class="p">,</span> <span class="s">'D'</span><span class="p">,</span> <span class="s">'L'</span><span class="p">,</span> <span class="s">'R'</span><span class="p">)</span>

<span class="c1"># next state and reward will now have some randomness
# you'll go in your desired direction with probability 0.5
# you'll go in a random direction a' != a with probability 0.5/3
</span>
<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">'__main__'</span><span class="p">:</span>
  <span class="c1"># this grid gives you a reward of -0.1 for every non-terminal state
</span>  <span class="c1"># we want to see if this will encourage finding a shorter path to the goal
</span>  <span class="n">grid</span> <span class="o">=</span> <span class="n">negative_grid</span><span class="p">(</span><span class="n">step_cost</span><span class="o">=-</span><span class="mf">1.0</span><span class="p">)</span>
<span class="c1"># what is the step_cost? for example, if we are three steps away from the goal
# we get minus three reward before we even have a chance to get to the goal
# if we are only one step away from the losing state, then we only get minus one reward
</span>  <span class="c1"># grid = negative_grid(step_cost=-0.1)
</span>  <span class="c1"># grid = standard_grid()
</span>
  <span class="c1"># print rewards
</span>  <span class="k">print</span><span class="p">(</span><span class="s">"rewards:"</span><span class="p">)</span>
  <span class="n">print_values</span><span class="p">(</span><span class="n">grid</span><span class="o">.</span><span class="n">rewards</span><span class="p">,</span> <span class="n">grid</span><span class="p">)</span>

  <span class="c1"># state -&gt; action
</span>  <span class="c1"># we'll randomly choose an action and update as we learn
</span>  <span class="n">policy</span> <span class="o">=</span> <span class="p">{}</span>
  <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">grid</span><span class="o">.</span><span class="n">actions</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
    <span class="n">policy</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">ALL_POSSIBLE_ACTIONS</span><span class="p">)</span>

  <span class="c1"># initial policy
</span>  <span class="k">print</span><span class="p">(</span><span class="s">"initial policy:"</span><span class="p">)</span>
  <span class="n">print_policy</span><span class="p">(</span><span class="n">policy</span><span class="p">,</span> <span class="n">grid</span><span class="p">)</span>

  <span class="c1"># initialize V(s)
</span>  <span class="n">V</span> <span class="o">=</span> <span class="p">{}</span>
  <span class="n">states</span> <span class="o">=</span> <span class="n">grid</span><span class="o">.</span><span class="n">all_states</span><span class="p">()</span>
  <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">states</span><span class="p">:</span>
    <span class="c1"># V[s] = 0
</span>    <span class="k">if</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">grid</span><span class="o">.</span><span class="n">actions</span><span class="p">:</span>
      <span class="n">V</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="c1"># terminal state
</span>      <span class="n">V</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

  <span class="c1"># repeat until convergence - will break out when policy does not change
</span>  <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>

    <span class="c1"># policy evaluation step - we already know how to do this!
</span>    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
      <span class="n">biggest_change</span> <span class="o">=</span> <span class="mi">0</span>
      <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">states</span><span class="p">:</span>
        <span class="n">old_v</span> <span class="o">=</span> <span class="n">V</span><span class="p">[</span><span class="n">s</span><span class="p">]</span>
<span class="c1">#         print(old_v)
</span>
        <span class="c1"># V(s) only has value if it's not a terminal state
</span>        <span class="n">new_v</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">if</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">policy</span><span class="p">:</span>
          <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">ALL_POSSIBLE_ACTIONS</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">a</span> <span class="o">==</span> <span class="n">policy</span><span class="p">[</span><span class="n">s</span><span class="p">]:</span>
              <span class="n">p</span> <span class="o">=</span> <span class="mf">0.5</span>
            <span class="k">else</span><span class="p">:</span>
              <span class="n">p</span> <span class="o">=</span> <span class="mf">0.5</span><span class="o">/</span><span class="mi">3</span>
            <span class="n">grid</span><span class="o">.</span><span class="n">set_state</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
            <span class="n">r</span> <span class="o">=</span> <span class="n">grid</span><span class="o">.</span><span class="n">move</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
            <span class="n">new_v</span> <span class="o">+=</span> <span class="n">p</span><span class="o">*</span><span class="p">(</span><span class="n">r</span> <span class="o">+</span> <span class="n">GAMMA</span> <span class="o">*</span> <span class="n">V</span><span class="p">[</span><span class="n">grid</span><span class="o">.</span><span class="n">current_state</span><span class="p">()])</span>
          <span class="n">V</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_v</span>
          <span class="n">biggest_change</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">biggest_change</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">old_v</span> <span class="o">-</span> <span class="n">V</span><span class="p">[</span><span class="n">s</span><span class="p">]))</span>

      <span class="k">if</span> <span class="n">biggest_change</span> <span class="o">&lt;</span> <span class="n">SMALL_ENOUGH</span><span class="p">:</span>
        <span class="k">break</span>

    <span class="c1"># policy improvement step
</span>    <span class="n">is_policy_converged</span> <span class="o">=</span> <span class="bp">True</span>
    <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">states</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">policy</span><span class="p">:</span>
        <span class="n">old_a</span> <span class="o">=</span> <span class="n">policy</span><span class="p">[</span><span class="n">s</span><span class="p">]</span>
        <span class="n">new_a</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="n">best_value</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s">'-inf'</span><span class="p">)</span>
        <span class="c1"># loop through all possible actions to find the best current action
</span>        <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">ALL_POSSIBLE_ACTIONS</span><span class="p">:</span> <span class="c1"># chosen action
</span>          <span class="n">v</span> <span class="o">=</span> <span class="mi">0</span>
          <span class="k">for</span> <span class="n">a2</span> <span class="ow">in</span> <span class="n">ALL_POSSIBLE_ACTIONS</span><span class="p">:</span> <span class="c1"># resulting action
</span>            <span class="k">if</span> <span class="n">a</span> <span class="o">==</span> <span class="n">a2</span><span class="p">:</span>
              <span class="n">p</span> <span class="o">=</span> <span class="mf">0.5</span>
            <span class="k">else</span><span class="p">:</span>
              <span class="n">p</span> <span class="o">=</span> <span class="mf">0.5</span><span class="o">/</span><span class="mi">3</span>
            <span class="n">grid</span><span class="o">.</span><span class="n">set_state</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
            <span class="n">r</span> <span class="o">=</span> <span class="n">grid</span><span class="o">.</span><span class="n">move</span><span class="p">(</span><span class="n">a2</span><span class="p">)</span>
            <span class="n">v</span> <span class="o">+=</span> <span class="n">p</span><span class="o">*</span><span class="p">(</span><span class="n">r</span> <span class="o">+</span> <span class="n">GAMMA</span> <span class="o">*</span> <span class="n">V</span><span class="p">[</span><span class="n">grid</span><span class="o">.</span><span class="n">current_state</span><span class="p">()])</span>
          <span class="k">if</span> <span class="n">v</span> <span class="o">&gt;</span> <span class="n">best_value</span><span class="p">:</span>
            <span class="n">best_value</span> <span class="o">=</span> <span class="n">v</span>
            <span class="n">new_a</span> <span class="o">=</span> <span class="n">a</span>
        <span class="n">policy</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_a</span>
        <span class="k">if</span> <span class="n">new_a</span> <span class="o">!=</span> <span class="n">old_a</span><span class="p">:</span>
          <span class="n">is_policy_converged</span> <span class="o">=</span> <span class="bp">False</span>

    <span class="k">if</span> <span class="n">is_policy_converged</span><span class="p">:</span>
      <span class="k">break</span>

  <span class="k">print</span><span class="p">(</span><span class="s">"values:"</span><span class="p">)</span>
  <span class="n">print_values</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">grid</span><span class="p">)</span>
  <span class="k">print</span><span class="p">(</span><span class="s">"policy:"</span><span class="p">)</span>
  <span class="n">print_policy</span><span class="p">(</span><span class="n">policy</span><span class="p">,</span> <span class="n">grid</span><span class="p">)</span>
  <span class="c1"># result: every move is as bad as losing, so lose as quickly as possible
</span></code></pre></div></div>

<h2 id="6-value-iteration">6. Value Iteration</h2>
<ul>
  <li>Alternative technique for solving the <strong>control Problem</strong> called value iteration</li>
  <li>Previous technique: policy Iteration</li>
  <li>Disadvantage of policy iteration:
    <ul>
      <li>iterative algorithm
        <ul>
          <li>inside another iterative algorithm</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Value Iteration is that Policy evaluation step ends when V coverages</li>
  <li>is there a point before V Converges, s.t the resulting greedy policy wouldn‚Äôt change?</li>
  <li>Yes</li>
  <li>therefore, we don‚Äôt need to wait for policy evaluation to finish. just do a few steps</li>
  <li>because the policy improvement step will find the same policy anyway</li>
  <li>Value iteration takes this one step further</li>
  <li>it combines policy evaluation and policy improvement into one step:
<a href="https://postimg.cc/bsP2bky5"><img src="https://i.postimg.cc/ZKBPGx1J/4121313132.png" width="500px" title="source: imgur.com" /></a></li>
  <li>what is the difference? <em>taking the max over all possible action</em></li>
  <li>Iterative, but don‚Äôt need to wait for (k)th iteration of V finish before calculating (k+1)th</li>
  <li>just update it ‚Äúin-place(Âú®Ê≠£Á°ÆÁöÑ‰ΩçÁΩÆ)‚Äù as before</li>
  <li>since policy improvement uses <strong>argmax</strong>, by taking the max, we‚Äôre just doing the next policy evaluation step without calculating policy explicitly
<a href="https://postimg.cc/r0tQdxVm"><img src="https://i.postimg.cc/bJmMB94Q/41242131312.png" width="500px" title="source: imgur.com" /></a>
<a href="https://postimg.cc/SjkgQ20y"><img src="https://i.postimg.cc/NGrS4mpr/1221.png" width="500px" title="source: imgur.com" /></a>
<a href="https://postimg.cc/WDxxf2td"><img src="https://i.postimg.cc/MTGxHHSm/141123131.png" width="200px" title="source: imgur.com" /></a>
<a href="https://postimg.cc/5QVTbnB1"><img src="https://i.postimg.cc/L5YpkWhq/4124141241312.png" width="500px" title="source: imgur.com" /></a></li>
</ul>

<h2 id="7-summary">7. Summary</h2>
<ul>
  <li>Last section : Defined Markov Decision Process</li>
  <li>This section : one method for finding solutions to MDP: Dynamic Programming</li>
  <li><strong>Prediction Problem</strong>Ôºö Iterative Policy evaluation</li>
  <li><strong>Control Problem</strong>: Policy iteration, Value iteration
    <ul>
      <li>finding the optimal policy and optimal value function</li>
    </ul>
  </li>
</ul>

<h3 id="asynchronous‰∏çÂêåÊó∂Â≠òÂú®-dynamic-programming">Asynchronous(‰∏çÂêåÊó∂Â≠òÂú®) Dynamic Programming</h3>
<ul>
  <li>Every DP algorithm we looked at involved looping through entire set of states</li>
  <li>Recall that for many realistic games, state space is ridiculously large</li>
  <li>thus, even one iteration can take a long time</li>
  <li>one way to speed up: update V(s) ‚Äúin-place‚Äù</li>
  <li>we can take that one step further: Asynchronous(‰∏çÂêåÊó∂Â≠òÂú®) Dynamic Programming</li>
  <li>instead of looping through all states, loop through a few or only one</li>
  <li>Choose based one which states are most-visited
    <ul>
      <li>can be learned by playing the game</li>
    </ul>
  </li>
</ul>

<h3 id="generalized-policy-iteration">Generalized Policy iteration</h3>
<ul>
  <li>Main concept behind policy iteration: we iteratively alternate between 2 steps - policy evaluation and policy improvement</li>
  <li>Converge when bellman‚Äôs equation becomes true(i.e. V(s) = right-hand side)
<a href="https://postimg.cc/ctKCG7Ss"><img src="https://i.postimg.cc/L5D1fvQZ/412312313.png" width="500px" title="source: imgur.com" /><a></a></a></li>
</ul>

<h3 id="efficiency-of-dp">Efficiency of DP</h3>
<ul>
  <li>Consider how long it would take to do brute force search</li>
  <li>. # States = N, # actions = M</li>
  <li>if we assume we can go from start to goal state in <strong>O(N) time</strong>, then we want to explore action <strong>sequences of length O(N)</strong></li>
  <li>M x M x ‚Ä¶ x M</li>
  <li>we did this problem earlier in tic-tac-toe section</li>
  <li>
    <h1 id="of-possible-permutationÊéíÂàóÊñπÂºè-is-omn">of possible permutation(ÊéíÂàó(ÊñπÂºè)) is O(M^n)</h1>
  </li>
  <li>Once we generate all possible sequences, do policy evaluation on all to find the best V(s)</li>
  <li>exponential in # of states</li>
</ul>

<h3 id="model-based-vs-model-free">Model-based vs Model-free</h3>
<ul>
  <li>Notice how DP requires full model of the environment</li>
  <li>In Particular, p(s‚Äô,r I s,a)</li>
  <li>In the real world, these may be hard to measuring, especially if ISI is large</li>
  <li>next sections will look at methods which don‚Äôt require such a model - called <strong>model-free</strong> methods</li>
  <li>also notice there iterative methods requires an initial estimate</li>
  <li>we make estimates from other estimates (V(s) and policy )</li>
  <li>Making an initial estimate is called <strong>Bootstrapping</strong></li>
  <li>Monte Carlo (MC) Does not require boot Bootstrapping</li>
  <li>Temporal difference(TD) learning does</li>
</ul>

<p>Reference:</p>

<p><a href="https://www.udemy.com/course/artificial-intelligence-reinforcement-learning-in-python/">Artificial Intelligence Reinforcement Learning</a></p>

<p><a href="https://www.udemy.com/deep-reinforcement-learning-in-python/">Advance AI : Deep-Reinforcement Learning</a></p>

<p><a href="https://www.udemy.com/cutting-edge-artificial-intelligence/learn/lecture/14650508#overview">Cutting-Edge Deep-Reinforcement Learning</a></p>
:ET