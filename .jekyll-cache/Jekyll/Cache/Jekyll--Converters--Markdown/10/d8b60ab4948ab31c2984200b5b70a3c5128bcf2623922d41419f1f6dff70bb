I"N<h1 id="optimization-in-signal-and-image-processing">Optimization in Signal and Image processing</h1>

<h2 id="model-arising-in-compressive-sensing-and-imaging-sciences">Model arising in compressive sensing and imaging sciences</h2>

<ul>
  <li>Separable(可分开的) Structures of problems</li>
</ul>

<p><a href="https://postimg.cc/zL2TZB2S"><img src="https://i.postimg.cc/vBbhnxWj/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li>Convexity and easy subproblems(子问题).</li>
  <li>Often involving large, nonsmooth convex functions with separable structure.</li>
  <li><a href="https://math.stackexchange.com/questions/2201384/what-is-the-definition-of-a-first-order-method">First-order method</a> can be very slow for producing high accuracy solutions, but also share many advantages:
    <ul>
      <li>Non-differentiability</li>
      <li>Use minimal information, e.g.,(f;∇f)</li>
      <li>Often lead to very simple and “cheap” iterative schemes</li>
      <li><strong>Complexity/iteration mildly dependent (e.g., linear) in problem’s dimension</strong></li>
      <li>Suitable when high accuracy is not crucial [in many large scale applications, the data is anyway corrupted or known only roughly</li>
    </ul>
  </li>
</ul>

<h2 id="example-compressive-sensingkernel-같은-개념">Example: compressive sensing(Kernel 같은 개념)</h2>
<ul>
  <li>Goal: Recover sparse signal from very few linear measurements.
<a href="https://postimg.cc/gX55JGj3"><img src="https://i.postimg.cc/WzbP53PY/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></li>
  <li>Basis pursuit model:
<a href="https://postimg.cc/gLFDQjrt"><img src="https://i.postimg.cc/3NWcyDhh/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></li>
</ul>

<h2 id="compressive-sensing">Compressive Sensing</h2>
<ul>
  <li>Find the sparest solution
    <ul>
      <li>Given n=256, m=128.
        <ul>
          <li>n is original signal</li>
          <li>m is output value</li>
        </ul>
      </li>
      <li>A = randn(m,n); u = sprandn(n, 1, 0.1); b = A*u;
<a href="https://postimg.cc/pmT6D9jK"><img src="https://i.postimg.cc/3xCQQmJf/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></li>
    </ul>
  </li>
</ul>

<h2 id="sparse흔히-넓은-지역에-분포된-정도가-드문-밀도가-희박한-recovery-models">Sparse((흔히 넓은 지역에 분포된 정도가) 드문, (밀도가) 희박한) recovery models</h2>
<p><a href="https://postimg.cc/YhjPFGYY"><img src="https://i.postimg.cc/wTkYSDBf/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>

<h2 id="sparsity흔히-넓은-지역에-분포된-정도가-드문-밀도가-희박한-under-a-basis">Sparsity((흔히 넓은 지역에 분포된 정도가) 드문, (밀도가) 희박한) under a basis</h2>
<ul>
  <li>Given a reprenting basis or dictionary, we want to recover a signal which is sparse under this basis.</li>
  <li>Two types of models:</li>
</ul>

<p><a href="https://postimg.cc/1gGk58N6"><img src="https://i.postimg.cc/HLRdz5YS/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li>Commonly used dictionaries include both analytic and trained ones.
    <ul>
      <li>analytic bases: Id, DCT, wavelets, curvelets, gabor, etc., also their combinations; they have analytic properties, often easy to compute (for example, multiplying a vector takes O(n log n) instead of O($n^2$))
        <h1 id="reference">Reference</h1>
      </li>
    </ul>
  </li>
</ul>

<p><a href="https://web.stanford.edu/class/ee364a/lectures.html"> Optimization method - Standford University </a></p>
:ET