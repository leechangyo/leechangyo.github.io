I"“4<h1 id="openai-gym-tutorial">OpenAI Gym Tutorial</h1>
<ul>
  <li>Tutorial on the basics of Open AI Gym</li>
  <li>install gym : pip install openai</li>
  <li>what we‚Äôll do:
    <ul>
      <li>Connect to an environment</li>
      <li>Play an episode with purely random actions</li>
    </ul>
  </li>
  <li>Purpose: Familiarize ourselves with the API</li>
</ul>

<h2 id="import-gym">Import Gym</h2>
<ul>
  <li>First things :
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">gym</span>
</code></pre></div>    </div>
  </li>
</ul>

<h2 id="get-our-environment">Get our environment</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s">'CartPole-v0'</span><span class="p">)</span>
</code></pre></div></div>
<ul>
  <li>Full list: https://gym.openai.com/envs</li>
  <li>has short blurbs(ÁÆÄ‰ªã)
    <ul>
      <li>what the task is</li>
      <li>whether it‚Äôs been solved</li>
      <li>leaderboard</li>
    </ul>
  </li>
</ul>

<h2 id="start-an-episode">Start an episode</h2>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># put ourselves in the start state
# it also return the state
</span><span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<span class="c1"># out: array([-0.04533, -0.032314, -0.0146921, 0.04151])
</span></code></pre></div></div>

<h2 id="what-is-the-state">What is the state?</h2>
<ul>
  <li>what do these numbers mean?</li>
  <li>https://github.com/openai/gym/wiki/CartPole-v0</li>
  <li>Documentation is somewhat sparse
<a href="https://imgur.com/PCxa56M"><img src="https://i.imgur.com/PCxa56M.png" width="500px" title="source:imgur.com" /><a></a></a></li>
</ul>

<h2 id="in-the-console">In the console</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Box</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span>
<span class="c1"># In : Box
# out : box(4,)
# in box.
# box.contains box.high, box.simple, box.to_jsonable, box.from_jasonable, box.low , box.shape
</span></code></pre></div></div>
<ul>
  <li>The observation_space defines the structure of the observations our environment will be returning. Learning agents usually need to know this before they start running, in order to set up the policy function. some general-purpose learning agents can bandle a wide range of overvation types: Discrete, box, or pixels(which is usually a Box(0,255,[height,width,3])for RGB pixels)</li>
</ul>

<h2 id="action-space">Action Space</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">env</span><span class="o">.</span><span class="n">action_space</span>
<span class="c1"># in : env.action_space
# out : Discrete(2)
# in : env.action_space
# env.action_space.contains, env.action_space.n, env.action_space.to_jsonable, env.action_space.form_jsonable, env.action_space.sample
</span></code></pre></div></div>

<h2 id="play-an-episode">play an episode</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">observation</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
</code></pre></div></div>
<ul>
  <li>Typically ignore info, since it can‚Äôt be used in submission(Â±àÊúç)(although it‚Äôs possible it can help training)</li>
</ul>

<h2 id="finish-an-episode">Finish an episode</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">done</span> <span class="o">=</span> <span class="bp">False</span>
<span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
  <span class="n">observation</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">sample</span><span class="p">())</span>
</code></pre></div></div>

<ul>
  <li>will end quickly since random actions can‚Äôt keep the pole up for long</li>
</ul>

<h2 id="exercise">Exercise</h2>
<ul>
  <li>determine how many steps, on average, are taken when actions are randomly sampled</li>
  <li>can be a benchmark to compare later algorithms</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># https://deeplearningcourses.com/c/deep-reinforcement-learning-in-python
# https://www.udemy.com/deep-reinforcement-learning-in-python
</span><span class="kn">import</span> <span class="nn">gym</span>
<span class="c1"># Wiki:
# https://github.com/openai/gym/wiki/CartPole-v0
# Environment page:
# https://gym.openai.com/envs/CartPole-v0
</span>
<span class="c1"># get the environment
</span><span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s">'CartPole-v0'</span><span class="p">)</span>

<span class="c1"># put yourself in the start state
# it also returns the state
</span><span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<span class="c1"># Out[50]: array([-0.04533731, -0.03231478, -0.01469216,  0.04151   ])
</span>
<span class="c1"># what do the state variables mean?
# Num Observation Min Max
# 0 Cart Position -2.4  2.4
# 1 Cart Velocity -Inf  Inf
# 2 Pole Angle  ~ -41.8¬∞  ~ 41.8¬∞
# 3 Pole Velocity At Tip  -Inf  Inf
</span>
<span class="n">box</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span>

<span class="c1"># In [53]: box
# Out[53]: Box(4,)
</span>
<span class="c1"># In [54]: box.
# box.contains       box.high           box.sample         box.to_jsonable
# box.from_jsonable  box.low            box.shape
</span>
<span class="n">env</span><span class="o">.</span><span class="n">action_space</span>

<span class="c1"># In [71]: env.action_space
# Out[71]: Discrete(2)
</span>
<span class="c1"># In [72]: env.action_space.
# env.action_space.contains       env.action_space.n              env.action_space.to_jsonable
# env.action_space.from_jsonable  env.action_space.sample
</span>
<span class="c1"># pick an action
</span><span class="n">action</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>

<span class="c1"># do an action
</span><span class="n">observation</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>


<span class="c1"># run through an episode
</span><span class="n">done</span> <span class="o">=</span> <span class="bp">False</span>
<span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
  <span class="n">observation</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">sample</span><span class="p">())</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">box</span><span class="o">.</span><span class="n">contains</span><span class="p">)</span> <span class="c1"># &lt;bound method Box.contains of Box(4,)&gt;
</span><span class="n">num_states</span><span class="o">=</span><span class="mi">10</span><span class="o">**</span><span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">num_actions</span><span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="n">a</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">low</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">num_states</span><span class="p">,</span> <span class="n">num_actions</span><span class="p">))</span>
<span class="nb">len</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="c1">#10000
</span><span class="n">observation_examples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">)])</span>
<span class="n">observation_examples</span><span class="o">.</span><span class="n">shape</span>
<span class="c1">#(10000, 4)
</span></code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>observation_examples

array([[-9.7157693e-01, -7.4095410e+37, -4.1871965e-01,  6.7507521e+37],
       [-3.7798457e+00, -2.5458868e+38,  2.8346911e-01,  3.4012554e+38],
       [ 2.6570508e+00, -3.3872902e+38, -4.0225080e-01,  4.4559389e+37],
       ...,
       [-2.4882526e+00,  2.3085303e+38, -4.1779616e-01, -1.0087459e+38],
       [-3.6637935e-01,  8.6948986e+37, -2.4446332e-01, -3.9084766e+37],
       [ 2.8921950e+00,  3.1869669e+38, -2.1086818e-02, -8.7358295e+37]],
      dtype=float32)
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="mi">20000</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span><span class="o">*</span><span class="mi">2</span> <span class="o">-</span> <span class="mi">1</span>
<span class="n">a</span> <span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="mi">20000</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span><span class="o">*</span><span class="mi">2</span> <span class="o">-</span> <span class="mi">1</span>
<span class="n">a</span><span class="o">.</span><span class="n">shape</span> <span class="c1">#(20000, 4)
</span></code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([[-0.24646925, -0.71345292, -0.13534508,  0.56783749],
       [ 0.94458905,  0.51363389,  0.16503288, -0.42810724],
       [ 0.09917716, -0.44244101, -0.46836587, -0.04689816],
       ...,
       [-0.59025553, -0.39560999,  0.35734313,  0.43326763],
       [ 0.03911455, -0.6548753 , -0.11969308,  0.91571025],
       [-0.10430864,  0.15491529, -0.73753709, -0.05311987]])
</code></pre></div></div>

<p>Reference:</p>

<p><a href="https://www.udemy.com/course/artificial-intelligence-reinforcement-learning-in-python/">Artificial Intelligence Reinforcement Learning</a></p>

<p><a href="https://www.udemy.com/deep-reinforcement-learning-in-python/">Advance AI : Deep-Reinforcement Learning</a></p>

<p><a href="https://www.udemy.com/cutting-edge-artificial-intelligence/learn/lecture/14650508#overview">Cutting-Edge Deep-Reinforcement Learning</a></p>
:ET