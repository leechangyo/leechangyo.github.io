I"ª<h1 id="approximation-methods">Approximation Methods</h1>
<ul>
  <li>Major Disadvantage of methods weâ€™ve learned</li>
  <li>V - Need to estimate ISI values</li>
  <li>Q - Need to estimate ISI x IAI values</li>
  <li>ISI and IAI can be very large</li>
  <li>solution : Approximation</li>
  <li>Recall: neural networks are universal function approximator</li>
  <li>First, we do feature extraction: convert state s to <strong>feature vector x</strong>
<a href="https://postimg.cc/VSsNgQMT"><img src="https://i.postimg.cc/N0mrgQ0g/414141412312.png" width="300px" title="source: imgur.com" /><a></a></a></li>
  <li>we want a function, parameterized by theta, that accurately approximates V
<a href="https://postimg.cc/yJBDGp3q"><img src="https://i.postimg.cc/MGf1vhWG/123131.png" width="300px" title="source: imgur.com" /><a></a></a></li>
</ul>

<h2 id="1-linear-approximation">1. Linear Approximation</h2>
<ul>
  <li>Function approximation methods requires us to use models that are differentiable</li>
  <li>Canâ€™t use something like decision tree or k-nearest neighbour</li>
  <li>in sequal(æŸ¥è¯¢è¯­è¨€) to this section, we will use deep learning methods, which are differentiable</li>
  <li>Neural networks donâ€™t require us to engineer features beforehand, but it can help</li>
  <li>FOr Approximation methods, we will need linear regression and gradient descent</li>
</ul>

<h2 id="2-section-outline">2. Section outline</h2>
<ul>
  <li>MC prediction</li>
  <li>TD(0) prediction</li>
  <li>SARSA</li>
</ul>

<h2 id="3-sanityæ˜æ™º-checking">3. Sanity(æ˜æ™º) Checking</h2>
<ul>
  <li>we can always sanity check our work by comparing to non-approximated versions</li>
  <li>we expect approximation to be close, but perhaps not exactly equal</li>
  <li>Obstacle we may encounter: our code is implemented perfectly, but our model is bad</li>
  <li>Linear models are not very expressive</li>
  <li>if we extract a poor set of features, model wonâ€™t learn value function well</li>
  <li>will need to put in manual work to do feature engineering</li>
</ul>

<h2 id="4-linear-model">4. Linear Model</h2>
<ul>
  <li>Supervised learning aka,<strong>Function Approximation</strong></li>
  <li>The function we are trying to approximate is V(s) or Q(s,a)</li>
  <li>Rewards are real numbers</li>
  <li>So returns, which are sums of rewards are real number</li>
  <li>2 Supervised learning techniques
    <ul>
      <li>Classification</li>
      <li>regression</li>
    </ul>
  </li>
  <li>we are doing regression</li>
</ul>

<h2 id="5-error">5. Error</h2>
<ul>
  <li>Supervised Learning methods have cost Functions</li>
  <li>Regression means squared error is the appropriate choice</li>
  <li>Squared difference between V and estimate of V
<a href="https://postimg.cc/m1G258rz"><img src="https://i.postimg.cc/Gt489fwQ/121231231231.png" width="300px" title="source: imgur.com" /><a></a></a></li>
  <li>Replace V with its definition</li>
  <li>But we donâ€™t know this expected value
<a href="https://postimg.cc/s1rvggVY"><img src="https://i.postimg.cc/9X4T1DKn/124123124123.png" width="300px" title="source: imgur.com" /><a></a></a></li>
  <li>as we learned from MC(Monte Carlo), we can replace the expected value with its sample means
<a href="https://postimg.cc/0MrZ60G4"><img src="https://i.postimg.cc/DZPNMCS7/433.png" width="300px" title="source: imgur.com" /><a></a></a></li>
  <li>Alternatively, we can treat each G_(i,s) as a training sample</li>
  <li>And try to minimize all the <strong>individual squared differences</strong> simultaneously(just like linear regression)
<a href="https://postimg.cc/m1kDWtsC"><img src="https://i.postimg.cc/4ybht92L/41241312321.png" width="300px" title="source: imgur.com" /><a>
<a href="https://postimg.cc/XZKcpDyx"><img src="https://i.postimg.cc/vmkP2J8R/412313124123.png" width="300px" title="source: imgur.com" /><a></a></a></a></a></li>
</ul>

<h2 id="6-stochastic-gradient-descent">6. stochastic Gradient Descent</h2>
<ul>
  <li>we can now do stochastic(éšæœºçš„) gradient decent</li>
  <li>Move in direction of gradient of error wrt only one sample at a time</li>
</ul>

<h3 id="gradient-descent-ì„¤ëª…">Gradient Descent ì„¤ëª…</h3>
<ul>
  <li>Neural Networkì˜ loss functionì˜ í˜„ weightì˜ ê¸°ìš¸ê¸°(gradient)ë¥¼ êµ¬í•˜ê³  Lossë¥¼ ì¤„ì´ëŠ” ë°©í–¥ìœ¼ë¡œ ì—…ë°ì´íŠ¸ ë‚˜ê°€ëŠ” ë°©ë²•
<a href="https://postimg.cc/XGVhJ5XG"><img src="https://i.postimg.cc/0QmqVdsf/124141312312.png" width="500px" title="source: imgur.com" /><a></a></a></li>
  <li>Loss function (ERROR) ëŠ” í˜„ì¬ ê°€ì¤‘ì¹˜(weight)ì—ì„œ í‹€ë¦° ì •ë„ë¥¼ ì•Œë ¤ì£¼ëŠ” í•¨ìˆ˜ì´ë‹¤.</li>
  <li>ì¦‰, í˜„ì¬ ë„¤íŠ¸ì›Œí¬ì˜ weightì—ì„œ ë‚´ê°€ ê°€ì§„ ë°ì´í„°ë¥¼ ì§‘ì–´ ë„£ìœ¼ë©´ ì—ëŸ¬ê°€ ìƒê¸¸ ê²ƒì´ë‹¤. ê±°ê¸°ì— ë¯¸ë¶„ì„ í•˜ë©´ ì—ëŸ¬ë¥¼ ì¤„ì´ëŠ” ë°©í–¥ì„ ì•Œ ìˆ˜ ìˆë‹¤.</li>
  <li>ê·¸ ë°©í–¥ìœ¼ë¡œ ì •í•´ì§„ Learning rateë¥¼ í†µí•´ì„œ weightë¥¼ ì´ë™ì‹œí‚¨ë‹¤. ì´ê²ƒì„ ë°˜ë³µí•´ì„œ í•™ìŠµí•˜ëŠ” ê²ƒì´ë‹¤.</li>
  <li><strong>ì¦‰ Gradient DescentëŠ” í˜„ì¬ ë„¤íŠ¸ì›Œí¬ì˜ Weightì— ì–´ë–¤ ë°ì´í„°ë¥¼ ë„£ì„ ë–„ ì—ëŸ¬ê°’ì„ ë¯¸ë¶„í•´ì„œ ì—ëŸ¬ì˜ ë°©í–¥(Gradient)ì„ ì•Œì• ì²´ê³  ê·¸ ë°©í–¥ìœ¼ë¡œ ì •í•´ì§„ Learning rateë¥¼ í†µí•´ì„œ weightì„ ì´ë™ì‹œí‚¨ë‹¤(Descent).</strong>
<a href="https://postimg.cc/kR47V4fs"><img src="https://i.postimg.cc/fy7yrSdG/141412412312312.png" width="500px" title="source: imgur.com" /><a></a></a></li>
  <li>Gradient Descentì˜ ë‹¨ì ì€ ê°€ì§„ ë°ì´í„°ë¥¼ ë‹¤ ë„£ìœ¼ë©´ ì „ì²´ ì—ëŸ¬ê°€ ê³„ì‚°ì´ ë  ê²ƒì´ë‹¤.</li>
  <li>ê·¸ë ‡ê¸° ë•Œë¬¸ì— ìµœì ê°’ì„ ì°¾ì•„ ë‚˜ê°€ê¸° ìœ„í•´ì„œ í•œì¹¸ ì „ì§„í•  ë•Œë§ˆë‹¤ ëª¨ë“  ë°ì´í„° ì…‹ì„ ë„£ì–´ì£¼ì–´ì•¼ í•´ì•¼í•˜ê¸° ë•Œë¬¸ì— í•™ìŠµì´ êµ‰ì¥íˆ ì˜¤ë˜ ê±¸ë¦¬ëŠ” ë¬¸ì œê°€ ë°œìƒí•˜ê²Œ ëœë‹¤.</li>
  <li>ê·¸ëŸ¼ Gradient Descent ë§ê³  ë” ë¹ ë¥¸ Optimizer ëŠ” ì—†ì„ê¹Œ? ê·¸ê²ƒì´ ë°”ë¡œ <strong>stochastic Gradient Descent</strong> ì´ë‹¤.</li>
</ul>

<h3 id="stochastic-gradient-descent">Stochastic Gradient Descent</h3>
<ul>
  <li>Stochastic Gradient Descent(SGD)ëŠ” ì¡°ê¸ˆë§Œ íì–´ë³´ê³ (Mini Batch)ë¹ ë¥´ê²Œ í•™ìŠµ í•˜ëŠ” ê²ƒì´ë‹¤.
<a href="https://postimg.cc/qhZ26tpM"><img src="https://i.postimg.cc/d3J6XdXd/41241231312312.png" width="500px" title="source: imgur.com" /><a></a></a></li>
  <li>ìµœì ê°’ì„ ì°¾ì•„ ê³¼ì •ì€ ì´ì™€ ê°™ë‹¤
<a href="https://postimg.cc/D8YyVqBz"><img src="https://i.postimg.cc/vZG1Vhn5/111.png" width="500px" title="source: imgur.com" /><a></a></a></li>
  <li>GDì˜ ê²½ìš° í•­ìƒ ì „ì²´ ë°ì´í„° ì…‹ì„ ê°€ì§€ê³  Learning rateë¡œ ìµœì ì˜ ê°’ì„ ì°¾ì•„ê°€ëŠ” ëª¨ìŠµì„ ë¶ˆ ìˆ˜ ìˆë‹¤.</li>
  <li>SGD ê°™ì€ ê²½ìš° Mini-batch ì‚¬ì´ì¦ˆ ë§Œí¼ ì¡°ê¸ˆì”© ëŒë ¤ì„œ ìµœì ì˜ ê°’ìœ¼ë¡œ ì°¾ì•„ë‚˜ê°„ë‹¤.</li>
</ul>

<h2 id="7-gradient-descent">7. Gradient Descent</h2>
<p><a href="https://postimg.cc/Hrs5z4js"><img src="https://i.postimg.cc/bw1Tzmhb/4121312312312.png" width="500px" title="source: imgur.com" /><a></a></a></p>
<ul>
  <li>we would work for all models, not just linear models
<a href="https://postimg.cc/r0L516r7"><img src="https://i.postimg.cc/C1RJVSLM/222.png" width="500px" title="source: imgur.com" /><a></a></a></li>
</ul>

<h3 id="gradient-descent-for-linear-models">Gradient Descent for Linear Models</h3>
<p><a href="https://postimg.cc/B8JqHMvM"><img src="https://i.postimg.cc/CKqzy2pT/412312312312321312.png" width="500px" title="source: imgur.com" /><a></a></a></p>
:ET