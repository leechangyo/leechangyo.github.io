I"é'<h1 id="the-value-function">The Value function</h1>

<h2 id="1-assigning-rewards">1. Assigning Rewards</h2>
<p><a href="https://postimg.cc/rd9dKMSW"><img src="https://i.postimg.cc/zfsT2fvd/51212413.png" width="700px" title="source: imgur.com" /></a></p>
<ul>
  <li>the programmers are like â€œcoaches(æ•™ç»ƒ)â€ to the AI</li>
  <li>Pet Owner is a good analogy</li>
  <li>We define how to give rewards to agent (Ex. Give same reward no matter what agent does - agent will always just behave randomly)(Ex2. Give our dog a treat when it behaves badly - encourages bad behavior)</li>
</ul>

<h2 id="2-maze-example">2. Maze Example</h2>
<p><a href="https://postimg.cc/0bCTKWVT"><img src="https://i.postimg.cc/mgxBqKCg/512312412321413.png" width="700px" title="source: imgur.com" /></a></p>
<ul>
  <li>Robot Trying to solve maze</li>
  <li>Reward of 1 for finding the maze exit, else 0</li>
  <li>But robot unlikely to solve the maze with this structure</li>
  <li>if robot only sees reward is the max reward if all we see is 0</li>
  <li>better: Every step yields reward of -1</li>
  <li>Now robot is encouraged so solve maze as quickly as possible</li>
</ul>

<h2 id="3-something-like">3. Something like</h2>
<ul>
  <li>be careful not to build our own prior knowledge into the AI ( Ex. Chess)</li>
  <li>Agent should be rewarded for winning, not taking opponentâ€™s pieces</li>
  <li>No reward for implementing strategy us read about in chess book</li>
  <li>Free the agent to find its own solution</li>
  <li><em>OK</em> to lose all but one piece and then win</li>
  <li>tell the agent <strong>what</strong> we want it to achieve, not <strong>how</strong> we want it to be achieved</li>
</ul>

<h2 id="4-planning">4. Planning</h2>
<ul>
  <li>Scenario: we are thinking about studying for tomorrowâ€™s exam. we would rather hang out with friends.
    <ul>
      <li>hangout with friends -&gt; dopamine hit -&gt; happy</li>
      <li>Study -&gt; feel tired and bored</li>
      <li>why study?</li>
    </ul>
  </li>
  <li>we donâ€™t think about immediate rewards, but future rewards too. we want to assign some value to the current state that reflects the future too. call this the <strong>â€œVALUE FUNCTIONâ€</strong>
<script async="" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script></li>
</ul>
<div align="center" style="margin: 1em 0;">
<ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-5150894678574694" data-ad-slot="9221331439" data-ad-format="auto" data-full-width-responsive="true"></ins>
     </div>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

<h2 id="5-credit-assignment-problem">5. Credit Assignment Problem</h2>
<ul>
  <li>we receive a reward - getting hired for our dream job</li>
  <li>what previous action led to this success</li>
  <li>that called the <strong>â€œCredit Assignment Problemâ€</strong></li>
  <li>Ask the Question: â€œWhat did I do in the past that led to the reward Iâ€™m receiving now â€œ</li>
  <li>what action gets the credit</li>
</ul>

<h2 id="6-attribution">6. Attribution</h2>
<ul>
  <li>Related to online advertising concept of attribution</li>
  <li>if we show the user the same ad 10 times before they buy, which ad gets the credit?</li>
  <li>In RL we donâ€™t just assign ad-hoc(ç‚¹å¯¹ç‚¹) like this
<a href="https://postimg.cc/wtdR4kKQ"><img src="https://i.postimg.cc/yNDhdfvt/55123.png" width="500px" title="source: imgur.com" /></a></li>
</ul>

<h2 id="7-delayed-rewards">7. Delayed Rewards</h2>
<ul>
  <li>Delayed Rewards: another way of thinking of the same thing</li>
  <li>Credit assignment: Present -&gt; Past</li>
  <li>Delayed from the other direction: Present -&gt; Future</li>
  <li>Related to field known as â€œplanningâ€</li>
  <li>the idea of delayed rewards tell us that an AI needs to have the ability of foresight or planning, planning is actually a field of study that crosses over with reinforcement learning</li>
</ul>

<h2 id="8-scenario">8. Scenario</h2>
<p><a href="https://postimg.cc/Cz5FYnHT"><img src="https://i.postimg.cc/SN7j6Wns/5514123412.png" width="500px" title="source: imgur.com" /></a></p>
<ul>
  <li>2 possible next states from A: B or C</li>
  <li>50% probability of ending up in either</li>
  <li>Reasonable value of A?</li>
  <li>Value(A) = 0.5x1 + 0.5x0 = 0.5</li>
</ul>

<p><a href="https://postimg.cc/RWCwvRpC"><img src="https://i.postimg.cc/7Z09Mtgz/5123124213.png" width="500px" title="source: imgur.com" /></a></p>

<ul>
  <li>1 Possible next state form A:B</li>
  <li>100% Probability of ending up in B</li>
  <li>Reasonable value for A ?</li>
  <li>Value(A) = 1 x 1 = 1</li>
  <li>Value tells us the â€œFuture goodnessâ€ of a state</li>
</ul>

<h2 id="9-value-function">9. Value Function</h2>
<p><a href="https://postimg.cc/0bKHv932"><img src="https://i.postimg.cc/3JtMgRFp/525.jpg" width="500px" title="source: imgur.com" /></a></p>
<ul>
  <li>V(s) - The Value(Taking into Account the probability of all possible future rewards) of a state</li>
  <li>Value is a measure of possible future rewards we may get from being in this state</li>
  <li>reward is immediate(Ex. Jumping on a Goomba will immediately increase our score)(Ex2. Standing in front of a Goomba will not increase our score, but will put us in a position to jump in the next few states)</li>
  <li>Estimating the value function is a central task in RL</li>
  <li>Not RL algorithms require it(Ex, Evolutionary algorithm that mutates &amp; spawns offspring, only those who survive the longest make it to next generation)</li>
  <li>by pure evolution + natural selection we can breed better and better agents</li>
  <li>but not the type of algorithm we are interested in for RL most of the time</li>
</ul>

<h1 id="10-efficiency">10. Efficiency</h1>
<p><a href="https://postimg.cc/nsyfqDpQ"><img src="https://i.postimg.cc/9MMX5dk1/512124123124.png" width="500px" title="source: imgur.com" /></a></p>
<ul>
  <li>the Value function is a fast &amp; efficient way of searching the game tree</li>
  <li>time consuming to enumerate every possible state transition and their probabilities of occurring.</li>
  <li>tic-tac-toe: 3^(3*3) = 19683</li>
  <li>Connect 4ï¼š 3^(4*4) = 43Millon</li>
  <li>Exponential Growth is never good</li>
  <li>Curse of dimensionality</li>
  <li>V(S) gives answer instantly O(1)</li>
</ul>

<h2 id="11-finding-vs">11. Finding V(s)</h2>
<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>V(s) = E[all future rewards</td>
          <td>S(t) = s]</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>E[x] = average of X</li>
  <li>this is generic algorithm here</li>
  <li>so we need to introduce some constraints here</li>
  <li>iterative algorithm</li>
  <li>initialize V(s):
    <ul>
      <li>V(s) = 1 if s = winning rate</li>
      <li>V(s) = 0 if s = lose or draw</li>
      <li>V(s) = 0.5 otherwise</li>
    </ul>
  </li>
  <li>when we study â€œrealâ€ algorithm, we wonâ€™t need such careful initialization</li>
  <li>V(s) can be interpreted(è¯´æ˜) as probability of winning after arriving in s( for this game only )(S.t V(s) is winning rate )</li>
  <li>After we initialize V(s), we update it as follows
<a href="https://postimg.cc/gXw9wQ4Y"><img src="https://i.postimg.cc/3R13qHvD/512314123.png" width="300px" title="source: imgur.com" /></a></li>
  <li>s = current state, sâ€™=next state</li>
  <li><strong>s represents every state we encounter in an episode</strong></li>
  <li>Means we need to actually play an episode and keep track of state history</li>
  <li>terminal state never updated since it doesnâ€™t have a next state</li>
  <li>weâ€™ll do this over many episode</li>
</ul>

<blockquote>
  <p>pseudocode</p>
</blockquote>

<pre><code class="language-pseudocode">for t in range(max_iterations):
  state_history = play_game
  for(s,s') in state_history from end to start:
    V(s) = V(s) + learning_rate*(V(s')-V(s))
</code></pre>

<h2 id="12-playing-the-game">12. Playing the Game</h2>
<ul>
  <li>how do we actually play the game/generate an episode</li>
  <li>Take random action? No!</li>
  <li>we donâ€™t need to, we have the value function</li>
</ul>

<pre><code class="language-pseudocode">maxV = 0
maxA = None
for a,s' in possible_next_states:
  if V(s') &gt; maxV :
    maxV = V(s') # ì¢‹ì€ max Valueë¥¼ ê³¨ëìœ¼ë‹ˆ ë‹¤ìŒì— ë‚˜ì˜¤ëŠ” stationì´ ëœë‹¤.
    maxA = a
perform action max A
</code></pre>

<blockquote>
  <p>Note : would taking random actions even give us the right value functions? No, Because a game tree w/ random actions has different probabilities than a game tree w/ â€œbestâ€ actions</p>
</blockquote>

<h2 id="13-problem">13. Problem</h2>
<script async="" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

<div align="center" style="margin: 1em 0;">
<ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-5150894678574694" data-ad-slot="9221331439" data-ad-format="auto" data-full-width-responsive="true"></ins>
     </div>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

<ul>
  <li>Problem with previous approach: Value function isnâ€™t accurate</li>
  <li>if we had the true value function, we wouldnâ€™t need to do any of this work</li>
  <li>Example of the explore-exploit dilemma</li>
  <li>Random actions lead us to states we may not have otherwise visited</li>
  <li>we can thus improve our value estimate for those states</li>
  <li>but to win, we need to do the action that yields maximum value</li>
  <li>we will use upsilon-greedy</li>
</ul>

<h2 id="14-intuition">14. Intuition</h2>
<p><a href="https://postimg.cc/gXw9wQ4Y"><img src="https://i.postimg.cc/3R13qHvD/512314123.png" width="300px" title="source: imgur.com" /></a></p>
<ul>
  <li>should remind you of the low-pass-filter / average-value-finding equation we saw earlier(+ gradient descent if weâ€™ve seen that)</li>
  <li>since we visit the states stochastically(éšæœºåœ°), V(s) will try to get close to V(sâ€™) for all possible next sâ€™</li>
  <li>by playing infinitely many episodes, the proportion of time we spend in eachâ€™s will approach the true probabilities</li>
  <li>Extremely Important detail, hard to discern(è¯†åˆ«)</li>
</ul>
:ET