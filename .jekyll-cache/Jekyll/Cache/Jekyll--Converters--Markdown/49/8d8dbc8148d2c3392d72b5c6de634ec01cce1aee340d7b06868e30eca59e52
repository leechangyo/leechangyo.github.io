I"¼><h1 id="temporal-difference-td-learning">Temporal Difference (TD) Learning</h1>
<ul>
  <li>this section is a 3rd technique for solving MDPs</li>
  <li>TD = Temporal Difference(TD) Learning</li>
  <li>Combines ideas from DP and MC</li>
  <li>Disadvantage of DP: requires full model of environment, never learns from experience</li>
  <li>MC and TD learn from experience</li>
  <li>MC can only update after completing episode, but DP uses Bootstrapping(Making an initial estimate)</li>
  <li>We will see that TD also uses Bootstrapping and is fully online, can update value during an episode
    <h2 id="1-td-learning">1. TD Learning</h2>
  </li>
  <li>Same approach as before</li>
  <li>First Predict Problem</li>
  <li>them control problem</li>
  <li>2 control methods:
    <ul>
      <li>SARSA</li>
      <li>Q-Learning</li>
    </ul>
  </li>
  <li>Model-Free Reinforcement Learning</li>
  <li>TD methods learn directly from episode of experience.</li>
  <li>no Knowledge of MDP Transition / rewards</li>
  <li>TD learns from incomplete episodes by Bootstrapping</li>
  <li>TD updates a guess towards a guess
<a href="https://postimg.cc/fkPHCsWG"><img src="https://i.postimg.cc/28kPzjsS/41231312313.png" width="500px" title="source: imgur.com" /><a></a></a></li>
  <li>MCì™€ ë‹¤ë¥¸ì ì€, MCëŠ” ì‹¤ì œ ì—í”¼ì†Œë“œê°€ ëë‚˜ê³  ë°›ê²Œ ë˜ëŠ” ë³´ìƒì„ ì‚¬ìš©í•˜ì—¬ Value Functionì„ ì—…ë°ì´íŠ¸ í•˜ì˜€ë‹¤</li>
  <li>TDì—ì„œëŠ” ì‹¤ì œ ë³´ìƒê³¼ ë‹¤ìŒ stepì— ëŒ€í•œ ë¯¸ë˜ì¶”ì •ê°€ì¹˜ë¥¼ ì‚¬ìš©í•´ì„œ í•™ìŠµí•œë‹¤.</li>
  <li>ì´ë•Œ ì‚¬ìš©í•˜ëŠ” ë³´ìƒê³¼ Value Functionì˜ í•©ì„ TD Target
-ê·¸ë¦¬ê³  TD Target ê³¼ ì‹¤ì œ V(S)ì™€ì˜ ì°¨ì´ë¥¼ TD errorë¼ê³  í‘œí˜„í•œë‹¤.
<a href="https://postimg.cc/FfJFfNRh"><img src="https://i.postimg.cc/TPtK4Pv5/14141241313.png" width="500px" title="source: imgur.com" /><a></a></a></li>
  <li>MCì—ì„œì˜ Value functionì´ ì—…ë°ì´íŠ¸ ë˜ëŠ” ê³¼ì •ì´ ì™¼ìª½(ì—í”¼ì†Œë“œê°€ ì „ì²´ì ìœ¼ë¡œ ëë‚˜ì„œ ê·¸ì˜ ë³´ìƒì„ ë‚˜ëˆ„ì–´ ë‹¨ê³„ë³„ë¡œ ì—…ë°ì´íŠ¸)</li>
  <li>TDëŠ” ê° ë‹¨ê³„ë³„ë¡œ ì—…ë°ì´íŠ¸ê°€ ë˜ëŠ” ê³¼ì •ìœ¼ë¡œ ì˜¤ë¥¸ìª½ ê·¸ë¦¼</li>
  <li>TDì˜ ì¥ì ì€ ì—í”¼ì†Œë“œ ì¤‘ê°„ì—ì„œë„ í•™ìŠµì„ í•˜ê²Œ ëœë‹¤ëŠ” ê²ƒì´ë‹¤.</li>
  <li>MCì—ì„œëŠ” ì—í”¼ì†Œë“œê°€ ëë‚ ë•Œê¹Œì§€ ê¸°ë‹¤ë ¸ë‹¤ê°€ ì—…ë°ì´íŠ¸ê°€ ë°œìƒí•˜ê³  í•™ìŠµí•˜ê¸° ë–„ë¬¸ì´ë‹¤.</li>
  <li>TDëŠ” ì¢…ë£Œ ì—†ëŠ” ì—°ì†ì ì¸ ì—í”¼ì†Œë“œì—ì„œë„ í•™ìŠµí•  ìˆ˜ ìˆë‹¤.</li>
  <li>Return $G_t$ = R(t+1)+É£R(t+2)+â€¦+É£^(t-1)$R_T) is unbiased estimate of $v_Ï€$($S_t$)</li>
  <li>True TD Target  R(t+1)+É£V(S(t+1)) is biased estimate of $v_Ï€$($S_t$)</li>
  <li>TD Target is much lower variance than the return:
    <ul>
      <li>return depends on many random actions, transitions, rewards</li>
      <li>TD Target depends on one random action, transition, reward</li>
    </ul>
  </li>
  <li>V policyê°€ ì‹¤ì œ Gì— ëŒ€í•´ì„œ unbiasë¼ í• ë•ŒëŠ” TD Targetë„ V policyë¥¼ ì¶”ì¢…í•˜ê¸° unbiasì´ë‹¤. í•˜ì§€ë§Œ TD Targetì— V policyë¥¼ ì¶”ì •í•˜ëŠ” V(St+1)ë¥¼ ì‚¬ìš©í•˜ê¸°ì—  ì‹¤ì œê°’ì´ ì•„ë‹ˆë¼ ì‹¤ì œê°’ì„ ì¶”ì •í•˜ëŠ” ê°’ì„ìœ¼ë¡œ biasê°€ ë°œìƒí•œë‹¤. ê·¸ë¦¬ê³  TD Targetì€ ë‹¨ì§€ í•˜ë‚˜ì˜ stepì—ì„œë§Œ ê³„ì‚°í•˜ê¸°ì— noiseê°€ ì‘ê²Œ ë˜ë¯€ë¡œ ìƒëŒ€ì ìœ¼ë¡œ varianceê°€ ë‚®ê²Œ ë‚˜íƒ€ë‚œë‹¤.</li>
  <li>MC has high variance, zero bias
    <ul>
      <li>good convergence properties</li>
      <li>even with function approximation</li>
      <li>not very sensitive to initial value</li>
      <li>very simple to understand and use</li>
    </ul>
  </li>
  <li>TD has low variance, some bias
    <ul>
      <li>Usually more efficient than MC</li>
      <li>TD(0) converges to $v_Ï€$(s)</li>
      <li>but not always with function approximation</li>
      <li>More sensitive to initial value</li>
    </ul>
  </li>
  <li>Compare on variance between MC and TD
<a href="https://postimg.cc/hhyS0pYJ"><img src="https://i.postimg.cc/L6Rn2WBV/41241313123.png" width="500px" title="source: imgur.com" /><a></a></a></li>
  <li>Bootstrappingì´ ë” í•™ìŠµí•˜ëŠ”ë° íš¨ìœ¨ì ì´ë‹¤.</li>
  <li>MC and TD converge : V(s) -&gt; $v_Ï€$(s) as experience -&gt; âˆ (ì—í”¼ì†Œë“œëŠ” ë¬´í•œ ë°˜ë³µí•˜ê²Œ ë˜ë©´ ê²°êµ­ ìˆ˜ë ´í•˜ê²Œ ë˜ì–´ìˆë‹¤.)</li>
  <li>but what about batch solution for finite experience
<a href="https://postimg.cc/RqDW0S4b"><img src="https://i.postimg.cc/QxX10CPs/41241312312312312312.png" width="300px" title="source: imgur.com" /><a></a></a></li>
  <li>e.g repeatedly sample episode k âˆˆ [1,K]</li>
  <li>Apply MC or TD(0) to episode k
<a href="https://postimg.cc/VrwVnPwx"><img src="https://i.postimg.cc/wvRYK9Gj/3121.png" width="500px" title="source: imgur.com" /><a></a></a></li>
  <li>For example, first episode A got reward â€œ0â€ and B got reward â€œ0â€</li>
  <li>from Second Episode B got reward â€œ1â€ to seven episode. and B got 0 at the last episode.</li>
  <li>then A will go B within 100 %  and then reward will be â€œ0â€</li>
  <li>in the MC methods, A will get reward â€œ0â€ because the episode pass Through A is one that final reward is zero
6- in the TD methods, running another episode because of B value update to â€œ6â€ then A value also going to be updated</li>
  <li>MC converges to solution with minimum mean-square
    <ul>
      <li>best fit to the observe returns :
<a href="https://postimg.cc/bSwDGrBs"><img src="https://i.postimg.cc/4yzzShvb/412312123123.png" width="300px" title="source: imgur.com" /><a></a></a></li>
      <li>in the AB example, V(A)=0</li>
    </ul>
  </li>
  <li>TD(0) converges to solution of max likelihood Markov Model
    <ul>
      <li>Solution to the MDP (S,A,P^,R^,É£) that best fits the data
<a href="https://postimg.cc/zytd3H2R"><img src="https://i.postimg.cc/QMsv4ppS/12412412312312.png" width="500px" title="source: imgur.com" /><a></a></a></li>
      <li>in the AB example, V(A) = 0.75</li>
    </ul>
  </li>
  <li>TD(0)ë°©ì‹ì€ max likelihood Markv ë°©ì‹ì„ ì‚¬ìš©í•˜ì—¬ ìˆ˜ë ´í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì´ë‹¤. MDP ê¸°ë°˜ìœ¼ë¡œ ì‹¤ì œì ì¸ ê°’ì„ ì°¾ì•„ê°€ê²Œ ë˜ê¸° ë•Œë¬¸ì— V(A)ì˜ ê°’ì´ 6/8 í‰ê· ê°€ì¹˜ê°€ ê³„ì‚°ë˜ì–´ 0.75ê°’ìœ¼ë¡œ ì—…ë°ì´íŠ¸ê°€ ëœë‹¤.</li>
  <li>TD exploits Markov property
    <ul>
      <li>Usually more efficient in Markov environment</li>
    </ul>
  </li>
  <li>MC does not exploit Markov property
    <ul>
      <li>Usually more effective in Non-Markov environment</li>
    </ul>
  </li>
  <li>MC ì•Œê³ ë¥´ì§
<a href="https://postimg.cc/RNjYTHL5"><img src="https://i.postimg.cc/cLHN07KH/12412312312.png" width="500px" title="source: imgur.com" /><a></a></a></li>
  <li>Bootstrappingì„ ì‚¬ìš©í•˜ì—¬ statesì— ëŒ€í•œ valueë“¤ì„ ì¶”ì •í•˜ì—¬ ì‚¬ìš©í•˜ëŠ” ë°©ë²• TD
<a href="https://postimg.cc/MMVHvR56"><img src="https://i.postimg.cc/B6wK7B1K/1414131312.png" width="500px" title="source: imgur.com" /><a></a></a></li>
  <li>DP ë°©ì‹
<a href="https://postimg.cc/RqGJkkPX"><img src="https://i.postimg.cc/3JQjv725/41231231231231.png" width="500px" title="source: imgur.com" /><a></a></a></li>
  <li>DP ë°©ì‹ì—ì„œëŠ” ëª¨ë¸ì„ í†µí•´ì„œ ì´ë¯¸ MDPë¥¼ ì•Œê³  ìˆê³  ì´ì— ëŒ€í•œ value ì™€ rewardë¥¼ í†µí•´ í•™ìŠµì„ í•˜ê¸° ë•Œë¬¸ì— ìœ„ì—ì™€ ê°™ì´ ë‚˜ì˜µë‹ˆë‹¤.</li>
  <li>Bootstrapping: update involves an estimate
    <ul>
      <li>MC does not bootstrap</li>
      <li>DP bootstraps</li>
      <li>TD bootstrap</li>
    </ul>
  </li>
  <li>Sampling update samples an expectation
    <ul>
      <li>MC samples</li>
      <li>DP does not sample</li>
      <li>TD samples</li>
    </ul>
  </li>
  <li>DPì™€ TPì—ì„œ ì‚¬ìš©í•˜ëŠ” Bootstrappingì€ ì¶”ì • ê°’ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ì—¬ ì—…ë°ì´íŠ¸</li>
  <li>MCì—ì„œ ì‚¬ìš©í•˜ëŠ” ìƒ˜í”Œë§ì€ expectationì„ ìƒ˜í”Œí•˜ì—¬ ì—…ë°ì´íŠ¸ í•œë‹¤.</li>
  <li>TDë„ ìƒ˜í”Œë§ì„ í•œì§€ë§Œ DPì²˜ëŸ¼ full backupì„ í•˜ì§€ ì•ŠëŠ”ë‹¤.
<a href="https://postimg.cc/RqbK6rcq"><img src="https://i.postimg.cc/WbsnyTzm/4123131231231.png" width="500px" title="source: imgur.com" /><a></a></a></li>
</ul>

<h2 id="2-td0-prediction">2. TD(0) Prediction</h2>
<ul>
  <li>Apply TD to prediction problem</li>
  <li>algorithm is called TD(0)</li>
  <li>there is also TD(1) and TD(Î»)</li>
  <li>it is related to Q-Learning and approximation methods</li>
</ul>

<h3 id="mc">MC</h3>
<ul>
  <li>Recall: one Disadvantage of MC is we need to wait until the episode is finished then we calculate return</li>
  <li>also recall: Multiple ways of calculating averages</li>
  <li>General â€œaverage-findingâ€ equation</li>
  <li>Does not require us to store all returns</li>
  <li>Constant alpha is moving average/exponential decay
<a href="https://postimg.cc/B8LQVyj7"><img src="https://i.postimg.cc/cJTgrG0W/41312312312.png" width="500px" title="source: imgur.com" /><a></a></a></li>
</ul>

<h3 id="value-function">Value function</h3>
<ul>
  <li>now recall: the definition of V</li>
  <li>We can define it recursively
<a href="https://postimg.cc/D80ZG82K"><img src="https://i.postimg.cc/15cNLq24/4123123123123124123.png" width="500px" title="source:imgur.com" /><a></a></a></li>
</ul>

<h3 id="td0">TD(0)</h3>
<ul>
  <li>can we just combine these(MC &amp; Value Function)</li>
  <li>instead of sampling the return, use the recursive definition
<a href="https://postimg.cc/94MJ5P2n"><img src="https://i.postimg.cc/HL0KM3hd/4124123124123.png" width="500px" title="source:imgur.com" /><a></a></a></li>
  <li>this is TD(0)</li>
  <li>why this is fully online? <strong>we can update V(s) as soon as we know sâ€™</strong></li>
</ul>

<h3 id="sources-of-randomness">sources of randomness</h3>
<ul>
  <li>In Mc, randomness arises when an episode can play out in different ways(due to stochastic policy, or stochastic state transitions)</li>
  <li>in TD(0), we have another source of randomness
    <ul>
      <li>G is an exact sample in MC</li>
      <li>r + É£V(sâ€™) is itself an estimate of G</li>
    </ul>
  </li>
  <li>we are estimating from other estimates(bootstrapping)</li>
</ul>

<h3 id="summary">Summary</h3>
<ul>
  <li>TD(0) is advantageous in comparison to MC/DP</li>
  <li>Unlike DP, we donâ€™t require a model of the environment, and only update V for states we visit</li>
  <li>unlike MC, we donâ€™t need to wait for an episode to finish</li>
  <li>advantageous for very long episodes</li>
  <li>also applicable to continuous(non-episodic)tasks</li>
</ul>

<h2 id="3-td-learning-for-control">3. TD Learning for Control</h2>
<ul>
  <li>Apply TD(0) to Control</li>
  <li>we can probably guess what weâ€™re going to do by now</li>
  <li>use Generalized policy iteration, alternate between TD(0) for prediction, policy improvement using <strong>greedy action selection</strong></li>
  <li><strong>Use Value iteration method</strong>: Update Q in-place, improve policy after every change</li>
  <li>skip the part where we do full policy evaluation</li>
</ul>

<h2 id="4-sarsa">4. SARSA</h2>
<ul>
  <li>Recall from MC: we want to use Q because itâ€™s indexed by a, V is only indexed by s</li>
  <li>Q has the same recursive form</li>
  <li>Same limitation as MC: need lots of samples in order to converge</li>
  <li>require the 5-tuuple:(s,a,r,sâ€™,aâ€™)</li>
  <li>Hence the name
<a href="https://postimg.cc/dZ3DB1cp"><img src="https://i.postimg.cc/C5HDwBPh/41412131231232.png" width="500px" title="source:imgur.com" /><a></a></a></li>
  <li>TDë°©ì‹ê³¼ ë‹¤ë¥¸ì ì€ Value functionì„ ì“°ì§€ ì•Šê³  Q Fucntionì„ ì“´ë‹¤.</li>
  <li>Like MC, still requires us to know Q(s,a) for all a, in order to choose argmax</li>
  <li>problem: if we follow a deterministic policy, we would only fill in 1/IAI values on each episode</li>
  <li>Leave most of Q untouched</li>
  <li>Remedy(å¤„ç†æ–¹æ³•): Exploring starts or policy that includes exploration</li>
  <li>use epsilon-greedy</li>
</ul>

<blockquote>
  <p>Pseudocode</p>
</blockquote>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Q(s,a) = arbitrary, Q(terminal,a) = 0
for t=1..N
  s = start_state, a = epsilon_greedy_from(Q(s))
  while not game over:
    s', r = do_action(a)
    a' = episode_greedy_from(Q(s'))
    Q(s,a) = Q(s,a) + alpha + [r + gamma*Q(s',a')-Q(s,a)]
    s = s', a = q'
</code></pre></div></div>

<ul>
  <li>Interesting fact : convergence proof has never been published</li>
  <li>has been stated informally that it will converge of policy converges to greedy</li>
  <li>we can achieve this by seeing Îµ = 1/t</li>
  <li>Or Îµ = c/t or Îµ= c/$t^a$</li>
  <li>Hyperparameters</li>
</ul>

<h3 id="learning-rate">Learning Rate</h3>
<ul>
  <li>Recall that learning rate can also decay</li>
  <li>problem:
    <ul>
      <li>if we set Î± = 1/t</li>
      <li>at every iteration, only one (s,a) pair will be updated for Q(s,a)</li>
      <li>Learning rate will decay even for values we have never updated</li>
      <li>Could we just only decrease Î± after every episode?</li>
      <li>No</li>
      <li>Many elements of Q(s,a) are not updated during an episode</li>
    </ul>
  </li>
  <li>we take inspiration from deep learning: AdaGrad and RMSprop(adaptive learning rates)</li>
  <li>Effective Learning rate Decays more when previous gradient has been larger</li>
  <li>in other words: the more it has changed in the past, the less it will change in the future</li>
  <li>our version is simpler: keep a count of every(s,a) pair seen:
    <ul>
      <li>Î±(s,a) = $Î±_0$ / count(s,a)</li>
    </ul>
  </li>
  <li>Equivalently
    <ul>
      <li>Î±(s,a) = $Î±_0$ / (k+m*count(s,a))</li>
    </ul>
  </li>
</ul>

<h2 id="5-q-learning">5. Q-Learning</h2>
<ul>
  <li>Main Theme: Generalize policy iteration
    <ul>
      <li>Policy evaluation</li>
      <li>Policy Improvement(greedy wrt(with regard to) current value)</li>
    </ul>
  </li>
  <li>what weâ€™ve been studying: <strong>on-policy</strong> methods</li>
  <li>we always follow the current best policy</li>
  <li>Q-Learning is an <strong>off-policy</strong> methds</li>
  <li>do any random action, and still find Q*</li>
  <li>Looks similar SARSA
<a href="https://postimg.cc/dZ3DB1cp"><img src="https://i.postimg.cc/C5HDwBPh/41412131231232.png" width="500px" title="source:imgur.com" /><a></a></a></li>
  <li>instead of choosing aâ€™ based on argmax of Q, we update Q(s,a) directly with max over Q(sâ€™,aâ€™)</li>
  <li>isnâ€™t that the same, since aâ€™=argmax[aâ€™]{Q(sâ€™,aâ€™)}?
<a href="https://postimg.cc/hQJPqywD"><img src="https://i.postimg.cc/nr2sBNGD/41241241231231.png" width="500px" title="source:imgur.com" /><a></a></a></li>
  <li>we donâ€™t need to actually do action aâ€™ as the next move</li>
  <li>therefore, we use Q(sâ€™,aâ€™) in the update for Q(s,a), even if we donâ€™t do aâ€™ next</li>
  <li>Doesnâ€™t matter what policy we follow</li>
  <li>Reality: Random actions -&gt; suboptimal(then use greed) -&gt; takes longer for episode to finish</li>
  <li>takeaway: doesnâ€™t matter what policy we use</li>
  <li>Under What circumstance is Q-learning == SARSA?</li>
  <li>if policy used for Q-learning is greedy</li>
  <li>then weâ€™ll be doing Sarsa, but we also be doing Q-Learning</li>
</ul>

<h2 id="6summary">6.Summary</h2>
<ul>
  <li>TD combines aspects of MC and DP</li>
  <li>MC: Learn From experience / play the game</li>
  <li>Generalized idea of taking sample mean of returns
    <ul>
      <li>Multi-armed bandit</li>
    </ul>
  </li>
  <li>MC is not fully online</li>
  <li>DP: bootstrapping, recursive from of value function</li>
  <li>TD(0) = MC + DP (combines)</li>
  <li>Instead of taking <strong>sample mean of returns</strong>, we take sample mean of estimated returns, based on r and V(sâ€™)</li>
</ul>

<h3 id="td-summary">TD Summary</h3>
<ul>
  <li>Control</li>
  <li>On-policy: SARSA</li>
  <li>Off-policy : Q-Learning</li>
</ul>

<h3 id="td-disadvantage">TD Disadvantage</h3>
<ul>
  <li>Need Q(s,a)</li>
  <li>state space can easily become infeasible to enumerate</li>
  <li>need to enumerate every action for every state</li>
  <li>Q may not even fit into memory</li>
  <li>Measuring Q(s,a) for all s and a is called the tabular(è¡¨æ ¼å¼çš„) method</li>
  <li>Next, we will learn about function approximated methods which allow us to compress the amount of space needed to represent Q</li>
</ul>
:ET