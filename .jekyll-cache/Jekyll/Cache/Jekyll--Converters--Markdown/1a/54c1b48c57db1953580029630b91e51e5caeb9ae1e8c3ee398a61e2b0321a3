I"} <h1 id="optimization-in-signal-and-image-processing">Optimization in Signal and Image processing</h1>

<h2 id="model-arising-in-compressive-sensing-and-imaging-sciences">Model arising in compressive sensing and imaging sciences</h2>

<ul>
  <li>Separable(可分开的) Structures of problems</li>
</ul>

<p><a href="https://postimg.cc/zL2TZB2S"><img src="https://i.postimg.cc/vBbhnxWj/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li>Convexity and easy subproblems(子问题).</li>
  <li>Often involving large, nonsmooth convex functions with separable structure.</li>
  <li><a href="https://math.stackexchange.com/questions/2201384/what-is-the-definition-of-a-first-order-method">First-order method</a> can be very slow for producing high accuracy solutions, but also share many advantages:
    <ul>
      <li>Non-differentiability</li>
      <li>Use minimal information, e.g.,(f;∇f)</li>
      <li>Often lead to very simple and “cheap” iterative schemes</li>
      <li><strong>Complexity/iteration mildly dependent (e.g., linear) in problem’s dimension</strong></li>
      <li>Suitable when high accuracy is not crucial [in many large scale applications, the data is anyway corrupted or known only roughly</li>
    </ul>
  </li>
</ul>

<h2 id="example-compressive-sensingkernel-같은-개념">Example: compressive sensing(Kernel 같은 개념)</h2>
<ul>
  <li>Goal: Recover sparse signal from very few linear measurements.
<a href="https://postimg.cc/gX55JGj3"><img src="https://i.postimg.cc/WzbP53PY/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></li>
  <li>Basis pursuit model:
<a href="https://postimg.cc/gLFDQjrt"><img src="https://i.postimg.cc/3NWcyDhh/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></li>
</ul>

<h2 id="compressive-sensing">Compressive Sensing</h2>
<ul>
  <li>Find the sparest solution
    <ul>
      <li>Given n=256, m=128.
        <ul>
          <li>n is original signal</li>
          <li>m is output value</li>
        </ul>
      </li>
      <li>A = randn(m,n); u = sprandn(n, 1, 0.1); b = A*u;
<a href="https://postimg.cc/pmT6D9jK"><img src="https://i.postimg.cc/3xCQQmJf/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></li>
    </ul>
  </li>
</ul>

<h2 id="sparse흔히-넓은-지역에-분포된-정도가-드문-밀도가-희박한-recovery-models">Sparse((흔히 넓은 지역에 분포된 정도가) 드문, (밀도가) 희박한) recovery models</h2>
<p><a href="https://postimg.cc/YhjPFGYY"><img src="https://i.postimg.cc/wTkYSDBf/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>
<ul>
  <li>Basis pursuit model : nonunique solution</li>
  <li>Ax = b = {$x^*$+N(A)} =&gt; ∀ v ∈ N(A), Av = 0
    <ul>
      <li>N(A) is non-space</li>
    </ul>
  </li>
  <li>μ is given parameter(constant variable)</li>
</ul>

<h2 id="sparsity흔히-넓은-지역에-분포된-정도가-드문-밀도가-희박한-under-a-basis">Sparsity((흔히 넓은 지역에 분포된 정도가) 드문, (밀도가) 희박한) under a basis</h2>
<ul>
  <li>Given a reprenting basis or dictionary, we want to recover a signal which is sparse under this basis.</li>
  <li>Two types of models:</li>
</ul>

<p><a href="https://postimg.cc/1gGk58N6"><img src="https://i.postimg.cc/HLRdz5YS/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li>Commonly used dictionaries include both analytic and trained ones.
    <ul>
      <li>analytic bases: Id, DCT, wavelets, curvelets, gabor, etc., also their combinations; they have analytic properties, often easy to compute (for example, multiplying a vector takes O(n log n) instead of O($n^2$))</li>
      <li>data driven: can also be numerically learned from training data or partial signal, patches (offline or online learning).</li>
      <li>they can be orthogonal, frame, normalized or general.</li>
      <li><strong>If Φ is orthogonal (or just invertible), the analyse based model is equivalent to synthesis based one by changing the variable. When it is not, the model is harder to solve.</strong></li>
    </ul>
  </li>
</ul>

<h2 id="parallel-mri-reconstruction">Parallel MRI reconstruction</h2>
<p><a href="https://postimg.cc/GHvC73c4"><img src="https://i.postimg.cc/VLVkN5pW/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>

<h2 id="nonconvex-sparsity-models">Nonconvex sparsity models</h2>
<ul>
  <li>min f (x) + h(x)</li>
  <li>where either f or h is nonconvex or both are nonconvex. Some popular models:
    <ul>
      <li>Nonlinear least square for nonlinear inverse problems f (x) = ‖F(x) − y‖^2.</li>
      <li>h(x) = ‖x‖_p, where 0 &lt;= p &lt; 1 or rank constraint.</li>
      <li>Alternating minimization methods in many applications, such as blind deconvolution, matrix factorization, or dictionary learning etc.</li>
    </ul>
  </li>
</ul>

<h1 id="optimization-of-matrices">Optimization of Matrices</h1>

<h2 id="the-netflix-problem">The Netflix problem</h2>
<p><a href="https://postimg.cc/cKDmg7My"><img src="https://i.postimg.cc/TwxHsCKw/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>

<h2 id="matrix-rank-minimization">Matrix Rank Minimization</h2>
<p><a href="https://postimg.cc/tsdXGjLJ"><img src="https://i.postimg.cc/K8w4LcfP/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>

<h2 id="video-separation">Video separation</h2>
<p><a href="https://postimg.cc/k2f6xKf7"><img src="https://i.postimg.cc/N0w6vxX2/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>

<h2 id="sparse-and-low-rank-matrix-separation">Sparse and low-rank matrix separation</h2>
<ul>
  <li>Given a matrix M, we want to find a low rank matrix W and a sparse matrix E, so that W + E = M.</li>
  <li>Convex approximation:</li>
</ul>

<p><a href="https://postimg.cc/xXmkZ8Gz"><img src="https://i.postimg.cc/y8QFkSkv/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li>Robust PCA (reduce dimension)</li>
</ul>

<h2 id="extension">Extension</h2>
<p><a href="https://postimg.cc/CZ644zwf"><img src="https://i.postimg.cc/QtjnhTPS/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>

<h2 id="correlation-matrices">Correlation Matrices</h2>
<ul>
  <li>A correlation(相互关系) matrix satisfies:
    <ul>
      <li>X = X^T , X_ii = 1, i = 1, . . . , n, X &gt;= 0.</li>
    </ul>
  </li>
  <li>Example: (low-rank) nearest correlation matrix estimation</li>
</ul>

<p><a href="https://postimg.cc/fVvZ1GLM"><img src="https://i.postimg.cc/t4KTnyGn/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>

<h1 id="optimization-in-machine-learning">Optimization in Machine learning</h1>

<h2 id="why-optimization-in-machine-learning">Why Optimization in Machine Learning?</h2>
<p><a href="https://postimg.cc/PpB35qxY"><img src="https://i.postimg.cc/x8dWgJVx/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>

<h2 id="loss-functions-in-neural-network">Loss functions in neural network</h2>
<p><a href="https://postimg.cc/3kZnMTvr"><img src="https://i.postimg.cc/Z5GgdYYN/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>

<h2 id="convolution-operator">convolution operator</h2>
<p><a href="https://postimg.cc/Wth5KC8g"><img src="https://i.postimg.cc/QNqvBrW4/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>

<h2 id="optimization-algorithms-in-deep-learning-stochastic-gradient-methods">Optimization algorithms in Deep learning: stochastic gradient methods</h2>
<ul>
  <li>pytorch/caffe2: adadelta, adagrad, adam, nesterov, rmsprop, YellowFin https://github.com/pytorch/pytorch/tree/master/caffe2/sgd</li>
  <li>pytorch/torch: sgd, asgd, adagrad, rmsprop, adadelta, adam, adamax https://github.com/pytorch/pytorch/tree/master/torch/optim</li>
  <li>tensorflow: Adadelta, AdagradDA, Adagrad, ProximalAdagrad, Ftrl, Momentum, adam, Momentum, CenteredRMSProp https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/training_ops.cc</li>
</ul>

<h2 id="generative-adversarial-network-gan1">Generative adversarial network (GAN)1</h2>

<h1 id="reference">Reference</h1>

<p><a href="https://web.stanford.edu/class/ee364a/lectures.html"> Optimization method - Standford University </a></p>
:ET