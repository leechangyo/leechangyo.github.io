I" <h1 id="the-value-function">The Value function</h1>

<h2 id="1-assigning-rewards">1. Assigning Rewards</h2>
<p><a href="https://postimg.cc/rd9dKMSW"><img src="https://i.postimg.cc/zfsT2fvd/51212413.png" width="700px" title="source: imgur.com" /></a></p>
<ul>
  <li>the programmers are like “coaches(教练)” to the AI</li>
  <li>Pet Owner is a good analogy</li>
  <li>We define how to give rewards to agent (Ex. Give same reward no matter what agent does - agent will always just behave randomly)(Ex2. Give our dog a treat when it behaves badly - encourages bad behavior)</li>
</ul>

<h2 id="2-maze-example">2. Maze Example</h2>
<p><a href="https://postimg.cc/0bCTKWVT"><img src="https://i.postimg.cc/mgxBqKCg/512312412321413.png" width="700px" title="source: imgur.com" /></a></p>
<ul>
  <li>Robot Trying to solve maze</li>
  <li>Reward of 1 for finding the maze exit, else 0</li>
  <li>But robot unlikely to solve the maze with this structure</li>
  <li>if robot only sees reward is the max reward if all we see is 0</li>
  <li>better: Every step yields reward of -1</li>
  <li>Now robot is encouraged so solve maze as quickly as possible</li>
</ul>

<h2 id="3-something-like">3. Something like</h2>
<ul>
  <li>be careful not to build our own prior knowledge into the AI ( Ex. Chess)</li>
  <li>Agent should be rewarded for winning, not taking opponent’s pieces</li>
  <li>No reward for implementing strategy us read about in chess book</li>
  <li>Free the agent to find its own solution</li>
  <li><em>OK</em> to lose all but one piece and then win</li>
  <li>tell the agent <strong>what</strong> we want it to achieve, not <strong>how</strong> we want it to be achieved</li>
</ul>

<h2 id="4-planning">4. Planning</h2>
<ul>
  <li>Scenario: we are thinking about studying for tomorrow’s exam. we would rather hang out with friends.
    <ul>
      <li>hangout with friends -&gt; dopamine hit -&gt; happy</li>
      <li>Study -&gt; feel tired and bored</li>
      <li>why study?</li>
    </ul>
  </li>
  <li>we don’t think about immediate rewards, but future rewards too. we want to assign some value to the current state that reflects the future too. call this the <strong>“VALUE FUNCTION”</strong>
<script async="" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script></li>
</ul>
<div align="center" style="margin: 1em 0;">
<ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-5150894678574694" data-ad-slot="9221331439" data-ad-format="auto" data-full-width-responsive="true"></ins>
     </div>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

<h2 id="5-credit-assignment-problem">5. Credit Assignment Problem</h2>
<ul>
  <li>we receive a reward - getting hired for our dream job</li>
  <li>what previous action led to this success</li>
  <li>that called the <strong>“Credit Assignment Problem”</strong></li>
  <li>Ask the Question: “What did I do in the past that led to the reward I’m receiving now “</li>
  <li>what action gets the credit</li>
</ul>

<h2 id="6-attribution">6. Attribution</h2>
<ul>
  <li>Related to online advertising concept of attribution</li>
  <li>if we show the user the same ad 10 times before they buy, which ad gets the credit?</li>
  <li>In RL we don’t just assign ad-hoc(点对点) like this
<a href="https://postimg.cc/wtdR4kKQ"><img src="https://i.postimg.cc/yNDhdfvt/55123.png" width="500px" title="source: imgur.com" /></a></li>
</ul>

<h2 id="7-delayed-rewards">7. Delayed Rewards</h2>
<ul>
  <li>Delayed Rewards: another way of thinking of the same thing</li>
  <li>Credit assignment: Present -&gt; Past</li>
  <li>Delayed from the other direction: Present -&gt; Future</li>
  <li>Related to field known as “planning”</li>
  <li>the idea of delayed rewards tell us that an AI needs to have the ability of foresight or planning, planning is actually a field of study that crosses over with reinforcement learning</li>
</ul>

<h2 id="8-scenario">8. Scenario</h2>
<p><a href="https://postimg.cc/Cz5FYnHT"><img src="https://i.postimg.cc/SN7j6Wns/5514123412.png" width="500px" title="source: imgur.com" /></a></p>
<ul>
  <li>2 possible next states from A: B or C</li>
  <li>50% probability of ending up in either</li>
  <li>Reasonable value of A?</li>
  <li>Value(A) = 0.5x1 + 0.5x0 = 0.5</li>
</ul>

<p><a href="https://postimg.cc/RWCwvRpC"><img src="https://i.postimg.cc/7Z09Mtgz/5123124213.png" width="500px" title="source: imgur.com" /></a></p>

<ul>
  <li>1 Possible next state form A:B</li>
  <li>100% Probability of ending up in B</li>
  <li>Reasonable value for A ?</li>
  <li>Value(A) = 1 x 1 = 1</li>
  <li>Value tells us the “Future goodness” of a state</li>
</ul>

<h2 id="9-value-function">9. Value Function</h2>
<p><a href="https://postimg.cc/0bKHv932"><img src="https://i.postimg.cc/3JtMgRFp/525.jpg" width="500px" title="source: imgur.com" /></a></p>
<ul>
  <li>V(s) - The Value(Taking into Account the probability of all possible future rewards) of a state</li>
  <li>Value is a measure of possible future rewards we may get from being in this state</li>
  <li>reward is immediate(Ex. Jumping on a Goomba will immediately increase our score)(Ex2. Standing in front of a Goomba will not increase our score, but will put us in a position to jump in the next few states)</li>
  <li>Estimating the value function is a central task in RL</li>
  <li>Not RL algorithms require it(Ex, Evolutionary algorithm that mutates &amp; spawns offspring, only those who survive the longest make it to next generation)</li>
  <li>by pure evolution + natural selection we can breed better and better agents</li>
  <li>but not the type of algorithm we are interested in for RL most of the time</li>
</ul>

<h1 id="10-efficiency">10. Efficiency</h1>
<p><a href="https://postimg.cc/nsyfqDpQ"><img src="https://i.postimg.cc/9MMX5dk1/512124123124.png" width="500px" title="source: imgur.com" /></a></p>
<ul>
  <li>the Value function is a fast &amp; efficient way of searching the game tree</li>
  <li>time consuming to enumerate every possible state transition and their probabilities of occurring.</li>
  <li>tic-tac-toe: 3^(3*3) = 19683</li>
  <li>Connect 4： 3^(4*4) = 43Millon</li>
  <li>Exponential Growth is never good</li>
  <li>Curse of dimensionality</li>
  <li>V(S) gives answer instantly O(1)</li>
</ul>

<h2 id="11-finding-vs">11. Finding V(s)</h2>
<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>V(s) = E[all future rewards</td>
          <td>S(t) = s]</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>E[x] = average of X</li>
  <li>this is generic algorithm here</li>
  <li>so we need to introduce some constraints here</li>
  <li>iterative algorithm</li>
  <li>initialize V(s):
    <ul>
      <li>V(s) = 1 if s = winning rate</li>
      <li>V(s) = 0 if s = lose or draw</li>
      <li>V(s) = 0.5 otherwise</li>
    </ul>
  </li>
  <li>when we study “real” algorithm, we won’t need such careful initialization</li>
  <li>V(s) can be interpreted(说明) as probability of winning after arriving in s( for this game only )(S.t V(s) is winning rate )</li>
  <li>After we initialize V(s), we update it as follows</li>
</ul>
:ET