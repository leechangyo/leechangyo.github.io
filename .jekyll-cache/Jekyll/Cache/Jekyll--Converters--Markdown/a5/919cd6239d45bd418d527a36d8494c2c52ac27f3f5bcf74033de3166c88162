I"Ux<h1 id="n-step-methods">N-step Methods</h1>
<ul>
  <li>N-step Methods : Further our understanding of TD methods</li>
  <li>we know so far： TD(0)</li>
  <li>we will learn :
    <ul>
      <li>λ = 0 gives us TD(0), λ = 1 gives us monte Carlo</li>
    </ul>
  </li>
  <li>any other λ is a trade-off(协调) between the two</li>
</ul>

<p><a href="https://postimg.cc/n9hf3sZq"><img src="https://i.postimg.cc/gjvckRTt/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>
<ul>
  <li>1-Step TD의 step을 증가시켜 나가면서 n까지 보게 된다면 n-step TD로 일반화 할 수 있다.</li>
  <li>만약 step이 무한대에 가깝게 되면 MC와 동일하게 될 것이다.</li>
  <li>2-Step TD에서 업데이트 방식은 척번째 보상과 두번째 보상 그리고 부전째 상태에서의 Value function의 합으로 업데이트가 된다.</li>
</ul>

<p><a href="https://postimg.cc/62Yp6xpL"><img src="https://i.postimg.cc/J0WDq1wg/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li>n step TD에서의 Value 함수는 N-step 에서 얻은 총 보상에서 기존 Value 함수값과 차이를 알파만큼 가중치하여 더함으로서 업데이트가 되게 됩니다.</li>
</ul>

<p><a href="https://postimg.cc/RJtC76TG"><img src="https://i.postimg.cc/prG9tK5N/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li>n 이 몇일때가 가장 최고의 결과값늘 나타낼까? 위에 그래프가 실험에 대한 결과 값입니다.</li>
  <li>실시간 업데이트하는 온라인 방식, 에피소드 완료후 업데이트하는 오프라인 방식에 대한 결과는 비슷하게 나온다.</li>
  <li>n이 커질수록 에러가 커지는 것을 볼 수 있다.</li>
  <li>3~5 step 이 좋은거 같다.</li>
</ul>

<p><a href="https://postimg.cc/8J2fgVKK"><img src="https://i.postimg.cc/MHT7R6yS/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li>n-step 에서 보상값을 다른 값으로 평균을 낼 수 있다.</li>
  <li>하지만 2~4 step에서의 평균을 내어보면 위와 같은 식이 될 것이다.</li>
  <li>이 두가지를 결합하여 효율적으로 만들수 있을까?</li>
</ul>

<p><a href="https://postimg.cc/WFwhDfsS"><img src="https://i.postimg.cc/FR2cB28t/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li>답은 가능하다</li>
  <li>여기서 람다 보상은 모든 n-step 까지의 가중평균된 보상이다.</li>
  <li>기존에 n-step에서 사용한 보상은 총합이였다.</li>
  <li>이렇게 평균을 이용하는 방식은 오류를 더 낮출 수 있다.</li>
  <li>람다의 총합이 1 되도록 하기 위해 (1-lamda)계수로 노멀라이즈를 하여 0부터 1까지의 값을 갖도록 합니다.</li>
  <li><strong>람다는 n-step이 커질 수록 보상에 대한 값을 감소시키게 합니다.</strong></li>
  <li>마지막에 공식이 TD-Lamda 의 Value 함수입니다.</li>
</ul>

<p><a href="https://postimg.cc/HcVqsMx6"><img src="https://i.postimg.cc/KzNxfryh/4564.png" width="700px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li>Step 시간이 흐를수록 weight가 지수형태로 감소하는 것을 볼수 있다.</li>
  <li>그리고 이들의 총합은 1이 된다.</li>
  <li><strong>지수형태의 가중치</strong> 를 사용하는 것이 알고리즘 연상에 효율성을 주고 메모리를 덜 사용하게 되는 이점이 있다.</li>
</ul>

<p><a href="https://postimg.cc/t1Xp3DRJ"><img src="https://i.postimg.cc/1zFXZYLG/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li>Value 함수가 업데이트 하는 과정</li>
  <li>동일하게 n-step까지 도닿라고 나서 얻은 보상들을 lamda를 이용하여 보상값을 업데이트 하게 된다.</li>
  <li>정방향의 관점에서 보면 이론접이다.</li>
  <li>반대방향 관점에서 보면 컴퓨터 적이다.</li>
  <li>반대 방향으로 보면 TD(Lamda) 알고리즘은 온라인 방식과 같이 매 step마다 업데이트가 된다.</li>
</ul>

<p><a href="https://postimg.cc/HJJTxn6s"><img src="https://i.postimg.cc/5tgysFvv/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li>위에서 번개가 발생한 이유는 자주 발생한 것이 영향이 큰지 최근에 발생한 것이 영향이 큰지를 결합하여 사용할 수 있다.</li>
  <li>하나의 특정한 state를 방문하는 횟수에 따라서 Eligibility Traces해보면 위에와 같이 나온다.</li>
</ul>

<p><a href="https://postimg.cc/HVRqzmHV"><img src="https://i.postimg.cc/pLLP2V6Y/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li>이를 적용해보면 각 state 마다 업데이트가 발생 될때 TD에러의 비율 만큼 업데이트를 가중적용하는 한다.</li>
  <li>이것은 에피소드의 길이보다 짧은 기억을 하는 단기 메모리 같은 역할을 한다.</li>
</ul>

<p><a href="https://postimg.cc/xkKfyj4P"><img src="https://i.postimg.cc/pdSmvpQ3/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li>lamda는 얼마나 빨리 값을 감소시키는가를 의미한다.</li>
  <li>lamda의 값이 0이 되면 완전 가파르게 decay가 발생할 것이다.</li>
  <li>결국 현재 state의 value 함수만 업데이트가 되며 니는 TD(0)가 동일한 방식이 된다.</li>
</ul>

<p><a href="https://postimg.cc/239WMT0B"><img src="https://i.postimg.cc/wTgktGJF/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li>반대로 lamda가 1이 되면 에피소드를 모두 커버하게 된다.</li>
  <li>MC와 같은 방식으로 오프라인 업데이트가 된다.</li>
</ul>

<p><a href="https://postimg.cc/t1XPLgVB"><img src="https://i.postimg.cc/GpDQT83n/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li>value function can be described recursively(bellman’s equation)</li>
</ul>

<p><a href="https://postimg.cc/tY4gQVrZ"><img src="https://i.postimg.cc/MKyfB7VD/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li>we can estimate V by taking the sample mean of G’s</li>
</ul>

<p><a href="https://postimg.cc/CR0mxSqq"><img src="https://i.postimg.cc/kg6Zw5Mf/Capture.png" width="400px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li>TD(0) is to combine these 2 things to estimate G itself(all we know for sure is r)</li>
</ul>

<p><a href="https://postimg.cc/K10dCJ9w"><img src="https://i.postimg.cc/XvW4GP9j/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li>All we know for sure is R</li>
  <li>it’s plausible(有道理的) to ask: what if we use more r’s and less of V?</li>
</ul>

<p><a href="https://postimg.cc/3dC0TvQ2"><img src="https://i.postimg.cc/3JzX2XLf/Capture.png" width="500px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li>Monte Carlo is just the extreme of this, where we don’t use V at all</li>
</ul>

<p><a href="https://postimg.cc/WFRL7SbZ"><img src="https://i.postimg.cc/vBTYbSJ0/Capture.png" width="500px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li>Let’s superscript G to tell us how many R’s we are using</li>
</ul>

<p><a href="https://postimg.cc/kB92MvCf"><img src="https://i.postimg.cc/7YJ0VKW8/Capture.png" width="500px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li>gives us a discrete transition from TD(0) to Monte Carlo</li>
  <li><strong>TD(0) - wait 1 step to update V</strong></li>
  <li>Monte Carlo - Wait until end of episode to update V</li>
  <li>N-step -wait N step to update V
    <ul>
      <li>why? we need to collect all the R’s</li>
    </ul>
  </li>
</ul>

<p><a href="https://postimg.cc/6TpNwM19"><img src="https://i.postimg.cc/SQWQ656z/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li>For the actual update(which is in terms of G already). no change is needed</li>
  <li>the only change is to G itself</li>
</ul>

<p><a href="https://postimg.cc/Y4WkTR8p"><img src="https://i.postimg.cc/g0MJXBZ6/Capture.png" width="500px" title="source: imgur.com" /><a></a></a></p>

<h2 id="control">Control</h2>
<ul>
  <li>Use Q instead of V</li>
  <li>Ex. SARSA</li>
</ul>

<p><a href="https://postimg.cc/23r42K87"><img src="https://i.postimg.cc/HW7BWqqR/Capture.png" width="500px" title="source: imgur.com" /><a></a></a></p>

<blockquote>
  <p>Code</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># https://deeplearningcourses.com/c/deep-reinforcement-learning-in-python
# https://www.udemy.com/deep-reinforcement-learning-in-python
</span><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">print_function</span><span class="p">,</span> <span class="n">division</span>
<span class="kn">from</span> <span class="nn">builtins</span> <span class="kn">import</span> <span class="nb">range</span>
<span class="c1"># Note: you may need to update your version of future
# sudo pip install -U future
#
# Note: gym changed from version 0.7.3 to 0.8.0
# MountainCar episode length is capped at 200 in later versions.
# This means your agent can't learn as much in the earlier episodes
# since they are no longer as long.   
#
# Adapt Q-Learning script to use N-step method instead
</span>
<span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">gym</span> <span class="kn">import</span> <span class="n">wrappers</span>
<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span>

<span class="c1"># code we already wrote
</span><span class="kn">import</span> <span class="nn">q_learning</span>
<span class="kn">from</span> <span class="nn">q_learning</span> <span class="kn">import</span> <span class="n">plot_cost_to_go</span><span class="p">,</span> <span class="n">FeatureTransformer</span><span class="p">,</span> <span class="n">Model</span><span class="p">,</span> <span class="n">plot_running_avg</span>

</code></pre></div></div>

<blockquote>
  <p>SGDRegressor</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SGDRegressor</span><span class="p">:</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="mf">1e-2</span>

  <span class="k">def</span> <span class="nf">partial_fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
      <span class="n">D</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">D</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">D</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr</span><span class="o">*</span><span class="p">(</span><span class="n">Y</span> <span class="o">-</span> <span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">))</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">)</span>
    <span class="c1"># replace SKLearn Regressor
</span><span class="n">q_learning</span><span class="o">.</span><span class="n">SGDRegressor</span> <span class="o">=</span> <span class="n">SGDRegressor</span>

<span class="c1"># calculate everything up to max[Q(s,a)]
# Ex.
# R(t) + gamma*R(t+1) + ... + (gamma^(n-1))*R(t+n-1) + (gamma^n)*max[Q(s(t+n), a(t+n))]
# def calculate_return_before_prediction(rewards, gamma):
#   ret = 0
#   for r in reversed(rewards[1:]):
#     ret += r + gamma*ret
#   ret += rewards[0]
#   return ret
</span></code></pre></div></div>

<blockquote>
  <p>play</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># returns a list of states_and_rewards, and the total reward
</span><span class="k">def</span> <span class="nf">play_one</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
    <span class="c1"># n is n-step
</span>  <span class="n">observation</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
  <span class="n">done</span> <span class="o">=</span> <span class="bp">False</span>
  <span class="n">totalreward</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="n">rewards</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">states</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">actions</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">iters</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="c1"># array of [gamma^0, gamma^1, ..., gamma^(n-1)]
</span>  <span class="n">multiplier</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">gamma</span><span class="p">]</span><span class="o">*</span><span class="n">n</span><span class="p">)</span><span class="o">**</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
  <span class="c1"># while not done and iters &lt; 200:
</span>  <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span> <span class="ow">and</span> <span class="n">iters</span> <span class="o">&lt;</span> <span class="mi">10000</span><span class="p">:</span>
    <span class="c1"># in earlier versions of gym, episode doesn't automatically
</span>    <span class="c1"># end when you hit 200 steps
</span>    <span class="n">action</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">sample_action</span><span class="p">(</span><span class="n">observation</span><span class="p">,</span> <span class="n">eps</span><span class="p">)</span>

    <span class="n">states</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">observation</span><span class="p">)</span>
    <span class="n">actions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>

    <span class="n">prev_observation</span> <span class="o">=</span> <span class="n">observation</span>
    <span class="n">observation</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>

    <span class="n">rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>

    <span class="c1"># update the model
</span>    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">n</span><span class="p">:</span>
      <span class="c1"># return_up_to_prediction = calculate_return_before_prediction(rewards, gamma)
</span>      <span class="n">return_up_to_prediction</span> <span class="o">=</span> <span class="n">multiplier</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">rewards</span><span class="p">[</span><span class="o">-</span><span class="n">n</span><span class="p">:])</span>
      <span class="n">G</span> <span class="o">=</span> <span class="n">return_up_to_prediction</span> <span class="o">+</span> <span class="p">(</span><span class="n">gamma</span><span class="o">**</span><span class="n">n</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="nb">max</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">observation</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
      <span class="n">model</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">states</span><span class="p">[</span><span class="o">-</span><span class="n">n</span><span class="p">],</span> <span class="n">actions</span><span class="p">[</span><span class="o">-</span><span class="n">n</span><span class="p">],</span> <span class="n">G</span><span class="p">)</span>

    <span class="c1"># if len(rewards) &gt; n:
</span>    <span class="c1">#   rewards.pop(0)
</span>    <span class="c1">#   states.pop(0)
</span>    <span class="c1">#   actions.pop(0)
</span>    <span class="c1"># assert(len(rewards) &lt;= n)
</span>
    <span class="n">totalreward</span> <span class="o">+=</span> <span class="n">reward</span>
    <span class="n">iters</span> <span class="o">+=</span> <span class="mi">1</span>

  <span class="c1"># empty the cache
</span>  <span class="k">if</span> <span class="n">n</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
    <span class="n">rewards</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">states</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">actions</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">rewards</span> <span class="o">=</span> <span class="n">rewards</span><span class="p">[</span><span class="o">-</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">:]</span>
    <span class="n">states</span> <span class="o">=</span> <span class="n">states</span><span class="p">[</span><span class="o">-</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">:]</span>
    <span class="n">actions</span> <span class="o">=</span> <span class="n">actions</span><span class="p">[</span><span class="o">-</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">:]</span>
  <span class="c1"># unfortunately, new version of gym cuts you off at 200 steps
</span>  <span class="c1"># even if you haven't reached the goal.
</span>  <span class="c1"># it's not good to do this UNLESS you've reached the goal.
</span>  <span class="c1"># we are "really done" if position &gt;= 0.5
</span>  <span class="k">if</span> <span class="n">observation</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mf">0.5</span><span class="p">:</span>
    <span class="c1"># we actually made it to the goal
</span>    <span class="c1"># print("made it!")
</span>    <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
      <span class="n">G</span> <span class="o">=</span> <span class="n">multiplier</span><span class="p">[:</span><span class="nb">len</span><span class="p">(</span><span class="n">rewards</span><span class="p">)]</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span>
      <span class="n">model</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">states</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">actions</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">G</span><span class="p">)</span>
      <span class="n">rewards</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
      <span class="n">states</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
      <span class="n">actions</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="c1"># we did not make it to the goal
</span>    <span class="c1"># print("didn't make it...")
</span>    <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
      <span class="n">guess_rewards</span> <span class="o">=</span> <span class="n">rewards</span> <span class="o">+</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">rewards</span><span class="p">))</span>
      <span class="n">G</span> <span class="o">=</span> <span class="n">multiplier</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">guess_rewards</span><span class="p">)</span>
      <span class="n">model</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">states</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">actions</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">G</span><span class="p">)</span>
      <span class="n">rewards</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
      <span class="n">states</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
      <span class="n">actions</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">totalreward</span>
</code></pre></div></div>

<blockquote>
  <p>main</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">'__main__'</span><span class="p">:</span>
  <span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s">'MountainCar-v0'</span><span class="p">)</span>
  <span class="n">ft</span> <span class="o">=</span> <span class="n">FeatureTransformer</span><span class="p">(</span><span class="n">env</span><span class="p">)</span>
  <span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">ft</span><span class="p">,</span> <span class="s">"constant"</span><span class="p">)</span>
  <span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.99</span>

  <span class="k">if</span> <span class="s">'monitor'</span> <span class="ow">in</span> <span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">:</span>
    <span class="n">filename</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">basename</span><span class="p">(</span><span class="n">__file__</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">'.'</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">monitor_dir</span> <span class="o">=</span> <span class="s">'./'</span> <span class="o">+</span> <span class="n">filename</span> <span class="o">+</span> <span class="s">'_'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">())</span>
    <span class="n">env</span> <span class="o">=</span> <span class="n">wrappers</span><span class="o">.</span><span class="n">Monitor</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">monitor_dir</span><span class="p">)</span>


  <span class="n">N</span> <span class="o">=</span> <span class="mi">300</span>
  <span class="n">totalrewards</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
  <span class="n">costs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
    <span class="c1"># eps = 1.0/(0.1*n+1)
</span>    <span class="n">eps</span> <span class="o">=</span> <span class="mf">0.1</span><span class="o">*</span><span class="p">(</span><span class="mf">0.97</span><span class="o">**</span><span class="n">n</span><span class="p">)</span>
    <span class="n">totalreward</span> <span class="o">=</span> <span class="n">play_one</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span>
    <span class="n">totalrewards</span><span class="p">[</span><span class="n">n</span><span class="p">]</span> <span class="o">=</span> <span class="n">totalreward</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"episode:"</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="s">"total reward:"</span><span class="p">,</span> <span class="n">totalreward</span><span class="p">)</span>
  <span class="k">print</span><span class="p">(</span><span class="s">"avg reward for last 100 episodes:"</span><span class="p">,</span> <span class="n">totalrewards</span><span class="p">[</span><span class="o">-</span><span class="mi">100</span><span class="p">:]</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
  <span class="k">print</span><span class="p">(</span><span class="s">"total steps:"</span><span class="p">,</span> <span class="o">-</span><span class="n">totalrewards</span><span class="o">.</span><span class="nb">sum</span><span class="p">())</span>

  <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">totalrewards</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Rewards"</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

  <span class="n">plot_running_avg</span><span class="p">(</span><span class="n">totalrewards</span><span class="p">)</span>

  <span class="c1"># plot the optimal state-value function
</span>  <span class="n">plot_cost_to_go</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
</code></pre></div></div>
<p>Reference:</p>

<p><a href="https://www.udemy.com/course/artificial-intelligence-reinforcement-learning-in-python/">Artificial Intelligence Reinforcement Learning</a></p>

<p><a href="https://www.udemy.com/deep-reinforcement-learning-in-python/">Advance AI : Deep-Reinforcement Learning</a></p>

<p><a href="https://www.udemy.com/cutting-edge-artificial-intelligence/learn/lecture/14650508#overview">Cutting-Edge Deep-Reinforcement Learning</a></p>
:ET