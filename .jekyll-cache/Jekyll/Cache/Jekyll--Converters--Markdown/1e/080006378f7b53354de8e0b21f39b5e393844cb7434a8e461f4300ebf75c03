I"<h1 id="markov-decision-processes">Markov Decision Processes</h1>
<ul>
  <li>this section: formalize some RL concepts we already know about</li>
  <li>Agent, Environment, action, state, reward, episode</li>
  <li>Formal Framework: Markov Decision Processes(MDPs)</li>
</ul>

<h2 id="1-typical-game-by-solving-mdps-is-gridword">1. Typical Game by solving MDPs is GridWord</h2>
<p><a href="https://postimg.cc/VdS9ym01"><img src="https://i.postimg.cc/FspTLK9f/54123.png" width="500px" title="source: imgur.com" /></a></p>
<ul>
  <li>possible actions:</li>
  <li>up,down,left,right</li>
  <li>(1,1) -&gt; wall,can’t go here</li>
  <li>(0,3) -&gt; Terminal(+1 Reward)</li>
  <li>(1,3) -&gt; Terminal(-1 Reward)</li>
  <li>12 Positions(w x h = 3 x 4 = 12)</li>
  <li>11 states (where the robot is)</li>
  <li>4 actions</li>
</ul>

<h2 id="2-markov-property">2. Markov Property</h2>
<ul>
  <li>Given a sequence:
 <a href="https://postimg.cc/fJKVgPBZ"><img src="https://i.postimg.cc/2Spn3RWq/4121233.png" width="500px" title="source: imgur.com" /></a></li>
  <li>Generally, this can’t be simplified:
 <a href="https://postimg.cc/jw0qj6ZZ"><img src="https://i.postimg.cc/sX1MwwSd/41233.png" width="500px" title="source: imgur.com" /></a></li>
  <li>First-order Markov:
 <a href="https://postimg.cc/bZ4tfBS5"><img src="https://i.postimg.cc/zfGSKmzq/2.png" width="500px" title="source: imgur.com" /></a></li>
  <li>second-order Markov:
 <a href="https://postimg.cc/bDgrrCdp"><img src="https://i.postimg.cc/cCynDPSg/44.png" width="500px" title="source: imgur.com" /></a></li>
  <li>Simple Example
```
Consider the sentence : “Let’s do a simple example”
Given:”let’s do a simple”
Predict the new word? Easy</li>
</ul>

<p>Given: “simple”
Predict the next word? not as easy</p>

<p>Given: “a”
predict the next word? very difficult</p>

<p>is the Markov Property limiting? Not necessarily
```</p>

<h2 id="3-markov-property-in-rl">3. Markov Property in RL</h2>
<ul>
  <li>{S(t), A(t)} produces 2 things -&gt; {S(t+1),R(t+1)}</li>
  <li>Markov Property:
<a href="https://postimg.cc/bDgrrCdp"><img src="https://i.postimg.cc/cCynDPSg/44.png" width="500px" title="source: imgur.com" /></a></li>
  <li>Convenience notation
<a href="https://postimg.cc/LJrhqgTQ"><img src="https://i.postimg.cc/g00hTv6p/44.png" width="500px" title="source: imgur.com" /></a></li>
  <li>joint on s’ and r, conditioned on 2 other variables</li>
  <li>different from “usual” Markov: 1 RV(Random Variable) Conditioned on 1 other RV</li>
</ul>

<h2 id="4-other-conditional-distributions">4. Other Conditional distributions</h2>
<ul>
  <li>can be found using rules of probability</li>
  <li>For pretty much all cases we’ll consider these will be deterministic</li>
  <li>i.e. states will always give us the same reward</li>
  <li>Action will always bring us to same next state</li>
  <li>But, these distributions are part of the core theory of RL
<a href="https://postimg.cc/BL5dTRGP"><img src="https://i.postimg.cc/W3xTvPW8/44.png" width="500px" title="source: imgur.com" /></a></li>
</ul>

<h2 id="5-is-the-markov-assumption-limiting">5. Is the Markov Assumption limiting?</h2>
<ul>
  <li>Not necessarily</li>
  <li>Recent application: DeepMind used concatenation(一系列相关联的事物) of 4 most recent frames to represent state when playing Atari Games</li>
  <li>State can be made up of anything from anytime past to current</li>
  <li>Typically think of state right now = something we measure right now</li>
  <li>Also, don’t need to use raw data(state can be features transformed from raw data)</li>
  <li>Any input from agent’s sensors can be used to form state</li>
</ul>

<h2 id="6-markov-decision-processesmdps">6. Markov Decision Processes(MDPs)</h2>
<ul>
  <li>Any RL task with a set of States, actions, and rewards, that follows the Markov Property, is a MDP</li>
  <li>MDP is defined as the collection of
    <ul>
      <li>set of states</li>
      <li>set of actions</li>
      <li>set of rewards</li>
      <li>State-Transition probability, Reward probability(as defined jointly(连带地) earlier)</li>
      <li>Discount factor</li>
    </ul>
  </li>
  <li>Often written as a 5 tuples</li>
</ul>

<h2 id="7-policy">7. Policy</h2>
<ul>
  <li>One more piece to complete the puzzle - the policy(denoted by π)</li>
  <li>Technically π is not part of the MDP itself, but it, along with the value function, form the solution</li>
  <li>Left out until now because it’s a weird symbol</li>
  <li>there’s no “equation” for it</li>
  <li>how do we write epsilon-greedy as an equation? it’s more like an algorithm</li>
  <li>the only exception is the <em>Optimal policy</em>, which can be defined in terms of the value function</li>
  <li>think of π as shorthand for the algorithm the agent is using to navigate the environment
    <h2 id="8-번외">8. 번외</h2>
  </li>
  <li>(Epsilon Greedy는 주사위 한번 던져서 결정하고 그 action 다음번에 사용하는 것이 on Policy)</li>
  <li>(Q-Learning은 다음 Step에서 실제로 사용할 action과 상관 없이 Max Q 취하기 때문에 off policy)
    <ul>
      <li>각 지점에서 계속 최적값을 찾으면서 Future Reward 计算</li>
      <li>벨멘 방정식을 이용하여 반복적으로 Q함수를 근사시킬 수 있음</li>
    </ul>
  </li>
  <li>(Sarsa, 다음번에 게임에 넣어줄 action을 미리 계산해서 사용)</li>
  <li>Reinforcement Learning Two Problem
    <ul>
      <li>Credit Assignment Problem</li>
      <li>Exploration-exploitation</li>
    </ul>
  </li>
  <li>DQN Property
    <ul>
      <li>Target Q function
        <ul>
          <li>학습 대상이 되는 Q함수가 학습이 되면서 계속 바뀌는 문제</li>
          <li>Learning Q-Fucntion from Gradient Descent</li>
          <li>일정 스텝 될 때마다 Q 함수의 가중치를 타켓 Q함수에 업데이트</li>
        </ul>
      </li>
      <li>Replay Memory
        <ul>
          <li>학습의 재료가 되는 Sample 저장소</li>
          <li>즉시 훈련하지 않고, 메로리 저장</li>
          <li>일정 수의 Samplr을 랜덤으로 꺼내 학습</li>
        </ul>
      </li>
      <li>Q-function made by ANN
        <ul>
          <li>DQN은 인공신경망으로 Policy function을 Approximate</li>
        </ul>
      </li>
      <li>Q function of DQN 특징
        <ol>
          <li>Model Free :
            <ul>
              <li>모델이 없고 샘플로 부터 직접적으로 정책을 근사화</li>
              <li>대부분 ANN을 활용한 훈련으로 Dimension Curse에 벗어난다.</li>
            </ul>
          </li>
          <li>Off-Policy
            <ul>
              <li>Learn thing from another agent’s action</li>
              <li>타겟 정책과 행동 정책 나눈다.</li>
            </ul>
            <ul>
              <li>Target policy : 우리가 강화학습 에이전트에게 가르치기 위한 기준이 되는 정책</li>
              <li>행동 Policy : 탐험을 하며 새로운 행동을 만들어 내는 정책
      - 두가지 폴리시를 다루어야 함으로 구현하기 어려움</li>
            </ul>
          </li>
          <li>MiniBatch
            <ul>
              <li>많은 데이터 중 임의로 샘플을 뽑아 학습시키는 것</li>
              <li>연속적인 샘플들 간의 강한 상관관계를 제거</li>
            </ul>
          </li>
          <li>Value-Based Reinforcement Learning
            <ul>
              <li>at first, Let Value Function Approximating to make a Policy</li>
            </ul>
          </li>
          <li>Decaying Epsilon-Greedy
            <ul>
              <li>at first, Random Act -&gt; random act to be reduced gradually -&gt; when it became 1% of random action, it stop</li>
              <li>Two Hyper parameter:</li>
            </ul>
          </li>
        </ol>
      </li>
    </ul>
  </li>
</ul>
:ET