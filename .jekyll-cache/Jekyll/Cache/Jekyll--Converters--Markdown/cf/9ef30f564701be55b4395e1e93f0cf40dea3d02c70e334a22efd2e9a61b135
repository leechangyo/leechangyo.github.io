I"<h1 id="intro-to-dynamic-programming">intro to Dynamic Programming</h1>
<ul>
  <li>Solutions to MDPs</li>
  <li>Centrepiece of MDP: The bellman Equation
<a href="https://postimg.cc/5XWK8Hhk"><img src="https://i.postimg.cc/zGymZK05/4141313123.png" width="500px" title="source: imgur.com" /></a></li>
  <li>if we look carefully, this can be used to solve for V(s) Directly</li>
  <li>I S I equations, I S I unknowns(linear Problem)</li>
  <li>Many entries(进入(指行动)) will be 0, since transitions s -&gt; s’ are sparse</li>
  <li>Instead, we will use <strong>Iterative Policy evaluation</strong></li>
</ul>

<h2 id="1-iterative-policy-evaluation">1. Iterative Policy evaluation</h2>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def iterative_policy_evaluation(π):
  initialize V(s) = 0 for all s ∈ S
  while true :
    ∆ = 0
    for each s ∈ S:
      old_v = V(s)
</code></pre></div></div>

<p><a href="https://postimg.cc/cvDD2DKP"><img src="https://i.postimg.cc/cJWVK2d1/4121241314123.png" width="500px" title="source: imgur.com" /></a></p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>     ∆ = max(∆, I V(s) - old_v I )
     if ∆ &lt; Threshold: break

     return V(s)
</code></pre></div></div>

<ul>
  <li>Technically, it’s defined where V(s) at iteration k+1 is updated from V(s) at iteration k</li>
  <li>But we can update V(s) “in place”, to use the most recently updated values</li>
  <li>Converges(汇集) Faster
<a href="https://postimg.cc/0MJPbKR2"><img src="https://i.postimg.cc/Ssgs1cg8/51212.png" width="500px" title="source: imgur.com" /></a></li>
</ul>

<h3 id="definition">Definition</h3>
<ul>
  <li>What we just did(Finding V(s) given a policy) is called the **Prediction Problem””</li>
</ul>
:ET