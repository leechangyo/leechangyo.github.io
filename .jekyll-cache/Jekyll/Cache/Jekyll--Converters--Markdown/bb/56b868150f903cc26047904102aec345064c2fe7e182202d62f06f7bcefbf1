I"*G<h1 id="ucb1">UCB1</h1>
<p><a href="https://postimg.cc/R3d6WKJM"><img src="https://i.postimg.cc/hjW930S7/52139182305921.png" width="500px" title="source: imgur.com" /></a></p>

<ul>
  <li>Another Method to Solve Explore-Exploit ( also from A/B Testing Classes)</li>
  <li>Idea: confidence bounds</li>
  <li>Intuitively, we know that sample mean from 10 samples is less accurate than a sample mean from 1000 samples</li>
  <li>Chefnoff Hoeffiding bound:
<a href="https://postimg.cc/YvWgXvG7"><img src="https://i.postimg.cc/T3k93mTK/521252152525215.png" width="200px" title="source: imgur.com" /></a></li>
  <li>Looks complicated, but leads to a simple algorithm
<a href="https://postimg.cc/wyGj8kZG"><img src="https://i.postimg.cc/j2swfgXt/1242141413123.png" width="200px" title="source: imgur.com" /></a></li>
  <li>square(2*ln(N)/N_j) &lt;- “Choose” epsilon equal to this upper bound</li>
  <li>N = number of times i’ve played in total, N_j = number of times i’ve played bandit j</li>
  <li>How do we use this formular? Same as optimistic initial values Method</li>
  <li>just be greedy, but with respect to <script type="math/tex">\ln{X}_{UCB-J}</script></li>
  <li>Key: ratio Between ln(N) and $N_j$</li>
  <li>if only $N_j$ is small, then upper bound is high</li>
  <li>if $N_j$ is large, upper bound is small</li>
  <li>ln(N) grow more slowly than N, so eventually all upper bounds will shrink</li>
  <li>by then we’ve collected lots of data, so it’s ok</li>
  <li>so we converge to purely greedy strategy</li>
  <li>ε-greedy는 추측값이나 불확실성과랑 관계없이 무조건 랜덤하게 실행하는 반면에 어떤 액션을 했을 때, 그 액션은 많이 해봤으니 어느정도 불확실할 것이라는 전재로 불확실성을 고려하여 액션을 취한다.)</li>
  <li>즉 시간이 지날 수록 ln(N)의 값은 작아지는데, 많은 샘플들이 쌓이면 0으로 수렴함으로 결곡 적은 샘플을 가지고 있을때 취하는 액션들을 보정해준다고 보면 된다.</li>
</ul>

<blockquote>
  <p>Code</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">Comparing_epsilons</span> <span class="kn">import</span> <span class="n">run_experiment</span> <span class="k">as</span> <span class="n">run_experiment_eps</span>

<span class="k">class</span> <span class="nc">bandit</span><span class="p">:</span>
    <span class="s">"""docstring for ."""</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">upper_limit</span> <span class="o">=</span> <span class="mi">0</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">m</span> <span class="o">=</span> <span class="n">m</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">=</span> <span class="n">upper_limit</span>
        <span class="c1">#the mean instance variable is our estimate of the bandit's mean
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">N</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="k">def</span> <span class="nf">pull</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">()</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">m</span>
    <span class="c1"># the pull function which is simulates pulling the bandit's arm
</span>    <span class="c1"># every bandit's reward will be a gassium with unit variant
</span>

    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">N</span> <span class="o">+=</span><span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="mf">1.0</span><span class="o">/</span><span class="bp">self</span><span class="o">.</span><span class="n">N</span><span class="p">)</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">+</span> <span class="mf">1.0</span><span class="o">/</span><span class="bp">self</span><span class="o">.</span><span class="n">N</span><span class="o">*</span><span class="n">x</span>

    <span class="c1">#Finally, we have the update function which takes an X, which is the latest sample recieved from the bandit
</span></code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">ucb</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">nj</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">nj</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">float</span><span class="p">(</span><span class="s">'inf'</span><span class="p">)</span> <span class="c1">#infinity
</span>
    <span class="k">return</span> <span class="n">mean</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="o">/</span> <span class="n">nj</span><span class="p">)</span>
</code></pre></div></div>

<script async="" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

<div align="center" style="margin: 1em 0;">
<ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-5150894678574694" data-ad-slot="9221331439" data-ad-format="auto" data-full-width-responsive="true"></ins>
     </div>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">run_experiment</span><span class="p">(</span><span class="n">m1</span><span class="p">,</span> <span class="n">m2</span><span class="p">,</span> <span class="n">m3</span><span class="p">,</span> <span class="n">N</span><span class="p">):</span>
    <span class="n">bandits</span> <span class="o">=</span> <span class="p">[</span><span class="n">bandit</span><span class="p">(</span><span class="n">m1</span><span class="p">),</span> <span class="n">bandit</span><span class="p">(</span><span class="n">m2</span><span class="p">),</span> <span class="n">bandit</span><span class="p">(</span><span class="n">m3</span><span class="p">)]</span>

    <span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
        <span class="n">j</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">([</span><span class="n">ucb</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">mean</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">b</span><span class="o">.</span><span class="n">N</span><span class="p">)</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">bandits</span><span class="p">])</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">bandits</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">pull</span><span class="p">()</span>
        <span class="n">bandits</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># for the plot
</span>        <span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>
    <span class="n">cumulative_average</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">data</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">N</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

  <span class="c1"># for b in bandits:
</span>  <span class="c1">#   print("bandit nj:", b.N)
</span>
  <span class="c1"># plot moving average ctr
</span>    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">cumulative_average</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">N</span><span class="p">)</span><span class="o">*</span><span class="n">m1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">N</span><span class="p">)</span><span class="o">*</span><span class="n">m2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">N</span><span class="p">)</span><span class="o">*</span><span class="n">m3</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xscale</span><span class="p">(</span><span class="s">'log'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

  <span class="c1"># for b in bandits:
</span>  <span class="c1">#   print(b.mean)
</span>    <span class="k">return</span> <span class="n">cumulative_average</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">'__main__'</span><span class="p">:</span>
    <span class="n">c_1</span> <span class="o">=</span> <span class="n">run_experiment_eps</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">100000</span><span class="p">)</span>
    <span class="n">ucb</span> <span class="o">=</span> <span class="n">run_experiment</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mi">100000</span><span class="p">)</span>

    <span class="c1"># log scale plot
</span>    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">c_1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'eps = 0.1'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ucb</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'ucb1'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xscale</span><span class="p">(</span><span class="s">'log'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

    <span class="c1"># linear plot
</span>    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">c_1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'eps = 0.1'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ucb</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'ucb1'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<h1 id="bayesian-method">Bayesian Method</h1>
<p><a href="https://postimg.cc/QKtZ0VDD"><img src="https://i.postimg.cc/Bb5ZZ1wj/4124124141.png" width="500px" title="source: imgur.com" /></a></p>
<ul>
  <li>thompson sampling(각 보상(reward) 분포의 파라미터 확률 변수를 보고 이 파라미터의 분포로 부터 무작위로 추출하여 해당 값에 대한 Reward 기대값이 가장 높은 기계를 선택한다. 그리고 나서 Bayesian 정리를 이용하여 기계에 대한 파라미터 분포를 업데이트 한다. Iteration을 하여 점점 높아지는 보상의 평균의 기계를 선택하게 된다.)</li>
  <li>Let’s think about confidence intervals again</li>
  <li>we know Intuitively that sample mean from 10 samples is less accurate than sample mean from 1000 samples</li>
  <li>Central Limit Theorem says that sample mean is <strong>approximately Gaussian</strong></li>
  <li>Any Function of RVs is a RV(random Variable)
<a href="https://postimg.cc/p9KtGgn0"><img src="https://i.postimg.cc/R0DVFmCz/141414141412314.png" width="200px" title="source: imgur.com" /></a></li>
  <li>Bayesian paradigm takes this a step further</li>
  <li>what if <em>μ</em> is Also a RV?</li>
  <li>Data is fixed; parameters are random</li>
  <li>we want to find the distribution of the parameter(入数) given the data</li>
  <li>should be more accurate than if did not have the data</li>
  <li>we call this the Posterior
<a href="https://postimg.cc/WD4s39DS"><img src="https://i.postimg.cc/MG7H9Nzk/4112414123131.png" width="200px" title="source: imgur.com" /></a></li>
  <li>Flip the Posterior to find it in terms of likelihood and prior
<a href="https://postimg.cc/14R3XcT8"><img src="https://i.postimg.cc/v8fgP0tt/4124141241312.png" width="500px" title="source: imgur.com" /></a></li>
  <li>Likelihood(연속 사건에서는 구간에 대한 추정은 가능하지만(PDF(Probability Density Function)), 특정사건에 대한 추정이 불가능할때 PDF에서 y값을 이용해서 P값을 구하겠다라는 것이다. 즉 0~100까지 있는 변수중에서 30을 뽑을 확률은 0이지만 likelihood로 보았을때에는 0.4인 것이다. 즉 세번 연속으로 뽑았을떄 2,49,20 이 나올 확룔은 0 X 0 X 0으로 0이지만, Likelihood에서는 0.4 x 0.2 x 0.01 = 0.008 이다.</li>
  <li>Disadvantage of Bayesian method: we must choose the prior
    <ul>
      <li>Non-negligible effect on Posterior
<a href="https://postimg.cc/m13n7HhV"><img src="https://i.postimg.cc/ZnXhtPmk/41414141.png" width="500px" title="source: imgur.com" /></a></li>
    </ul>
  </li>
  <li>Problem: Integral is usually intractable(很难处理的), we need “tricks” to solve this(만약 Bayesian으로 Posterior를 계산한다고 할 때, 각 항목이 일반적인 분포를 따르지 않늗다고 한다면(즉 들쑥 날쑥) 도출하는 방식이 매우 어렵기 때문에 Trick이 필요. 예로 입술에 빨간색이 뭍었을때 김치를 먹었는지 고추장을 먹었는지 예측할 수 있는 것이다.)</li>
  <li>special(likelihood,prior) pairs where we can easily solve this</li>
  <li>called conjugate priors(Likelihood가 특정 분포를 따른다고 할 때, Prior와 Posterior가 동일한 분포면 계산이 빨라진다.)</li>
  <li>Click-Through rate are like coin tosses; likelihood is bernoulli</li>
  <li>look up on wikipedia: what is <a href="https://rpubs.com/sitaramgautam/145048">conjugate prior</a> for bernoulli likelihood? <em>Beta</em></li>
  <li>make sense; beta is in [0,1]</li>
  <li>Likelihood:
<a href="https://postimg.cc/gXBgbHQx"><img src="https://i.postimg.cc/HnTR5St9/41414141414141.png" width="200px" title="source: imgur.com" /></a></li>
  <li>Prior:
<a href="https://postimg.cc/MM8QMMB9"><img src="https://i.postimg.cc/PxCz715n/51511.png" width="200px" title="source: imgur.com" /></a></li>
</ul>
:ET