I"A<h1 id="temporal-difference-td-learning">Temporal Difference (TD) Learning</h1>
<ul>
  <li>this section is a 3rd technique for solving MDPs</li>
  <li>TD = Temporal Difference(TD) Learning</li>
  <li>Combines ideas from DP and MC</li>
  <li>Disadvantage of DP: requires full model of environment, never learns from experience</li>
  <li>MC and TD learn from experience</li>
  <li>MC can only update after completing episode, but DP uses Bootstrapping(Making an initial estimate)</li>
  <li>We will see that TD also uses Bootstrapping and is fully online, can update value during an episode
    <h2 id="1-td-learning">1. TD Learning</h2>
  </li>
  <li>Same approach as before</li>
  <li>First Predict Problem</li>
  <li>them control problem</li>
  <li>2 control methods:
    <ul>
      <li>SARSA</li>
      <li>Q-Learning</li>
    </ul>
  </li>
  <li>Model-Free Reinforcement Learning</li>
  <li>TD methods learn directly from episode of experience.</li>
  <li>no Knowledge of MDP Transition / rewards</li>
  <li>TD learns from incomplete episodes by Bootstrapping</li>
  <li>TD updates a guess towards a guess
<a href="https://postimg.cc/fkPHCsWG"><img src="https://i.postimg.cc/28kPzjsS/41231312313.png" width="500px" title="source: imgur.com" /><a></a></a></li>
  <li>MCì™€ ë‹¤ë¥¸ì ì€, MCëŠ” ì‹¤ì œ ì—í”¼ì†Œë“œê°€ ëë‚˜ê³  ë°›ê²Œ ë˜ëŠ” ë³´ìƒì„ ì‚¬ìš©í•˜ì—¬ Value Functionì„ ì—…ë°ì´íŠ¸ í•˜ì˜€ë‹¤</li>
  <li>TDì—ì„œëŠ” ì‹¤ì œ ë³´ìƒê³¼ ë‹¤ìŒ stepì— ëŒ€í•œ ë¯¸ë˜ì¶”ì •ê°€ì¹˜ë¥¼ ì‚¬ìš©í•´ì„œ í•™ìŠµí•œë‹¤.</li>
  <li>ì´ë•Œ ì‚¬ìš©í•˜ëŠ” ë³´ìƒê³¼ Value Functionì˜ í•©ì„ TD Target
-ê·¸ë¦¬ê³  TD Target ê³¼ ì‹¤ì œ V(S)ì™€ì˜ ì°¨ì´ë¥¼ TD errorë¼ê³  í‘œí˜„í•œë‹¤.
<a href="https://postimg.cc/FfJFfNRh"><img src="https://i.postimg.cc/TPtK4Pv5/14141241313.png" width="500px" title="source: imgur.com" /><a></a></a></li>
  <li>MCì—ì„œì˜ Value functionì´ ì—…ë°ì´íŠ¸ ë˜ëŠ” ê³¼ì •ì´ ì™¼ìª½(ì—í”¼ì†Œë“œê°€ ì „ì²´ì ìœ¼ë¡œ ëë‚˜ì„œ ê·¸ì˜ ë³´ìƒì„ ë‚˜ëˆ„ì–´ ë‹¨ê³„ë³„ë¡œ ì—…ë°ì´íŠ¸)</li>
  <li>TDëŠ” ê° ë‹¨ê³„ë³„ë¡œ ì—…ë°ì´íŠ¸ê°€ ë˜ëŠ” ê³¼ì •ìœ¼ë¡œ ì˜¤ë¥¸ìª½ ê·¸ë¦¼</li>
  <li>TDì˜ ì¥ì ì€ ì—í”¼ì†Œë“œ ì¤‘ê°„ì—ì„œë„ í•™ìŠµì„ í•˜ê²Œ ëœë‹¤ëŠ” ê²ƒì´ë‹¤.</li>
  <li>MCì—ì„œëŠ” ì—í”¼ì†Œë“œê°€ ëë‚ ë•Œê¹Œì§€ ê¸°ë‹¤ë ¸ë‹¤ê°€ ì—…ë°ì´íŠ¸ê°€ ë°œìƒí•˜ê³  í•™ìŠµí•˜ê¸° ë–„ë¬¸ì´ë‹¤.</li>
  <li>TDëŠ” ì¢…ë£Œ ì—†ëŠ” ì—°ì†ì ì¸ ì—í”¼ì†Œë“œì—ì„œë„ í•™ìŠµí•  ìˆ˜ ìˆë‹¤.</li>
  <li>Return $G_t$ = R(t+1)+É£R(t+2)+â€¦+É£^(t-1)$R_T) is unbiased estimate of $v_Ï€$($S_t$)</li>
  <li>True TD Target  R(t+1)+É£V(S(t+1)) is biased estimate of $v_Ï€$($S_t$)</li>
  <li>TD Target is much lower variance than the return:
    <ul>
      <li>return depends on many random actions, transitions, rewards</li>
      <li>TD Target depends on one random action, transition, reward</li>
    </ul>
  </li>
  <li>V policyê°€ ì‹¤ì œ Gì— ëŒ€í•´ì„œ unbiasë¼ í• ë•ŒëŠ” TD Targetë„ V policyë¥¼ ì¶”ì¢…í•˜ê¸° unbiasì´ë‹¤. í•˜ì§€ë§Œ TD Targetì— V policyë¥¼ ì¶”ì •í•˜ëŠ” V(St+1)ë¥¼ ì‚¬ìš©í•˜ê¸°ì—  ì‹¤ì œê°’ì´ ì•„ë‹ˆë¼ ì‹¤ì œê°’ì„ ì¶”ì •í•˜ëŠ” ê°’ì„ìœ¼ë¡œ biasê°€ ë°œìƒí•œë‹¤. ê·¸ë¦¬ê³  TD Targetì€ ë‹¨ì§€ í•˜ë‚˜ì˜ stepì—ì„œë§Œ ê³„ì‚°í•˜ê¸°ì— noiseê°€ ì‘ê²Œ ë˜ë¯€ë¡œ ìƒëŒ€ì ìœ¼ë¡œ varianceê°€ ë‚®ê²Œ ë‚˜íƒ€ë‚œë‹¤.</li>
  <li>MC has high variance, zero bias
    <ul>
      <li>good convergence properties</li>
      <li>even with function approximation</li>
      <li>not very sensitive to inital value</li>
      <li>very simple to understand and use</li>
    </ul>
  </li>
  <li>TD has low variance, some bias
    <ul>
      <li>Usually more efficient than MC</li>
      <li>TD(0) converges to $v_Ï€$(s)</li>
    </ul>
  </li>
</ul>
:ET