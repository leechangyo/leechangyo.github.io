I"‹'<h1 id="markov-decision-processes">Markov Decision Processes</h1>
<ul>
  <li>this section: formalize some RL concepts we already know about</li>
  <li>Agent, Environment, action, state, reward, episode</li>
  <li>Formal Framework: Markov Decision Processes(MDPs)</li>
</ul>

<h2 id="1-typical-game-by-solving-mdps-is-gridword">1. Typical Game by solving MDPs is GridWord</h2>
<p><a href="https://postimg.cc/VdS9ym01"><img src="https://i.postimg.cc/FspTLK9f/54123.png" width="500px" title="source: imgur.com" /></a></p>
<ul>
  <li>possible actions:</li>
  <li>up,down,left,right</li>
  <li>(1,1) -&gt; wall,canâ€™t go here</li>
  <li>(0,3) -&gt; Terminal(+1 Reward)</li>
  <li>(1,3) -&gt; Terminal(-1 Reward)</li>
  <li>12 Positions(w x h = 3 x 4 = 12)</li>
  <li>11 states (where the robot is)</li>
  <li>4 actions</li>
</ul>

<h2 id="2-markov-property">2. Markov Property</h2>
<ul>
  <li>Given a sequence:
 <a href="https://postimg.cc/fJKVgPBZ"><img src="https://i.postimg.cc/2Spn3RWq/4121233.png" width="500px" title="source: imgur.com" /></a></li>
  <li>Generally, this canâ€™t be simplified:
 <a href="https://postimg.cc/jw0qj6ZZ"><img src="https://i.postimg.cc/sX1MwwSd/41233.png" width="500px" title="source: imgur.com" /></a></li>
  <li>First-order Markov:
 <a href="https://postimg.cc/bZ4tfBS5"><img src="https://i.postimg.cc/zfGSKmzq/2.png" width="500px" title="source: imgur.com" /></a></li>
  <li>second-order Markov:
 <a href="https://postimg.cc/bDgrrCdp"><img src="https://i.postimg.cc/cCynDPSg/44.png" width="500px" title="source: imgur.com" /></a></li>
  <li>Simple Example
```
Consider the sentence : â€œLetâ€™s do a simple exampleâ€
Given:â€letâ€™s do a simpleâ€
Predict the new word? Easy</li>
</ul>

<p>Given: â€œsimpleâ€
Predict the next word? not as easy</p>

<p>Given: â€œaâ€
predict the next word? very difficult</p>

<p>is the Markov Property limiting? Not necessarily
```</p>

<h2 id="3-markov-property-in-rl">3. Markov Property in RL</h2>
<ul>
  <li>{S(t), A(t)} produces 2 things -&gt; {S(t+1),R(t+1)}</li>
  <li>Markov Property:
<a href="https://postimg.cc/bDgrrCdp"><img src="https://i.postimg.cc/cCynDPSg/44.png" width="500px" title="source: imgur.com" /></a></li>
  <li>Convenience notation
<a href="https://postimg.cc/LJrhqgTQ"><img src="https://i.postimg.cc/g00hTv6p/44.png" width="500px" title="source: imgur.com" /></a></li>
  <li>joint on sâ€™ and r, conditioned on 2 other variables</li>
  <li>different from â€œusualâ€ Markov: 1 RV(Random Variable) Conditioned on 1 other RV</li>
</ul>

<h2 id="4-other-conditional-distributions">4. Other Conditional distributions</h2>
<ul>
  <li>can be found using rules of probability</li>
  <li>For pretty much all cases weâ€™ll consider these will be deterministic</li>
  <li>i.e. states will always give us the same reward</li>
  <li>Action will always bring us to same next state</li>
  <li>But, these distributions are part of the core theory of RL
<a href="https://postimg.cc/BL5dTRGP"><img src="https://i.postimg.cc/W3xTvPW8/44.png" width="500px" title="source: imgur.com" /></a></li>
</ul>

<h2 id="5-is-the-markov-assumption-limiting">5. Is the Markov Assumption limiting?</h2>
<ul>
  <li>Not necessarily</li>
  <li>Recent application: DeepMind used concatenation(ä¸€ç³»åˆ—ç›¸å…³è”çš„äº‹ç‰©) of 4 most recent frames to represent state when playing Atari Games</li>
  <li>State can be made up of anything from anytime past to current</li>
  <li>Typically think of state right now = something we measure right now</li>
  <li>Also, donâ€™t need to use raw data(state can be features transformed from raw data)</li>
  <li>Any input from agentâ€™s sensors can be used to form state</li>
</ul>

<h2 id="6-markov-decision-processesmdps">6. Markov Decision Processes(MDPs)</h2>
<ul>
  <li>Any RL task with a set of States, actions, and rewards, that follows the Markov Property, is a MDP</li>
  <li>MDP is defined as the collection of
    <ul>
      <li>set of states</li>
      <li>set of actions</li>
      <li>set of rewards</li>
      <li>State-Transition probability, Reward probability(as defined jointly(è¿å¸¦åœ°) earlier)</li>
      <li>Discount factor</li>
    </ul>
  </li>
  <li>Often written as a 5 tuples</li>
</ul>

<h2 id="7-policy">7. Policy</h2>
<ul>
  <li>One more piece to complete the puzzle - the policy(denoted by Ï€)</li>
  <li>Technically Ï€ is not part of the MDP itself, but it, along with the value function, form the solution</li>
  <li>Left out until now because itâ€™s a weird symbol</li>
  <li>thereâ€™s no â€œequationâ€ for it</li>
  <li>how do we write epsilon-greedy as an equation? itâ€™s more like an algorithm</li>
  <li>the only exception is the <em>Optimal policy</em>, which can be defined in terms of the value function</li>
  <li>think of Ï€ as shorthand for the algorithm the agent is using to navigate the environment
    <h2 id="8-state-transition-probability---pssa">8. State-Transition probability - p(sâ€™|s,a)</h2>
    <p><a href="https://postimg.cc/BL5dTRGP"><img src="https://i.postimg.cc/W3xTvPW8/44.png" width="500px" title="source: imgur.com" /></a></p>
  </li>
  <li>State Diagram</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>p(sâ€™</td>
          <td>s,a)</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>why is this stochastic? if i press â€œjumpâ€ button, doesnâ€™t it always do the same thing?</li>
  <li>Recall: state is only derived from what agent senses, itâ€™s not the environment itself</li>
  <li>State can be imperfect representation of environment</li>
  <li>Ex) state could represent multiple configuration of environment</li>
  <li>Ex) Blackjack - if weâ€™re the agent, the dealerâ€™s next card is not part of our state(but it is part of the environment)</li>
</ul>

<h2 id="9-actions-vs-environment">9. Actions vs Environment</h2>
<ul>
  <li>Typically we think of action like joystick inputs(up/down/left/right/jump) or Blackjack Moves(hit/stand)</li>
  <li>Actions can be very board: how to distribute government funding
<a href="https://postimg.cc/yD5c441M"><img src="https://i.postimg.cc/VNsRtm3k/42.png" width="500px" title="source: imgur.com" /></a></li>
  <li>we are navigating an environment, we are the agent - what constitutes(êµ¬ì„±í•˜ë‹¤) â€œusâ€?</li>
  <li>Are our body? No</li>
  <li>our body is part of the environment, our body doesnâ€™t make decisions/learn</li>
  <li>brain/mind does the learning</li>
</ul>

<h2 id="10-total-reward--future-reward">10. Total Reward &amp; Future Reward</h2>
<ul>
  <li>We are Interested in measuring total <em>future</em> reward</li>
  <li>Everything from t+1 onward(ç»§ç»­çš„)</li>
  <li>We call this the <em>Return</em>, G(t)</li>
  <li>Note: does not count current reward R(t)</li>
</ul>

<h2 id="ë²ˆì™¸">ë²ˆì™¸</h2>
<ul>
  <li>(Epsilon GreedyëŠ” ì£¼ì‚¬ìœ„ í•œë²ˆ ë˜ì ¸ì„œ ê²°ì •í•˜ê³  ê·¸ action ë‹¤ìŒë²ˆì— ì‚¬ìš©í•˜ëŠ” ê²ƒì´ on Policy)</li>
  <li>(Q-Learningì€ ë‹¤ìŒ Stepì—ì„œ ì‹¤ì œë¡œ ì‚¬ìš©í•  actionê³¼ ìƒê´€ ì—†ì´ Max Q ì·¨í•˜ê¸° ë•Œë¬¸ì— off policy)
    <ul>
      <li>ê° ì§€ì ì—ì„œ ê³„ì† ìµœì ê°’ì„ ì°¾ìœ¼ë©´ì„œ Future Reward è®¡ç®—</li>
      <li>ë²¨ë©˜ ë°©ì •ì‹ì„ ì´ìš©í•˜ì—¬ ë°˜ë³µì ìœ¼ë¡œ Qí•¨ìˆ˜ë¥¼ ê·¼ì‚¬ì‹œí‚¬ ìˆ˜ ìˆìŒ</li>
    </ul>
  </li>
  <li>(Sarsa, ë‹¤ìŒë²ˆì— ê²Œì„ì— ë„£ì–´ì¤„ actionì„ ë¯¸ë¦¬ ê³„ì‚°í•´ì„œ ì‚¬ìš©)</li>
  <li>Reinforcement Learning Two Problem
    <ul>
      <li>Credit Assignment Problem</li>
      <li>Exploration-exploitation</li>
    </ul>
  </li>
  <li>DQN Property
    <ul>
      <li>Target Q function
        <ul>
          <li>í•™ìŠµ ëŒ€ìƒì´ ë˜ëŠ” Qí•¨ìˆ˜ê°€ í•™ìŠµì´ ë˜ë©´ì„œ ê³„ì† ë°”ë€ŒëŠ” ë¬¸ì œ</li>
          <li>Learning Q-Fucntion from Gradient Descent</li>
          <li>ì¼ì • ìŠ¤í… ë  ë•Œë§ˆë‹¤ Q í•¨ìˆ˜ì˜ ê°€ì¤‘ì¹˜ë¥¼ íƒ€ì¼“ Qí•¨ìˆ˜ì— ì—…ë°ì´íŠ¸</li>
        </ul>
      </li>
      <li>Replay Memory
        <ul>
          <li>í•™ìŠµì˜ ì¬ë£Œê°€ ë˜ëŠ” Sample ì €ì¥ì†Œ</li>
          <li>ì¦‰ì‹œ í›ˆë ¨í•˜ì§€ ì•Šê³ , ë©”ë¡œë¦¬ ì €ì¥</li>
          <li>ì¼ì • ìˆ˜ì˜ Samplrì„ ëœë¤ìœ¼ë¡œ êº¼ë‚´ í•™ìŠµ</li>
        </ul>
      </li>
      <li>Q-function made by ANN
        <ul>
          <li>DQNì€ ì¸ê³µì‹ ê²½ë§ìœ¼ë¡œ Policy functionì„ Approximate</li>
        </ul>
      </li>
      <li>Q function of DQN íŠ¹ì§•
        <ol>
          <li>Model Free :
            <ul>
              <li>ëª¨ë¸ì´ ì—†ê³  ìƒ˜í”Œë¡œ ë¶€í„° ì§ì ‘ì ìœ¼ë¡œ ì •ì±…ì„ ê·¼ì‚¬í™”</li>
              <li>ëŒ€ë¶€ë¶„ ANNì„ í™œìš©í•œ í›ˆë ¨ìœ¼ë¡œ Dimension Curseì— ë²—ì–´ë‚œë‹¤.</li>
            </ul>
          </li>
          <li>Off-Policy
            <ul>
              <li>Learn thing from another agentâ€™s action</li>
              <li>íƒ€ê²Ÿ ì •ì±…ê³¼ í–‰ë™ ì •ì±… ë‚˜ëˆˆë‹¤.</li>
            </ul>
            <ul>
              <li>Target policy : ìš°ë¦¬ê°€ ê°•í™”í•™ìŠµ ì—ì´ì „íŠ¸ì—ê²Œ ê°€ë¥´ì¹˜ê¸° ìœ„í•œ ê¸°ì¤€ì´ ë˜ëŠ” ì •ì±…</li>
              <li>í–‰ë™ Policy : íƒí—˜ì„ í•˜ë©° ìƒˆë¡œìš´ í–‰ë™ì„ ë§Œë“¤ì–´ ë‚´ëŠ” ì •ì±…
      - ë‘ê°€ì§€ í´ë¦¬ì‹œë¥¼ ë‹¤ë£¨ì–´ì•¼ í•¨ìœ¼ë¡œ êµ¬í˜„í•˜ê¸° ì–´ë ¤ì›€</li>
            </ul>
          </li>
          <li>MiniBatch
            <ul>
              <li>ë§ì€ ë°ì´í„° ì¤‘ ì„ì˜ë¡œ ìƒ˜í”Œì„ ë½‘ì•„ í•™ìŠµì‹œí‚¤ëŠ” ê²ƒ</li>
              <li>ì—°ì†ì ì¸ ìƒ˜í”Œë“¤ ê°„ì˜ ê°•í•œ ìƒê´€ê´€ê³„ë¥¼ ì œê±°</li>
            </ul>
          </li>
          <li>Value-Based Reinforcement Learning
            <ul>
              <li>at first, Let Value Function Approximating to make a Policy</li>
            </ul>
          </li>
          <li>Decaying Epsilon-Greedy
            <ul>
              <li>at first, Random Act -&gt; random act to be reduced gradually -&gt; when it became 1% of random action, it stop</li>
              <li>Two Hyper parameter:
 (1) Exploration_fraction : ì–¸ì œê¹Œì§€ ê°ì†Œ ì‹œí‚¬ ê²ƒì¸ê°€? Default 0.5(Timestepsì˜ 50%ê°€ ë  ë•Œê¹Œì§€ ëœë¤ ì•¡ì…˜ ì·¨í•  í™•ë¥  ì´ ì¤„ì–´ë“ ë‹¤)
 (2) Exploration_final_eps : ìµœì¢… ì…ì‹¤ë¡  ê°’, ê¸°ë³¸ê°’ 0.01(0.01ì´ ë˜ë©´ ê°ì†Œê°€ ì¤„ì–´ë“¤ê³ , ê°’ì„ ìœ ì§€í•œë‹¤.)</li>
            </ul>
          </li>
        </ol>
      </li>
    </ul>
  </li>
</ul>
:ET