I"<h1 id="tdÎ»">TD(Î»)</h1>
<ul>
  <li>Generalize N-step method</li>
  <li>Î» is associated with something called the []â€eligibility traceâ€(ì „ê²© í”ì )](https://sumniya.tistory.com/14)
    <ul>
      <li>eligibility traceë¼ëŠ” ê°œë… : ì´ëŠ” ê³¼ê±°ì— ë°©ë¬¸í–ˆë˜ state ì¤‘ì—ì„œ í˜„ì¬ ì–»ê²Œ ë˜ëŠ” rewardì— ì˜í–¥ì„ ì£¼ëŠ” stateë¥¼ íŒë‹¨í•˜ì—¬, í˜„ì¬ ì–»ê²Œ ë˜ëŠ” rewardì„ í•´ë‹¹ stateì— ë‚˜ëˆ„ì–´ì£¼ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ë•Œ, ì˜í–¥ì„ ì£¼ì—ˆë‹¤ê³  íŒë‹¨í•˜ëŠ” indexë¥¼ creditì´ë¼ê³ í•˜ê³  ì´ creditì„ assigní•  ë•Œ, ë‘ ê°€ì§€ ê¸°ì¤€ì„ ì”ë‹ˆë‹¤. 1) Frequency heuristic(ì–¼ë§ˆë‚˜ ìì£¼ ë°©ë¬¸í–ˆëŠ”ê°€?) 2) Recency heuristic(ì–¼ë§ˆë‚˜ ìµœê·¼ì— ë°©ë¬¸ í–ˆëŠ”ê°€?)</li>
    </ul>
  </li>
  <li>N-step method code is complicated - not trivial to keep track of all rewards(then flush them once episode is over)</li>
  <li>TD(Î») allows us a more elegant method to trade-off between TD(0) and MC</li>
  <li>can update after just 1 step</li>
  <li>Î» = 0 gives us TD(0), Î»=1 gives us Monte Carlo
    <h2 id="n-step-method">N-step method</h2>
  </li>
  <li>Recall:</li>
</ul>

<p><a href="https://postimg.cc/kB92MvCf"><img src="https://i.postimg.cc/7YJ0VKW8/Capture.png" width="500px" title="source: imgur.com" /><a></a></a></p>
<ul>
  <li>strange idea: combine these :</li>
</ul>

<p><a href="https://postimg.cc/zVtqkv0D"><img src="https://i.postimg.cc/htghc7mm/Capture.png" width="300px" title="source: imgur.com" /><a></a></a></p>
<h2 id="tdÎ»-1">TD(Î»)</h2>
<ul>
  <li>letâ€™s take this strangeness to the next level - letâ€™s combine more Gâ€™s, even an infinite number of Gâ€™s</li>
</ul>

<p><a href="https://postimg.cc/ZvnMYb3j"><img src="https://i.postimg.cc/MHm2DXGk/Capture.png" width="500px" title="source: imgur.com" /><a></a></a></p>
<ul>
  <li>Î» must sum to 1 so that the result is on the same scale as individual Gâ€™s</li>
  <li>in TD(Î»). we make the coefficients decrease geometrically</li>
</ul>

<p><a href="https://postimg.cc/p5jKJ3tR"><img src="https://i.postimg.cc/SRTVkpT2/Capture.png" width="500px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li><strong>Problem: these donâ€™t sum to 1</strong></li>
  <li>we are most interested in the case when n - &gt; âˆ</li>
  <li>we know this from calculus:</li>
</ul>

<p><a href="https://postimg.cc/34T4m1ZD"><img src="https://i.postimg.cc/fL3c4gt5/Capture.png" width="200px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li>therefore, we should scale the sum by thins amount</li>
  <li>we call this the <em>**</em></li>
</ul>

<p><a href="https://postimg.cc/fJtzWXc4"><img src="https://i.postimg.cc/Dy58gPmz/Capture.png" width="500px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li>we assume our episode will end at some point(say, time step T)</li>
  <li>when we reach step T - t, the episode is over, so any N-step return beyond this is the Full MC return, G(t)</li>
  <li>separate the partial N-step returns and the full returns</li>
</ul>

<p><a href="https://postimg.cc/sM1sHW2H"><img src="https://i.postimg.cc/nhGrzYfx/Capture.png" width="500px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li>manipulate 2nd term to simplify the sum</li>
</ul>

<p><a href="https://postimg.cc/fSk8QM0s"><img src="https://i.postimg.cc/prJ48TWm/Capture.png" width="500px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li>simplify further</li>
</ul>

<p><a href="https://postimg.cc/TKQQ1yqx"><img src="https://i.postimg.cc/q7HFmyNN/Capture.png" width="500px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li>when Î»=0, we get the TD(0) return since $0^0$ = 1</li>
  <li>when Î»=1, we get the full return(MC)</li>
  <li>any Î» between 0 and 1 gives us a combination of individual returns(with geometrically decreasing weight)</li>
</ul>

<p><a href="https://postimg.cc/VJspKPKY"><img src="https://i.postimg.cc/QdcDxX5W/Capture.png" width="500px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li>isnâ€™t this much more computational effort than both MC and N-step methods?</li>
  <li>yes, we would need to calculate G(t), $G^1$(t),â€¦,$G^t$(t)</li>
  <li>lots of work (benefit is not clear)</li>
  <li>the algorithm weâ€™re about to learn is an approximation to calculate the true Î»-return(since that would be computationally infeasible)</li>
</ul>

<h2 id="td0">TD(0)</h2>
<ul>
  <li>letâ€™s go back to TD(0) for a moment</li>
  <li>we call target - prediction the TD error</li>
</ul>

<p><a href="https://postimg.cc/7CKKbC47"><img src="https://i.postimg.cc/B60kYDwh/Capture.png" width="500px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li>parameter update is still gradient descent</li>
</ul>

<p><a href="https://postimg.cc/dZPr0vhr"><img src="https://i.postimg.cc/XqX8QY3H/Capture.png" width="200px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li>Eligibility trace is a vector the same size as parameter vector:</li>
</ul>

<p><a href="https://postimg.cc/dZPr0vhr"><img src="https://i.postimg.cc/XqX8QY3H/Capture.png" width="200px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li><strong>Eligibility Trace/ vector keeps track of old gradients</strong>, much like momentum from deep learning</li>
  <li>Î» tell us <strong>â€œhow muchâ€ of the past we want to keep</strong>
    <ul>
      <li>$e_0$ = 0, $e_t$ = $âˆ‡_0$V($s_t$)</li>
    </ul>
  </li>
</ul>

<p>```
Reference:</p>

<p><a href="https://www.udemy.com/course/artificial-intelligence-reinforcement-learning-in-python/">Artificial Intelligence Reinforcement Learning</a></p>

<p><a href="https://www.udemy.com/deep-reinforcement-learning-in-python/">Advance AI : Deep-Reinforcement Learning</a></p>

<p><a href="https://www.udemy.com/cutting-edge-artificial-intelligence/learn/lecture/14650508#overview">Cutting-Edge Deep-Reinforcement Learning</a></p>
:ET