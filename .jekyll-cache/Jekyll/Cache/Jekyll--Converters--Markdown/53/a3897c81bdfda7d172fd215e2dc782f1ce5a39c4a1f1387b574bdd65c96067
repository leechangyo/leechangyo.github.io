I"E<h1 id="optimization-in-signal-and-image-processing">Optimization in Signal and Image processing</h1>

<h2 id="model-arising-in-compressive-sensing-and-imaging-sciences">Model arising in compressive sensing and imaging sciences</h2>

<ul>
  <li>Separable(å¯åˆ†å¼€çš„) Structures of problems</li>
</ul>

<p><a href="https://postimg.cc/zL2TZB2S"><img src="https://i.postimg.cc/vBbhnxWj/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li>Convexity and easy subproblems(å­é—®é¢˜).</li>
  <li>Often involving large, nonsmooth convex functions with separable structure.</li>
  <li><a href="https://math.stackexchange.com/questions/2201384/what-is-the-definition-of-a-first-order-method">First-order method</a> can be very slow for producing high accuracy solutions, but also share many advantages:
    <ul>
      <li>Non-differentiability</li>
      <li>Use minimal information, e.g.,(f;âˆ‡f)</li>
      <li>Often lead to very simple and â€œcheapâ€ iterative schemes</li>
      <li><strong>Complexity/iteration mildly dependent (e.g., linear) in problemâ€™s dimension</strong></li>
      <li>Suitable when high accuracy is not crucial [in many large scale applications, the data is anyway corrupted or known only roughly</li>
    </ul>
  </li>
</ul>

<h2 id="example-compressive-sensingkernel-ê°™ì€-ê°œë…">Example: compressive sensing(Kernel ê°™ì€ ê°œë…)</h2>
<ul>
  <li>Goal: Recover sparse signal from very few linear measurements.
<a href="https://postimg.cc/gX55JGj3"><img src="https://i.postimg.cc/WzbP53PY/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></li>
  <li>Basis pursuit model:
<a href="https://postimg.cc/gLFDQjrt"><img src="https://i.postimg.cc/3NWcyDhh/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></li>
</ul>

<h2 id="compressive-sensing">Compressive Sensing</h2>
<ul>
  <li>Find the sparest solution
    <ul>
      <li>Given n=256, m=128.
        <ul>
          <li>n is original signal</li>
          <li>m is output value</li>
        </ul>
      </li>
      <li>A = randn(m,n); u = sprandn(n, 1, 0.1); b = A*u;
<a href="https://postimg.cc/pmT6D9jK"><img src="https://i.postimg.cc/3xCQQmJf/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></li>
    </ul>
  </li>
</ul>

<h2 id="sparseí”íˆ-ë„“ì€-ì§€ì—­ì—-ë¶„í¬ëœ-ì •ë„ê°€-ë“œë¬¸-ë°€ë„ê°€-í¬ë°•í•œ-recovery-models">Sparse((í”íˆ ë„“ì€ ì§€ì—­ì— ë¶„í¬ëœ ì •ë„ê°€) ë“œë¬¸, (ë°€ë„ê°€) í¬ë°•í•œ) recovery models</h2>
<p><a href="https://postimg.cc/YhjPFGYY"><img src="https://i.postimg.cc/wTkYSDBf/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>
<ul>
  <li>Basis pursuit model : nonunique solution</li>
  <li>Ax = b = {$x^*$+N(A)} =&gt; âˆ€ v âˆˆ N(A), Av = 0
    <ul>
      <li>N(A) is non-space</li>
    </ul>
  </li>
  <li>Î¼ is given parameter(constant variable)</li>
</ul>

<h2 id="sparsityí”íˆ-ë„“ì€-ì§€ì—­ì—-ë¶„í¬ëœ-ì •ë„ê°€-ë“œë¬¸-ë°€ë„ê°€-í¬ë°•í•œ-under-a-basis">Sparsity((í”íˆ ë„“ì€ ì§€ì—­ì— ë¶„í¬ëœ ì •ë„ê°€) ë“œë¬¸, (ë°€ë„ê°€) í¬ë°•í•œ) under a basis</h2>
<ul>
  <li>Given a reprenting basis or dictionary, we want to recover a signal which is sparse under this basis.</li>
  <li>Two types of models:</li>
</ul>

<p><a href="https://postimg.cc/1gGk58N6"><img src="https://i.postimg.cc/HLRdz5YS/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li>Commonly used dictionaries include both analytic and trained ones.
    <ul>
      <li>analytic bases: Id, DCT, wavelets, curvelets, gabor, etc., also their combinations; they have analytic properties, often easy to compute (for example, multiplying a vector takes O(n log n) instead of O($n^2$))</li>
      <li>data driven: can also be numerically learned from training data or partial signal, patches (offline or online learning).</li>
      <li>they can be orthogonal, frame, normalized or general.</li>
      <li><strong>If Î¦ is orthogonal (or just invertible), the analyse based model is equivalent to synthesis based one by changing the variable. When it is not, the model is harder to solve.</strong></li>
    </ul>
  </li>
</ul>

<h2 id="parallel-mri-reconstruction">Parallel MRI reconstruction</h2>
<p><a href="https://postimg.cc/GHvC73c4"><img src="https://i.postimg.cc/VLVkN5pW/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>

<h2 id="nonconvex-sparsity-models">Nonconvex sparsity models</h2>
<ul>
  <li>min f (x) + h(x)</li>
  <li>where either f or h is nonconvex or both are nonconvex. Some popular models:
    <ul>
      <li>Nonlinear least square for nonlinear inverse problems f (x) = â€–F(x) âˆ’ yâ€–^2.</li>
      <li>h(x) = â€–xâ€–_p, where 0 &lt;= p &lt; 1 or rank constraint.</li>
      <li>Alternating minimization methods in many applications, such as blind deconvolution, matrix factorization, or dictionary learning etc.</li>
    </ul>
  </li>
</ul>

<h1 id="optimization-of-matrices">Optimization of Matrices</h1>

<h2 id="the-netflix-problem">The Netflix problem</h2>
<p><a href="https://postimg.cc/cKDmg7My"><img src="https://i.postimg.cc/TwxHsCKw/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>

<h2 id="matrix-rank-minimization">Matrix Rank Minimization</h2>
<p><a href="https://postimg.cc/tsdXGjLJ"><img src="https://i.postimg.cc/K8w4LcfP/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>

<h2 id="video-separation">Video separation</h2>
<p><a href="https://postimg.cc/k2f6xKf7"><img src="https://i.postimg.cc/N0w6vxX2/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>

<h2 id="sparse-and-low-rank-matrix-separation">Sparse and low-rank matrix separation</h2>
<ul>
  <li>Given a matrix M, we want to find a low rank matrix W and a sparse matrix E, so that W + E = M.</li>
  <li>Convex approximation:</li>
</ul>

<p><a href="https://postimg.cc/xXmkZ8Gz"><img src="https://i.postimg.cc/y8QFkSkv/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li>Robust PCA (reduce dimension)</li>
</ul>

<h2 id="extension">Extension</h2>
<p><a href="https://postimg.cc/CZ644zwf"><img src="https://i.postimg.cc/QtjnhTPS/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>

<h2 id="correlation-matrices">Correlation Matrices</h2>
<ul>
  <li>A correlation(ç›¸äº’å…³ç³») matrix satisfies:
    <ul>
      <li>X = X^T , X_ii = 1, i = 1, . . . , n, X &gt;= 0.</li>
    </ul>
  </li>
  <li>Example: (low-rank) nearest correlation matrix estimation</li>
</ul>

<p><a href="https://postimg.cc/fVvZ1GLM"><img src="https://i.postimg.cc/t4KTnyGn/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>

<h1 id="optimization-in-machine-learning">Optimization in Machine learning</h1>

<h2 id="why-optimization-in-machine-learning">Why Optimization in Machine Learning?</h2>
<h1 id="reference">Reference</h1>

<p><a href="https://web.stanford.edu/class/ee364a/lectures.html"> Optimization method - Standford University </a></p>
:ET