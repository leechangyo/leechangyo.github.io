I"~<h1 id="appendix-elementary-results-and-notations">Appendix: Elementary results and notations</h1>

<h2 id="1derivatives">1.Derivatives</h2>
<p><a href="https://postimg.cc/2V31NpXd"><img src="https://i.postimg.cc/J45ZJMNF/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>
<ul>
  <li><strong>A gradient represents the slope of the tangent of the graph of the function. It gives the linear approximation of f at a point. It points toward the greatest rate of increase.</strong></li>
</ul>

<h2 id="2-hessian">2. Hessian</h2>

<ul>
  <li>Let f be twice differentiable.</li>
</ul>

<p><a href="https://postimg.cc/5jVrrbyG"><img src="https://i.postimg.cc/NG9w50cg/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li>A Hessian gives a quadratic approximation of f at a point.</li>
  <li>Gradient and Hessian are local properties that help us recognize local solutions and determine a direction to move at toward the next point.</li>
</ul>

<h2 id="3-taylors-series-expansion">3. Taylor’s series Expansion</h2>
<p><a href="https://postimg.cc/N9JrRQzM"><img src="https://i.postimg.cc/gjdqFJyv/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></p>
<ul>
  <li>α is learning rate</li>
</ul>

<h1 id="reference">Reference</h1>

<p><a href="https://web.stanford.edu/class/ee364a/lectures.html"> Optimization method - Standford University </a></p>
:ET