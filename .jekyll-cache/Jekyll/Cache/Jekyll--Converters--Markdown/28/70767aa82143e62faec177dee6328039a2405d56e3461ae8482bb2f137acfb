I"¥<h1 id="review">Review</h1>
<ul>
  <li>Multi-armed bandit Review(Bayesian machine learning: A/B Testing)</li>
  <li>Explore-exploit dilemma</li>
  <li>4 Algorithms:
    <ul>
      <li>Epsilon-greedy</li>
      <li>Optimistic initial Value</li>
      <li>UCB1</li>
      <li>Thompson Sampling</li>
    </ul>
  </li>
  <li>Basic definitions in RL</li>
  <li>Tic-Tac-Toe</li>
  <li>MDPs</li>
  <li>Policies state-value functions, action-value funtions</li>
  <li>Return</li>
  <li>3 methods:
    <ul>
      <li>Dynamic Programming(direct application of bellman‚Äôs Equation)
        <ul>
          <li>Policy iteration, Value iteration</li>
        </ul>
      </li>
      <li>Monte Carlo
        <ul>
          <li>Learning from experience</li>
          <li>Not fully online</li>
        </ul>
      </li>
      <li>Temporal Difference Learning
        <ul>
          <li>Fully online with bootstrapping</li>
          <li>Also learn from experience</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Approximation Methods</li>
  <li>Tabular methods can be infeasible for large state spaces
    <ul>
      <li>action value funtionÏùÑ Q-tableÎ°ú ÏûëÏÑ±ÌïòÏó¨ Ìë∏Îäî Î∞©Î≤ï</li>
    </ul>
  </li>
  <li>differential models(feature engineering)
    <h2 id="1-review-of-mdps">1. Review of MDPs</h2>
  </li>
  <li>Markov decision Processes</li>
  <li>MDPs a collection of 5 things:
    <ul>
      <li>set of all states</li>
      <li>set of all actions</li>
      <li>set of all rewards</li>
      <li>state transition probabilities</li>
      <li>Discount factor(gamma)</li>
    </ul>
  </li>
  <li>States
    <ul>
      <li>State represents what the sensors of our agent measure from the environment</li>
      <li>In GridWorld, that would be our position on the board</li>
      <li>in tic-tac-toe: specific configuration of pieces on the board</li>
      <li>Video game: pixel on the screen</li>
      <li>Maybe also : # of lives we have left, health, etc</li>
      <li>For an AI to be as human as possible maybe we should require it to learn that only from pixels on the screen</li>
    </ul>
  </li>
  <li>Actions
    <ul>
      <li>Anything the agent can do while in a state</li>
      <li>tic-tac-toe: placing a piece on the board</li>
      <li>Video game: moving up/down/left/right, pressing an action button</li>
    </ul>
  </li>
  <li>Rewards
    <ul>
      <li>Agent receives a reward at every time step</li>
      <li>reward are real-valued</li>
      <li>goal of agent is to maximize total future reward</li>
      <li>careful to define rewards the right way</li>
      <li>Ex. Robot trying to solve a maze receives a reward of 0 at every step and 1 for solving the maze</li>
      <li>possible that robot will never solve the maze, or solve it very inefficiently</li>
      <li>it has only experience 0 reward, thinks that is the best it can do, no incentive to not act randomly</li>
      <li>better solution is -1 reward at every time step</li>
      <li>now it has incentive to solve the maze as quickly as possible</li>
      <li>‚ÄúNegative‚Äù and ‚ÄúPositive‚Äù don‚Äôt have connotations(Âê´‰πâ) when it comes to RL agents</li>
      <li>just a number on a scale</li>
      <li>E.g -3 is better reward than -300</li>
      <li>been debated whether or not we should override the default rewards</li>
      <li>in real world environment, we would be the ones defining rewards
        <h1 id="next-steps">Next Steps</h1>
      </li>
    </ul>
  </li>
  <li>Continuous state-spaces
    <ul>
      <li><a href="https://en.wikipedia.org/wiki/Light_intensity">light intensity</a> in 3-D space</li>
      <li>3-D space is Continuous, light intensity is Continuous</li>
    </ul>
  </li>
  <li>Continuous action-space
    <ul>
      <li><strong>Amount of force applied to a motor</strong></li>
    </ul>
  </li>
  <li>parameterized V</li>
  <li>parameterized œÄ
    <ul>
      <li>it called <strong>‚Äúpolicy gradient‚Äù</strong> method</li>
    </ul>
  </li>
  <li>learned from our current experience only</li>
  <li>as we play, we accumulate(input, target) pairs
    <ul>
      <li>E.g accumulate training data</li>
    </ul>
  </li>
  <li>save these in a file and do more training</li>
  <li>in deep learning, we loop through the same training data multiple times (called ‚Äòepochs‚Äô)</li>
  <li>No reason we can‚Äôt learn from our previous experience as well</li>
  <li>Replay a previous episode, update params based on old episodes</li>
  <li>CNNs</li>
  <li>RNNs</li>
  <li>Regression, Classification</li>
</ul>
:ET