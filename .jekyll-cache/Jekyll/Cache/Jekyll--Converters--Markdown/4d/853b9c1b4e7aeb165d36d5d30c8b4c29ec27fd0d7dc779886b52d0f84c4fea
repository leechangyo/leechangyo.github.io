I"8Z<h1 id="value-function--the-bellman-equation">Value Function &amp; The Bellman Equation</h1>
<p><a href="https://postimg.cc/ZCfNJ0tQ"><img src="https://i.postimg.cc/jST4QJ8R/22.png" width="500px" title="source: imgur.com" /></a></p>
<ul>
  <li>The full derivation(起源) can be tough if our probability skills are not yet strong enough</li>
</ul>

<h2 id="1-expected-values">1. Expected Values</h2>
<ul>
  <li>why is this strange to many people?</li>
  <li>Consider a coin toss: Heads = Win, Tails = Lose</li>
  <li>Numerically: H = 1, T = 0</li>
  <li>suppose, P(W) = 60%</li>
  <li><strong>Expected Value is 0.6 x 1 + 0.4 * 0 = 0.6</strong></li>
  <li>The “Expected Value” is a “Value” I can “never expect”
<a href="https://postimg.cc/Mvs9gWs6"><img src="https://i.postimg.cc/NjtZ9FmH/22.png" width="500px" title="source: imgur.com" /></a></li>
  <li>What’s the point of expected values?</li>
  <li>it tells us the <strong>mean/average</strong> (E.g. we gather up all the heights of students in the call and calculate the mean- no student may have the mean height, but it’s a useful statistic)</li>
  <li>it doesn’t matter if a coin flip will never give me 0.6, it just an average
<a href="https://postimg.cc/8fQ2J18Q"><img src="https://i.postimg.cc/BbX4z6M6/2222.png" width="500px" title="source: imgur.com" /></a></li>
  <li>x is state()</li>
</ul>

<h2 id="2-probability-trees">2. probability Trees</h2>
<ul>
  <li>our expected reward is the weighted sum of each possible outcome(weighted by the probability at the corresponding branch)
<a href="https://postimg.cc/kDYjpJYn"><img src="https://i.postimg.cc/zX8933nR/12.png" width="500px" title="source: imgur.com" /></a></li>
  <li>the same concept extends to any number of possible outcomes</li>
  <li>in general: Expected value = p(e1) x value(e1) + p(e2) x value(e2) + ….
<a href="https://postimg.cc/qt9JwHfQ"><img src="https://i.postimg.cc/xCdX096r/33212.png" width="500px" title="source: imgur.com" /></a></li>
</ul>

<h2 id="3-why-are-averages-important">3. Why are averages important?</h2>
<p><a href="https://postimg.cc/H8M6fKLp"><img src="https://i.postimg.cc/nrRNQfbQ/22.png" width="500px" title="source: imgur.com" /></a></p>
<ul>
  <li>A subsection(分部) of a tree is also a tree – recursion(递归)</li>
  <li>After Arriving in this state, what happens next can be considered Random</li>
  <li>Hence, we cannot say: “if i reach this state, i will get X reward”</li>
  <li>we can only say: “if i reach this state, i will get X reward on <strong>average</strong>”</li>
</ul>

<h2 id="4-a-fundamental-concept-of-the-value-function">4. A fundamental concept of The Value Function</h2>
<ul>
  <li>At each state s, i will get a reward R</li>
  <li>overall return G, is the sum of rewards i get</li>
  <li>we want to be able to answer:
    <ul>
      <li>“if i am in state, s what is the sum of reward i will get in the future, on average?”</li>
      <li>we say: V(s) = E(GIs)</li>
    </ul>
  </li>
  <li>“I” means “givens” - anything to the left is random/ right is not random</li>
  <li>note : this is called a “conditional expectation”</li>
  <li>Value Function is equal to Expected function of Total Return by state
<a href="https://postimg.cc/gwQzsXXf"><img src="https://i.postimg.cc/bJNZz1zN/2223123.png" width="500px" title="source: imgur.com" /></a></li>
  <li>Every Game we play is just a series of states and rewards</li>
  <li>Let’s pretend everything is deterministic for now, e.g. E(3) = 3</li>
  <li>The value of a state is just the sum of all future rewards(if they are deterministic)
    <ul>
      <li>V($s_1$) = $r_2$ + $r_3$ + $r_4$ + … + $r_N$</li>
      <li>V($s_2$) =         $r_3$ + $r_4$ + … + $r_N$</li>
      <li>Key: V($s_1$) = $r_2$ + V($s_2$)</li>
    </ul>
  </li>
  <li>Discounted Version
    <ul>
      <li>V($s_1$) = $r_2$ + ɣ$r_3$ + ɣ$r_4$ + … + $r_N$</li>
      <li>V($s_2$) =         ɣ$r_3$ + ɣ$r_4$ + … + ɣ$r_N$</li>
      <li>Key: V($s_1$) = $r_2$ + ɣV($s_2$)</li>
      <li>(still assuming everything is deterministic)</li>
    </ul>
  </li>
  <li>In more general terms
    <ul>
      <li>Let’s make s = current state, s’=next state</li>
      <li>Let’s make r = reward(reall means R(s,s’)- the reward i get from going from s to s’)</li>
      <li><strong>V(s) = E[r + ɣV(s’)]</strong></li>
      <li>this is the essence of the bellman equation
<script async="" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script></li>
    </ul>
  </li>
</ul>
<div align="center" style="margin: 1em 0;">
<ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-5150894678574694" data-ad-slot="9221331439" data-ad-format="auto" data-full-width-responsive="true"></ins>
     </div>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

<ul>
  <li>Expansion
    <ul>
      <li>V(s) = E[r + ɣE(r’+ɣV(s’’))]</li>
      <li>V(s) = E[r + ɣE(r’+ɣE(r’‘+…))]</li>
      <li>understanding this “Structure”</li>
    </ul>
  </li>
  <li>putting more details back in
    <ul>
      <li>s is “given” - we’ve already arrived here</li>
      <li><strong>V(s) = E[r + ɣV(s’) I s]</strong></li>
    </ul>
  </li>
  <li>Expansion again
    <ul>
      <li>V(s) = state-value function</li>
      <li>Q(s,a) = action-value function</li>
      <li><strong>Q(s,a) = E[GIs,a] = E[r + ɣV(s’) I s,a]</strong></li>
      <li>to understand how Q anv V are related, we need to look at policies(something that tells us what action a to do, given what state we are in)</li>
    </ul>
  </li>
  <li>we know earlier(informally) with tic-tac-Toe</li>
  <li>if any discrepancies(差异) between then and now, consider everything from here onward to be more correct</li>
  <li>Value Function is determined by a policy and has state “s” as parameter</li>
  <li><em>only</em> future rewards</li>
  <li>value of all terminal states is thus 0</li>
  <li>the state value of the terminal state in an episodic problem should always be zero, the value of a state is the expected sum of all future rewards when starting in that state and following a specific policy. for the terminal state, this is zero - there are no more rewards to be end
0
<a href="https://postimg.cc/v1MsYJgS"><img src="https://i.postimg.cc/xdNTRY0Y/33321.png" width="500px" title="source: imgur.com" /></a></li>
  <li>Recursiveness(递归性)
<a href="https://postimg.cc/f39k0nB7"><img src="https://i.postimg.cc/nLdmgnmW/31243.png" width="500px" title="source: imgur.com" /></a></li>
</ul>

<h2 id="5-some-algebra">5. Some Algebra</h2>
<ul>
  <li>since the expected value is over π, that means we can express it as a possibility distribute
    <ul>
      <li>π = π(a I s)</li>
    </ul>
  </li>
  <li>the expected values are linear operators, so we can find each term one at a time
<a href="https://postimg.cc/3yzyNJhF"><img src="https://i.postimg.cc/26SnHVXM/412333.png" width="500px" title="source: imgur.com" /></a>
    <ul>
      <li>policy with in return action value in respect to state</li>
      <li>reward and probabilty with return reward in respect to state and action</li>
    </ul>
  </li>
  <li>in terms of p(s’,r I s,a)
<a href="https://postimg.cc/676sZdW9"><img src="https://i.postimg.cc/R0KvjR2n/23131.png" width="500px" title="source: imgur.com" /></a></li>
  <li>we can do this for anything
    <ul>
      <li>it’s just a general expression of expected values, we can use it on anything
<a href="https://postimg.cc/SnH2N7cv"><img src="https://i.postimg.cc/FKHjpTzN/2313123.png" width="500px" title="source: imgur.com" /></a></li>
    </ul>
  </li>
</ul>

<h3 id="so-lets-do-it-for-all-of-vs">So let’s do it for all of V(s)</h3>
<p><a href="https://postimg.cc/Ny4cPQGk"><img src="https://i.postimg.cc/vBRZTmpk/412123123.png" width="500px" title="source: imgur.com" /></a>
<a href="https://postimg.cc/GTpGs3t7"><img src="https://i.postimg.cc/6qr0gyXX/12312313.png" width="500px" title="source: imgur.com" /></a>
<a href="https://postimg.cc/WFybXsSf"><img src="https://i.postimg.cc/KjGkczTF/12413123.png" width="500px" title="source: imgur.com" /></a>
<a href="https://postimg.cc/MchFD4Vm"><img src="https://i.postimg.cc/W33c2c4C/4123124123.png" width="500px" title="source: imgur.com" /></a></p>
<ul>
  <li>E(E(X)) = E(X)</li>
  <li>we can do this infinity and it won’t change the answer</li>
  <li>E(A+B) = E(A+E(B))</li>
  <li>Therefore if i have one expected value like that, i can insert another expected value in there</li>
  <li><strong>Law of total expectation: E(X)=E(E(X I Y))</strong></li>
  <li><strong>E(E(G(t+1) I ANY)) = E(G(t+1))</strong></li>
  <li>now we know that any condition or expectation could go in that spot</li>
  <li>pick <strong>ANY</strong> = S(t+1) = s’
<a href="https://postimg.cc/rDdH6MVL"><img src="https://i.postimg.cc/7ZXr2Zjb/412312312.png" width="500px" title="source: imgur.com" /></a></li>
  <li>if i want to know what to do next i just look at what is the reward i get by going there whay is this has to do with how to find an optimal policy.</li>
  <li>Key: No need to enumerate all possible future(which could be infinity long), in order to choose every action</li>
</ul>

<h2 id="6-bellman-equation">6. Bellman Equation</h2>
<ul>
  <li>Richard bellman</li>
  <li>Pioneered “dynamic programming”</li>
  <li>Bottom-up approach</li>
  <li>DP(dynamic programming) is also one of the solution we’ll study for MDPs
<a href="https://postimg.cc/2qG9h6Th"><img src="https://i.postimg.cc/pTMNN5FG/43123123.png" width="500px" title="source: imgur.com" /></a></li>
  <li>State-value Fucntion
<a href="https://postimg.cc/xcgkrVsg"><img src="https://i.postimg.cc/SxbcMyc0/4141412312.png" width="500px" title="source: imgur.com" /></a></li>
  <li>Action-value function
<a href="https://postimg.cc/zysH7S21"><img src="https://i.postimg.cc/wBv56wh3/41241241.png" width="500px" title="source: imgur.com" /></a></li>
  <li>Space Required is quadratic: ISI x IAI</li>
</ul>

<h2 id="7-bellman-equation-by-example">7. Bellman Equation by Example</h2>
<ul>
  <li>Simple models with just a handful of states, easy to solve by hand</li>
  <li>Big Picture perspective: All we want to do is “solve for V(S)”</li>
  <li><strong>‘Value Function’ represent how good is a state for an agent to be in</strong></li>
</ul>

<h3 id="example-1">Example 1</h3>
<p><a href="https://postimg.cc/FfzCbv66"><img src="https://i.postimg.cc/d0jMYtjJ/421312312.png" width="500px" title="source: imgur.com" /></a></p>
<ul>
  <li>probability of going to end, from start, is 1</li>
  <li>Reward for landing in end is 1</li>
  <li>Discount factor ɣ = 0.9</li>
  <li>Question: what problem are we solving again?</li>
  <li>if we said, “find V(s)”, we are right</li>
  <li>In particular, we want:
    <ul>
      <li>V(START)</li>
      <li>V(End)</li>
    </ul>
  </li>
  <li>Try it ourselves before moving on</li>
  <li><strong>Remember</strong> “Value” is sum of all Future rewards</li>
  <li>Value of Terminal state is always 0</li>
  <li>V(end) = 0</li>
  <li>V(start) = 1</li>
  <li>Note: ɣ only applies to future rewards</li>
  <li>V(start) = R + ɣ(End)</li>
</ul>

<h3 id="example-2">Example 2</h3>
<p><a href="https://postimg.cc/mtGH7Sqf"><img src="https://i.postimg.cc/Hsr9DZ9x/414124124.png" width="500px" title="source: imgur.com" /></a></p>
<ul>
  <li>Everything is still deterministic</li>
  <li>Discount factor ɣ = 0.9</li>
  <li>V(End)=0 (terminal state)</li>
  <li>V(Mid)=1 (Takes the role of V(Start) From Example 1)</li>
  <li>V(Start) = R(Start,Mid) + ɣV(Mid) = 0 + 0.9 x 1 = 0.9</li>
</ul>

<h3 id="example-3">Example 3</h3>
<p><a href="https://postimg.cc/sBqFvmcb"><img src="https://i.postimg.cc/rmFqZYty/421412312.png" width="500px" title="source: imgur.com" /></a></p>
<ul>
  <li>V(End) = 0 (unaffected)</li>
  <li>V(Mid) = 1 (Also unaffected, only calculated from future rewards)</li>
  <li>V(Star) = R(Start,Mid) + ɣV(Mid) = -0.1 + 0.9*1 = 0.8</li>
</ul>

<h3 id="example-4">Example 4</h3>
<p><a href="https://postimg.cc/yWXZXbZh"><img src="https://i.postimg.cc/wxnQHKxr/55123.pngg" width="500px" title="source: imgur.com" /></a></p>
<ul>
  <li>Start at S1</li>
  <li>Discount factor ɣ = 0.9</li>
  <li>Nuance: What do these Probabilities refer to?</li>
  <li>we model games as MDPs - not just coin flips</li>
  <li>as an intelligent agent, our policy tells me what action to do : π(a I s).</li>
  <li>Important : Does not tell me where we go</li>
  <li>p(s’,r I s,a) tells me where i end up</li>
  <li>this example oversimplifies MDPs, but is easier to verbalize(用言语)</li>
  <li>For these examples, we’ll consider the action to be “deciding where to go”</li>
  <li>V(S4) = 0</li>
  <li>V(S2) = 1</li>
  <li>V(S3) = 1 (Same logic as previous examples)</li>
  <li>V(S1) = p(S2 I S1)[R2+ɣV(S2)]+p(S3 I S1)[R3+ɣV(S3)] = 0.5(-0.2 + 0.9<em>1)+0.5(-0.3 + 0.9</em>1) = 0.65</li>
</ul>

<h3 id="example-5">Example 5</h3>
<p><a href="https://postimg.cc/T5M2zf57"><img src="https://i.postimg.cc/HnjJ7LYg/41233213.png" width="500px" title="source: imgur.com" /></a></p>
<ul>
  <li>V(S4), V(S5) =0 (Terminal)</li>
  <li>V(S2) = p(S4 I S2)[R4 + ɣV(S4)] + p(S5 I S2)[R5 + ɣV(S5)] = 0.8<em>-1 + 0.2</em>1 = -0.6</li>
  <li>V(S3) = p(S4 I S3)[R4 + ɣV(S4)] + p(S5 I S3)[R5 + ɣV(S5)] = 0.1<em>-1 + 0.9</em>1 = 0.8</li>
  <li>V(S1) = p(S2 I S1)[R2 + ɣV(S2)] + p(S3 I S1)[R3 + ɣV(S3)] = 0.5<em>(0.9</em>-0.6) + 0.5(0.9*0.8)= 0.09</li>
</ul>

<h3 id="example-6">Example 6</h3>
<p><a href="https://postimg.cc/SJW0Tcrp"><img src="https://i.postimg.cc/L6WHsTgh/312313123.png" width="500px" title="source: imgur.com" /></a></p>
<ul>
  <li>sperate policy randomness from state-arrival randomness</li>
  <li>situation:
    <ul>
      <li>someone throws a ball at me</li>
      <li>our action: either “duck” or “jump”</li>
      <li>next possible states:
        <ul>
          <li>Get hit: R = -1</li>
          <li>Don’t Get hit(safe): R = 0</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>the action cannot be “don’t get hit”
    <ul>
      <li>if one could simply choose not to get hit one would never lose</li>
    </ul>
  </li>
  <li>can aplly to any “real-life” scenario, e.g. starting a company</li>
  <li>π(jump I start) = 0.5</li>
  <li>π(duck I start) = 0.5</li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>p(hit, reward = -1 I jump,start) = 0.8
p(hit, reward = 0 I jump,start) = 0
p(safe, reward = -1 I jump,start) = 0
p(safe, reward = 0 I jump,start) = 0.2
p(hit, reward = -1 I duck,start) = 0.2
p(hit, reward = 0 I duck,start) = 0.4
p(safe, reward = -1 I duck,start) = 0.6
p(safe, reward = 0 I duck,start) = 0.4
</code></pre></div></div>

<ul>
  <li>just marginalize over reward since they are really deterministic</li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>p(hit, reward = -1 I jump,start) = 0.8
p(safe, reward = 0 I jump,start) = 0.2

p(hit, reward = 0 I duck,start) = 0.4
p(safe, reward = -1 I duck,start) = 0.6
</code></pre></div></div>

<ul>
  <li>as usual, V(safe) and V(hit) = 0</li>
</ul>

<p><a href="https://postimg.cc/DJwFR6gN"><img src="https://i.postimg.cc/2646bXBC/41231231231.png" width="500px" title="source: imgur.com" /></a></p>

<ul>
  <li>sanity(明智) check: we should have 4 things to sum over (2 x 2 - think of looping over all possibilities in code)</li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>for brevity(简洁):
Don't show start condition
j = jump, d = duck

V(Start) = π(j)p(safe I j)*0 + π(j)p(hit I j) * (-1) + π(d)p(safe I j)*0 + π(d)p(safe I j)*(-1)
</code></pre></div></div>

<h3 id="example-7">Example 7</h3>
<p><a href="https://postimg.cc/VdKYB3WC"><img src="https://i.postimg.cc/y8HkBKJv/41241231231.png" width="500px" title="source: imgur.com" /></a></p>
<ul>
  <li>The Previous examples were easy: just work backwards</li>
  <li>Now we have a cycle: no notion of “backwards”</li>
  <li>Back to actions being “go to next state”</li>
  <li>R1 = R2 = -0.1</li>
  <li>Discount Factor ɣ = 0.9</li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>V(s1) = p(s1 I s1)(R1+ɣV(s1)) + p(s2 I s1)(R2 + ɣV(s2))
V(s1) = 0.3(-0.1+0.9V(s1)) + 0.7(-0.1 + 0.9V(s2)) = -0.1 + 0.27V(s1) + 0.63V(s2)
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>V(s2) = p(s1 I s2)(R1+ɣV(s1)) + p(s3 I s2)(R3 + ɣV(s3))
V(s2) = 0.6(-0.1+0.9V(s1)) + 0.4(1 + 0.9V(s3)) = 0.34 + 0.54V(s1) + 0.36V(s3)
V(s3) = 0
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>V(s1) = -0.1 + 0.27V(s1) + 0.63V(s2)
V(s2) =0.34 + 0.54V(s1)
V(s3) = 0
</code></pre></div></div>

<ul>
  <li>this is linear system (3 equations, 3 unknowns)</li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0.1   = -0.73V(s1) + 0.63V(s2)
-0.34 = 0.54V(s1) - V(s2) + 0.36V(s3)
0     = V(s3)
</code></pre></div></div>
<p><a href="https://postimg.cc/Whq7VPYG"><img src="https://i.postimg.cc/7YmtgHgW/41241312312.png" width="500px" title="source: imgur.com" /></a></p>

<ul>
  <li>of the form Ax=b
x = np.linalg.solve(A,b)</li>
  <li>V(s1) = 0.293</li>
  <li>V(s2) = 0.498</li>
  <li>V(s3) = 0</li>
</ul>

<h2 id="7-bellman-equation-summary">7. Bellman Equation Summary</h2>
<ul>
  <li>“Working backwards method”</li>
  <li>“Linear eqaution method”</li>
  <li>big picture:</li>
  <li>Get away from the idea that : “i need to try this on a finance data”, “i need to try this on a biology dataset”, etc.. algorithm doesn’t change, all it sees is a list of numbers</li>
  <li>Rather, we want to know: “ What are scalable algorithms that solve this modelling problem?”</li>
</ul>

<h2 id="8-optimal-policies--optimal-value-functions">8. Optimal Policies &amp; Optimal Value Functions</h2>
<ul>
  <li>these are interdependent - a key concept in this course - lots of depth to this idea</li>
  <li>we can talk about the relative “goodness” of policies
<a href="https://postimg.cc/gXrg6Ls5"><img src="https://i.postimg.cc/vBtkkWVQ/4124213.png" width="500px" title="source: imgur.com" /></a></li>
</ul>

<h3 id="the-best-policy">The Best Policy</h3>
<ul>
  <li>optimal policy is the “best” policy</li>
  <li>the policy for which there is no greater value function
<a href="https://postimg.cc/JGQjM4cB"><img src="https://i.postimg.cc/CL37CRRJ/41241231213.png" width="500px" title="source: imgur.com" /></a></li>
  <li>Optimal Policies are not unique, optimal value functions are :
<a href="https://postimg.cc/tnk7Q7Qq"><img src="https://i.postimg.cc/qgVKKC13/124141321312.png" width="500px" title="source: imgur.com" /></a>
<a href="https://postimg.cc/grzZ4PPz"><img src="https://i.postimg.cc/G3YxLc7y/412412312321.png" width="500px" title="source: imgur.com" /></a></li>
</ul>

<h3 id="relationship-between-v-and-q">Relationship Between V and Q</h3>
<ul>
  <li>Implementation advantage:</li>
  <li>to find the best action, we must actually do it to find the best V(s’)</li>
  <li>With Q, we simply need to Look up Q(s,a)
<a href="https://postimg.cc/SXRPcPND"><img src="https://i.postimg.cc/1zrZ1hxh/412441231312.png" width="500px" title="source: imgur.com" /></a></li>
</ul>

<h3 id="bellman-optimality-equation">Bellman Optimality Equation</h3>
<p><a href="https://postimg.cc/9wDTpGq3"><img src="https://i.postimg.cc/VvRRzgfN/41141312312.png" width="500px" title="source: imgur.com" /></a>
<a href="https://postimg.cc/R6kCkfDx"><img src="https://i.postimg.cc/C51ZfsC5/41241241414141.png" width="500px" title="source: imgur.com" /></a></p>

<h2 id="9-implementing-the-optimal-policy">9. Implementing the Optimal Policy</h2>
<ul>
  <li>Key point : Value Function already takes future rewards into account</li>
  <li>Just greedily choose the action that yield the best next-state value V(s’)</li>
  <li>requires look-ahead search</li>
  <li>if we have Q(s,a), no need to look ahead, simply choose argmax</li>
  <li>Q(s,a) thus effectively caches the look-ahead search results</li>
</ul>

<h2 id="10-step-from-value-function-to-optimal-policy-and-q-function">10. step from Value Function to Optimal Policy and Q function</h2>
<ul>
  <li>the value function depends on the policy by which the agent picks actions to perform. so if the agent uses a given policy π to select actions, the corresponding value function is given by :
<a href="https://postimg.cc/p9nLqqsJ"><img src="https://i.postimg.cc/YSdL2PgZ/14124141312312.png" width="500px" title="source: imgur.com" /><a></a></a></li>
  <li>among all possible value-functions, there exist an <strong>optimal value function</strong> that has higher value than other functions for all states
<a href="https://postimg.cc/Hrr5XG7S"><img src="https://i.postimg.cc/jSXXVtqr/412413124123.png" width="500px" title="source: imgur.com" /><a></a></a></li>
  <li>the optimal policy <strong>π</strong> is the policy that corresponds to optimal value function.
<a href="https://postimg.cc/7ftg34Mj"><img src="https://i.postimg.cc/8zNtVPFk/123121.png" width="500px" title="source: imgur.com" /></a>
<a href="https://postimg.cc/yW57xws6"><img src="https://i.postimg.cc/xTj8x2Zm/1411413123.png" width="500px" title="source: imgur.com" /><a></a></a></li>
  <li>In addition to the state value-function, for Convenience RL algorithms introduce another function which is the state-action pair <strong>Q-Function</strong>. Q is a function of a state-action pair and returns a real value
<a href="https://postimg.cc/crqPVZd3"><img src="https://i.postimg.cc/kXD9RXZT/412312312312.png" width="500px" title="source: imgur.com" /><a></a></a></li>
</ul>

<h2 id="11-mdp-summary">11. MDP Summary</h2>
<ul>
  <li>Purely Theoretical</li>
  <li>MDPs</li>
  <li>Policies</li>
  <li>Returns-total future reward</li>
  <li>Discounting future rewards with the discount rate, gamma</li>
  <li>state-value function</li>
  <li>action-value function</li>
  <li>bellman equation
<a href="https://postimg.cc/yg10JDhR"><img src="https://i.postimg.cc/FRcpBc7T/41241412312312.png" width="500px" title="source: imgur.com" /><a></a></a></li>
  <li>Bellman Optimality Equations</li>
  <li>Next Section:
    <ul>
      <li>Finding V given a policy</li>
      <li>Finding Optimal Policies and optimal values
<a href="https://postimg.cc/5X5Hwx2S"><img src="https://i.postimg.cc/rw3S2s63/41241312312312.png" width="500px" title="source: imgur.com" /><a><br />
Reference:</a></a></li>
    </ul>
  </li>
</ul>

<p><a href="https://www.udemy.com/course/artificial-intelligence-reinforcement-learning-in-python/">Artificial Intelligence Reinforcement Learning</a></p>

<p><a href="https://www.udemy.com/deep-reinforcement-learning-in-python/">Advance AI : Deep-Reinforcement Learning</a></p>

<p><a href="https://www.udemy.com/cutting-edge-artificial-intelligence/learn/lecture/14650508#overview">Cutting-Edge Deep-Reinforcement Learning</a></p>
:ET