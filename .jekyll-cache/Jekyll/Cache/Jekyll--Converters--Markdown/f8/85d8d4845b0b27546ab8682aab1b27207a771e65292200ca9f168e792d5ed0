I"(k<h1 id="tdλ">TD(λ)</h1>
<ul>
  <li>Generalize N-step method</li>
  <li>λ is associated with something called the []”eligibility trace”(전격 흔적)](https://sumniya.tistory.com/14)
    <ul>
      <li>eligibility trace라는 개념 : 이는 과거에 방문했던 state 중에서 현재 얻게 되는 reward에 영향을 주는 state를 판단하여, 현재 얻게 되는 reward을 해당 state에 나누어주는 것입니다. 이때, 영향을 주었다고 판단하는 index를 credit이라고하고 이 credit을 assign할 때, 두 가지 기준을 씁니다. 1) Frequency heuristic(얼마나 자주 방문했는가?) 2) Recency heuristic(얼마나 최근에 방문 했는가?)</li>
    </ul>
  </li>
  <li>N-step method code is complicated - not trivial to keep track of all rewards(then flush them once episode is over)</li>
  <li>TD(λ) allows us a more elegant method to trade-off between TD(0) and MC</li>
  <li>can update after just 1 step</li>
  <li>λ = 0 gives us TD(0), λ=1 gives us Monte Carlo
    <h2 id="n-step-method">N-step method</h2>
  </li>
  <li>Recall:</li>
</ul>

<p><a href="https://postimg.cc/kB92MvCf"><img src="https://i.postimg.cc/7YJ0VKW8/Capture.png" width="500px" title="source: imgur.com" /><a></a></a></p>
<ul>
  <li>strange idea: combine these :</li>
</ul>

<p><a href="https://postimg.cc/zVtqkv0D"><img src="https://i.postimg.cc/htghc7mm/Capture.png" width="300px" title="source: imgur.com" /><a></a></a></p>
<h2 id="tdλ-1">TD(λ)</h2>
<ul>
  <li>let’s take this strangeness to the next level - let’s combine more G’s, even an infinite number of G’s</li>
</ul>

<p><a href="https://postimg.cc/ZvnMYb3j"><img src="https://i.postimg.cc/MHm2DXGk/Capture.png" width="500px" title="source: imgur.com" /><a></a></a></p>
<ul>
  <li>λ must sum to 1 so that the result is on the same scale as individual G’s</li>
  <li>in TD(λ). we make the coefficients decrease geometrically</li>
</ul>

<p><a href="https://postimg.cc/p5jKJ3tR"><img src="https://i.postimg.cc/SRTVkpT2/Capture.png" width="500px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li><strong>Problem: these don’t sum to 1</strong></li>
  <li>we are most interested in the case when n - &gt; ∞</li>
  <li>we know this from calculus:</li>
</ul>

<p><a href="https://postimg.cc/34T4m1ZD"><img src="https://i.postimg.cc/fL3c4gt5/Capture.png" width="200px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li>therefore, we should scale the sum by thins amount</li>
  <li>we call this the <em>**</em></li>
</ul>

<p><a href="https://postimg.cc/fJtzWXc4"><img src="https://i.postimg.cc/Dy58gPmz/Capture.png" width="500px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li>we assume our episode will end at some point(say, time step T)</li>
  <li>when we reach step T - t, the episode is over, so any N-step return beyond this is the Full MC return, G(t)</li>
  <li>separate the partial N-step returns and the full returns</li>
</ul>

<p><a href="https://postimg.cc/sM1sHW2H"><img src="https://i.postimg.cc/nhGrzYfx/Capture.png" width="500px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li>manipulate 2nd term to simplify the sum</li>
</ul>

<p><a href="https://postimg.cc/fSk8QM0s"><img src="https://i.postimg.cc/prJ48TWm/Capture.png" width="500px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li>simplify further</li>
</ul>

<p><a href="https://postimg.cc/TKQQ1yqx"><img src="https://i.postimg.cc/q7HFmyNN/Capture.png" width="500px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li>when λ=0, we get the TD(0) return since $0^0$ = 1</li>
  <li>when λ=1, we get the full return(MC)</li>
  <li>any λ between 0 and 1 gives us a combination of individual returns(with geometrically decreasing weight)</li>
</ul>

<p><a href="https://postimg.cc/VJspKPKY"><img src="https://i.postimg.cc/QdcDxX5W/Capture.png" width="500px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li>isn’t this much more computational effort than both MC and N-step methods?</li>
  <li>yes, we would need to calculate G(t), $G^1$(t),…,$G^t$(t)</li>
  <li>lots of work (benefit is not clear)</li>
  <li>the algorithm we’re about to learn is an approximation to calculate the true λ-return(since that would be computationally infeasible)</li>
</ul>

<h2 id="td0">TD(0)</h2>
<ul>
  <li>let’s go back to TD(0) for a moment</li>
  <li>we call target - prediction the TD error</li>
</ul>

<p><a href="https://postimg.cc/7CKKbC47"><img src="https://i.postimg.cc/B60kYDwh/Capture.png" width="500px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li>parameter update is still gradient descent</li>
</ul>

<p><a href="https://postimg.cc/dZPr0vhr"><img src="https://i.postimg.cc/XqX8QY3H/Capture.png" width="200px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li>Eligibility trace is a vector the same size as parameter vector:</li>
</ul>

<p><a href="https://postimg.cc/dZPr0vhr"><img src="https://i.postimg.cc/XqX8QY3H/Capture.png" width="200px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li><strong>Eligibility Trace/ vector keeps track of old gradients</strong>, much like momentum from deep learning</li>
  <li>λ tell us <strong>“how much” of the past we want to keep</strong>
    <ul>
      <li>$e_0$ = 0, $e_t$ = $∇<em>0$V($s_t$) + ɣλe</em>(t_1)</li>
    </ul>
  </li>
</ul>

<h2 id="back-to-tdλ">back to TD(λ)</h2>
<ul>
  <li>redefine the parameter update to use <em>e</em> instead of only gradient</li>
</ul>

<p><a href="https://postimg.cc/8FhjbYwm"><img src="https://i.postimg.cc/HLNXTDyR/Capture.png" width="300px" title="source: imgur.com" /><a></a></a></p>

<ul>
  <li>recall how momentum works</li>
  <li>update only depends on next state</li>
  <li>we can do updates in 1 step rather than waiting N steps</li>
  <li>but still, just an approximation to using true λ-return</li>
  <li>N-step method and true λ-return require waiting for future rewards
    <ul>
      <li>we call this the “forward view”</li>
    </ul>
  </li>
  <li>in TD(λ), we update the current param based on past errors
    <ul>
      <li>we call this the “backward view”</li>
    </ul>
  </li>
</ul>

<blockquote>
  <p>Code</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># https://deeplearningcourses.com/c/deep-reinforcement-learning-in-python
# https://www.udemy.com/deep-reinforcement-learning-in-python
</span><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">print_function</span><span class="p">,</span> <span class="n">division</span>
<span class="kn">from</span> <span class="nn">builtins</span> <span class="kn">import</span> <span class="nb">range</span>
<span class="c1"># Note: you may need to update your version of future
# sudo pip install -U future
#
# Note: gym changed from version 0.7.3 to 0.8.0
# MountainCar episode length is capped at 200 in later versions.
# This means your agent can't learn as much in the earlier episodes
# since they are no longer as long.
#
# Adapt Q-Learning script to use TD(lambda) method instead
</span>
<span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">gym</span> <span class="kn">import</span> <span class="n">wrappers</span>
<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span>

<span class="c1"># code we already wrote
</span><span class="kn">from</span> <span class="nn">q_learning</span> <span class="kn">import</span> <span class="n">plot_cost_to_go</span><span class="p">,</span> <span class="n">FeatureTransformer</span><span class="p">,</span> <span class="n">plot_running_avg</span>
</code></pre></div></div>

<blockquote>
  <p>Base model</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">BaseModel</span><span class="p">:</span>
    <span class="c1"># this is eligience traces
</span>  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">D</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">D</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">D</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">partial_fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">eligibility</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">+=</span> <span class="n">lr</span><span class="o">*</span><span class="p">(</span><span class="n">target</span> <span class="o">-</span> <span class="n">input_</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">))</span><span class="o">*</span><span class="n">eligibility</span>

  <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">)</span>
</code></pre></div></div>

<blockquote>
  <p>model</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Holds one BaseModel for each action
</span><span class="k">class</span> <span class="nc">Model</span><span class="p">:</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">feature_transformer</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">env</span> <span class="o">=</span> <span class="n">env</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">models</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">feature_transformer</span> <span class="o">=</span> <span class="n">feature_transformer</span>

    <span class="n">D</span> <span class="o">=</span> <span class="n">feature_transformer</span><span class="o">.</span><span class="n">dimensions</span> <span class="c1"># 2000 D
</span>    <span class="bp">self</span><span class="o">.</span><span class="n">eligibilities</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span><span class="p">,</span> <span class="n">D</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span><span class="p">):</span>
      <span class="n">model</span> <span class="o">=</span> <span class="n">BaseModel</span><span class="p">(</span><span class="n">D</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">feature_transformer</span><span class="o">.</span><span class="n">transform</span><span class="p">([</span><span class="n">s</span><span class="p">])</span>
    <span class="k">assert</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">m</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">models</span><span class="p">])</span><span class="o">.</span><span class="n">T</span>
    <span class="k">assert</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">result</span>

  <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">G</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">lambda_</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">feature_transformer</span><span class="o">.</span><span class="n">transform</span><span class="p">([</span><span class="n">s</span><span class="p">])</span>
    <span class="k">assert</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">eligibilities</span> <span class="o">*=</span> <span class="n">gamma</span><span class="o">*</span><span class="n">lambda_</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">eligibilities</span><span class="p">[</span><span class="n">a</span><span class="p">]</span> <span class="o">+=</span> <span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">models</span><span class="p">[</span><span class="n">a</span><span class="p">]</span><span class="o">.</span><span class="n">partial_fit</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">G</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">eligibilities</span><span class="p">[</span><span class="n">a</span><span class="p">])</span>

  <span class="k">def</span> <span class="nf">sample_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">eps</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">eps</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">s</span><span class="p">))</span>
</code></pre></div></div>

<blockquote>
  <p>play</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># returns a list of states_and_rewards, and the total reward
</span><span class="k">def</span> <span class="nf">play_one</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">lambda_</span><span class="p">):</span>
  <span class="n">observation</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
  <span class="n">done</span> <span class="o">=</span> <span class="bp">False</span>
  <span class="n">totalreward</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="n">iters</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="c1"># while not done and iters &lt; 200:
</span>  <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span> <span class="ow">and</span> <span class="n">iters</span> <span class="o">&lt;</span> <span class="mi">10000</span><span class="p">:</span>
    <span class="n">action</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">sample_action</span><span class="p">(</span><span class="n">observation</span><span class="p">,</span> <span class="n">eps</span><span class="p">)</span>
    <span class="n">prev_observation</span> <span class="o">=</span> <span class="n">observation</span>
    <span class="n">observation</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>

    <span class="c1"># update the model
</span>    <span class="nb">next</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">observation</span><span class="p">)</span>
    <span class="k">assert</span><span class="p">(</span><span class="nb">next</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span><span class="p">))</span>
    <span class="n">G</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">+</span> <span class="n">gamma</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="nb">max</span><span class="p">(</span><span class="nb">next</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">model</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">prev_observation</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">G</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">lambda_</span><span class="p">)</span>

    <span class="n">totalreward</span> <span class="o">+=</span> <span class="n">reward</span>
    <span class="n">iters</span> <span class="o">+=</span> <span class="mi">1</span>

  <span class="k">return</span> <span class="n">totalreward</span>


</code></pre></div></div>

<blockquote>
  <p>main</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">'__main__'</span><span class="p">:</span>
  <span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s">'MountainCar-v0'</span><span class="p">)</span>
  <span class="n">ft</span> <span class="o">=</span> <span class="n">FeatureTransformer</span><span class="p">(</span><span class="n">env</span><span class="p">)</span>
  <span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">ft</span><span class="p">)</span>
  <span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.9999</span>
  <span class="n">lambda_</span> <span class="o">=</span> <span class="mf">0.7</span>

  <span class="k">if</span> <span class="s">'monitor'</span> <span class="ow">in</span> <span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">:</span>
    <span class="n">filename</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">basename</span><span class="p">(</span><span class="n">__file__</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">'.'</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">monitor_dir</span> <span class="o">=</span> <span class="s">'./'</span> <span class="o">+</span> <span class="n">filename</span> <span class="o">+</span> <span class="s">'_'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">())</span>
    <span class="n">env</span> <span class="o">=</span> <span class="n">wrappers</span><span class="o">.</span><span class="n">Monitor</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">monitor_dir</span><span class="p">)</span>


  <span class="n">N</span> <span class="o">=</span> <span class="mi">300</span>
  <span class="n">totalrewards</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
  <span class="n">costs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
    <span class="c1"># eps = 1.0/(0.1*n+1)
</span>    <span class="n">eps</span> <span class="o">=</span> <span class="mf">0.1</span><span class="o">*</span><span class="p">(</span><span class="mf">0.97</span><span class="o">**</span><span class="n">n</span><span class="p">)</span>
    <span class="c1"># eps = 0.5/np.sqrt(n+1)
</span>    <span class="n">totalreward</span> <span class="o">=</span> <span class="n">play_one</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">lambda_</span><span class="p">)</span>
    <span class="n">totalrewards</span><span class="p">[</span><span class="n">n</span><span class="p">]</span> <span class="o">=</span> <span class="n">totalreward</span>
<span class="c1">#     print("episode:", n, "total reward:", totalreward)
</span>  <span class="k">print</span><span class="p">(</span><span class="s">"avg reward for last 100 episodes:"</span><span class="p">,</span> <span class="n">totalrewards</span><span class="p">[</span><span class="o">-</span><span class="mi">100</span><span class="p">:]</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
  <span class="k">print</span><span class="p">(</span><span class="s">"total steps:"</span><span class="p">,</span> <span class="o">-</span><span class="n">totalrewards</span><span class="o">.</span><span class="nb">sum</span><span class="p">())</span>

  <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">totalrewards</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Rewards"</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

  <span class="n">plot_running_avg</span><span class="p">(</span><span class="n">totalrewards</span><span class="p">)</span>

  <span class="c1"># plot the optimal state-value function
</span>  <span class="n">plot_cost_to_go</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
</code></pre></div></div>

<h1 id="td-lamda-summary">TD LAMDA summary</h1>
<ul>
  <li>```
Reference:</li>
</ul>

<p><a href="https://www.udemy.com/course/artificial-intelligence-reinforcement-learning-in-python/">Artificial Intelligence Reinforcement Learning</a></p>

<p><a href="https://www.udemy.com/deep-reinforcement-learning-in-python/">Advance AI : Deep-Reinforcement Learning</a></p>

<p><a href="https://www.udemy.com/cutting-edge-artificial-intelligence/learn/lecture/14650508#overview">Cutting-Edge Deep-Reinforcement Learning</a></p>
:ET