I"å<h1 id="the-value-function">The Value function</h1>

<h2 id="1-assigning-rewards">1. Assigning Rewards</h2>
<p><a href="https://postimg.cc/rd9dKMSW"><img src="https://i.postimg.cc/zfsT2fvd/51212413.png" width="700px" title="source: imgur.com" /></a></p>
<ul>
  <li>the programmers are like ‚Äúcoaches(ÊïôÁªÉ)‚Äù to the AI</li>
  <li>Pet Owner is a good analogy</li>
  <li>We define how to give rewards to agent (Ex. Give same reward no matter what agent does - agent will always just behave randomly)(Ex2. Give our dog a treat when it behaves badly - encourages bad behavior)</li>
</ul>

<h2 id="2-maze-example">2. Maze Example</h2>
<p><a href="https://postimg.cc/0bCTKWVT"><img src="https://i.postimg.cc/mgxBqKCg/512312412321413.png" width="700px" title="source: imgur.com" /></a></p>
<ul>
  <li>Robot Trying to solve maze</li>
  <li>Reward of 1 for finding the maze exit, else 0</li>
  <li>But robot unlikely to solve the maze with this structure</li>
  <li>if robot only sees reward is the max reward if all we see is 0</li>
  <li>better: Every step yields reward of -1</li>
  <li>Now robot is encouraged so solve maze as quickly as possible</li>
</ul>

<h2 id="3-something-like">3. Something like</h2>
<ul>
  <li>be careful not to build our own prior knowledge into the AI ( Ex. Chess)</li>
  <li>Agent should be rewarded for winning, not taking opponent‚Äôs pieces</li>
  <li>No reward for implementing strategy us read about in chess book</li>
  <li>Free the agent to find its own solution</li>
  <li><em>OK</em> to lose all but one piece and then win</li>
  <li>tell the agent <strong>what</strong> we want it to achieve, not <strong>how</strong> we want it to be achieved</li>
</ul>

<h2 id="4-planning">4. Planning</h2>
<ul>
  <li>Scenario: we are thinking about studying for tomorrow‚Äôs exam. we would rather hang out with friends.
    <ul>
      <li>hangout with friends -&gt; dopamine hit -&gt; happy</li>
      <li>Study -&gt; feel tired and bored</li>
      <li>why study?</li>
    </ul>
  </li>
  <li>we don‚Äôt think about immediate rewards, but future rewards too. we want to assign some value to the current state that reflects the future too. call this the <strong>‚ÄúVALUE FUNCTION‚Äù</strong>
<script async="" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script></li>
</ul>
<div align="center" style="margin: 1em 0;">
<ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-5150894678574694" data-ad-slot="9221331439" data-ad-format="auto" data-full-width-responsive="true"></ins>
     </div>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

<h2 id="5-credit-assignment-problem">5. Credit Assignment Problem</h2>
<ul>
  <li>we receive a reward - getting hired for our dream job</li>
  <li>what previous action led to this success</li>
  <li>that called the <strong>‚ÄúCredit Assignment Problem‚Äù</strong></li>
  <li>Ask the Question: ‚ÄúWhat did I do in the past that led to the reward I‚Äôm receiving now ‚Äú</li>
  <li>what action gets the credit</li>
</ul>

<h2 id="6-attribution">6. Attribution</h2>
<ul>
  <li>Related to online advertising concept of attribution</li>
  <li>if we show the user the same ad 10 times before they buy, which ad gets the credit?</li>
  <li>In RL we don‚Äôt just assign ad-hoc(ÁÇπÂØπÁÇπ) like this
<a href="https://postimg.cc/wtdR4kKQ"><img src="https://i.postimg.cc/yNDhdfvt/55123.png" width="500px" title="source: imgur.com" /></a></li>
</ul>
:ET