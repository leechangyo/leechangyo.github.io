I"i<h1 id="montel-carlomc-methods-introduction">Montel Carlo(MC) Methods Introduction</h1>
<ul>
  <li>Last Section: Dynamic Programming</li>
  <li>would that work for self-driving cars or video games?</li>
  <li>can i just set the state of the agent?</li>
  <li>“god-mode” capabilities?</li>
  <li><strong>MC Methods learn purely from experience</strong></li>
  <li>Montel Carlo usually refers to any method with a significant random component</li>
  <li>Random Component in RL is the return</li>
  <li>With MC, instead of calculating the true expected value of G, <strong>we calculate its sample mean</strong>
<a href="https://postimg.cc/6yrZL1Rr"><img src="https://i.postimg.cc/c4jcWpLb/22222.png" width="500px" title="source: imgur.com" /></a></li>
  <li>Need to assume episode tasks only</li>
  <li>Episode must terminate before we calculate return</li>
  <li>Not “fully” online since we need to wait for entire episode to finish before updating</li>
  <li>(full online mean is update after every action)</li>
  <li>monte carlo methods is not fully online which mean it is updated after episode to finish</li>
  <li>Should Remind you of multi-armed bandit</li>
  <li><strong>Multi-Armed bandit : average reward after every action</strong></li>
  <li>MDPs: average the return</li>
  <li>One way to think of MC-MDP is every state is a separate multi-armed bandit problem</li>
  <li>Follow the same pattern</li>
  <li>Prediction Problem(Finding Value given policy)</li>
  <li>Control Problem(finding optimal policy)</li>
</ul>

<h2 id="1-monte-carlo-for-prediction-problem">1. Monte Carlo for prediction Problem</h2>
<ul>
  <li>Recall what V(s) is :
 <a href="https://postimg.cc/t1yqhpWb"><img src="https://i.postimg.cc/hGQXRGGX/4124131233.png" width="500px" title="source: imgur.com" /></a></li>
  <li>Any expected value can be approximated like
<a href="https://postimg.cc/rzYYWjxF"><img src="https://i.postimg.cc/dtqMbS0d/41241231321312.png" width="500px" title="source: imgur.com" /></a></li>
  <li>“i” is episode, “t” is steps</li>
</ul>

<h3 id="how-do-we-generate-g">How do we generate G?</h3>
<ul>
  <li>just play a bunch of episode, log the states and reward sequences
    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>s1 s2 s3 ... sT
r1 r2 r3 ... rT
</code></pre></div>    </div>
  </li>
  <li>
    <p>Calculate G from Definition:
G(t) = r(t+1)+gamma*G(t+1)</p>
  </li>
  <li>very helpful to calculate G by iterating through states in reverse order</li>
  <li>Once we have(s,G) pairs, average them for each s</li>
</ul>

<h3 id="multiple-visits-to-s">Multiple Visits to s</h3>
<ul>
  <li>what if we see the same state more than once in an episode</li>
  <li>E.g. we see s at t=1 and t=3</li>
  <li>which return should we use? G(1) or G(3</li>
  <li>First-visit method :
    <ul>
      <li>Use t = 1 only</li>
    </ul>
  </li>
  <li>Every-visit method :
    <ul>
      <li>Use both t=1 and t=3 as samples</li>
    </ul>
  </li>
  <li>Surprisingly, it has been proven that both lead to same answer</li>
</ul>

<blockquote>
  <p>First-Visit MC Pseudocode</p>
</blockquote>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def first_visit_monte_carlo_prediction(π, N):
  V = random initialization
  all_return = {} # default = []
  do N times:
    states, returns = play_episode
    for s, g in zip(states, returns):
      if not seen s in this episode yet:
        all_return[s].append(g)
        V(s) = sample_mean(all_returns[s])
  return V
</code></pre></div></div>

<h3 id="sample-mean">Sample Mean</h3>
<ul>
  <li>Notice how we store all returns in a list</li>
  <li>Didn’t we discuss how that’s inefficient?</li>
  <li>Can also use previous mean to calculate current mean</li>
  <li>Can also use moving average for non-stationary problems</li>
  <li>Everything we learned before still applies
<a href="https://postimg.cc/6yrZL1Rr"><img src="https://i.postimg.cc/c4jcWpLb/22222.png" width="500px" title="source: imgur.com" /></a></li>
  <li>Rules of probability still apply</li>
  <li><a href="https://ko.wikipedia.org/wiki/%EC%A4%91%EC%8B%AC_%EA%B7%B9%ED%95%9C_%EC%A0%95%EB%A6%AC">Central limit Theorem</a></li>
  <li>Variance of estimate = Variance of RV(Random Value) / N</li>
</ul>

<blockquote>
  <p>Calculating Returns from Rewards</p>
</blockquote>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>s = grid.current_state()
states_and_rewards = [(s,0)]
while not game_over:
  a = policy(s)
  r = grid.move(a)
  s = grid.current_state()
  states_and_rewards.append((s,r))

G = 0
states_and_returns = []
for s,r in reverse(states_and_rewards):
  states_and_returns.((s,G))
  G = r + gamma*G
states_and_returns.reverse
</code></pre></div></div>

<h3 id="mc">MC</h3>
<ul>
  <li>Recall: one Disadvantage of DP is that <strong>we need to loop through all states</strong></li>
  <li>MC: only update V for visited states</li>
</ul>
:ET