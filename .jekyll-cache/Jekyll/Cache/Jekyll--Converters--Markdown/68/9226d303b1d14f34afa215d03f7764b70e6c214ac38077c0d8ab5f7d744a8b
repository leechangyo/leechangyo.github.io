I"<h1 id="value-function--the-bellman-equation">Value Function &amp; The Bellman Equation</h1>
<p><a href="https://postimg.cc/ZCfNJ0tQ">&lt;img src=â€https://i.postimg.cc/jST4QJ8R/22.pngâ€width=â€500pxâ€ title=â€source: imgur.comâ€ /&gt;</a></p>
<ul>
  <li>The full derivation(èµ·æº) can be tough if our probability skills are not yet strong enough</li>
</ul>

<h2 id="1-expected-values">1. Expected Values</h2>
<ul>
  <li>why is this strange to many people?</li>
  <li>Consider a coin toss: Heads = Win, Tails = Lose</li>
  <li>Numerically: H = 1, T = 0</li>
  <li>suppose, P(W) = 60%</li>
  <li><strong>Expected Value is 0.6 x 1 + 0.4 * 0 = 0.6</strong></li>
  <li>The â€œExpected Valueâ€ is a â€œValueâ€ I can â€œnever expectâ€
<a href="https://postimg.cc/Mvs9gWs6">&lt;img src=â€https://i.postimg.cc/NjtZ9FmH/22.pngâ€width=â€500pxâ€ title=â€source: imgur.comâ€ /&gt;</a></li>
  <li>Whatâ€™s the point of expected values?</li>
  <li>it tells us the <strong>mean/average</strong> (E.g. we gather up all the heights of students in the call and calculate the mean- no student may have the mean height, but itâ€™s a useful statistic)</li>
  <li>it doesnâ€™t matter if a coin flip will never give me 0.6, it just an average
<a href="https://postimg.cc/8fQ2J18Q">&lt;img src=â€https://i.postimg.cc/BbX4z6M6/2222.pngâ€width=â€500pxâ€ title=â€source: imgur.comâ€ /&gt;</a></li>
  <li>x is state()</li>
</ul>

<h2 id="2-probability-trees">2. probability Trees</h2>
<ul>
  <li>our expected reward is the weighted sum of each possible outcome(weighted by the probability at the corresponding branch)
<a href="https://postimg.cc/kDYjpJYn">&lt;img src=â€https://i.postimg.cc/zX8933nR/12.pngâ€width=â€500pxâ€ title=â€source: imgur.comâ€ /&gt;</a></li>
  <li>the same concept extends to any number of possible outcomes</li>
  <li>in general: Expected value = p(e1) x value(e1) + p(e2) x value(e2) + â€¦.
<a href="https://postimg.cc/qt9JwHfQ">&lt;img src=â€https://i.postimg.cc/xCdX096r/33212.pngâ€width=â€500pxâ€ title=â€source: imgur.comâ€ /&gt;</a></li>
</ul>

<h2 id="3-why-are-averages-important">3. Why are averages important?</h2>
<p><a href="https://postimg.cc/H8M6fKLp">&lt;img src=â€https://i.postimg.cc/nrRNQfbQ/22.pngâ€width=â€500pxâ€ title=â€source: imgur.comâ€ /&gt;</a></p>
<ul>
  <li>A subsection(åˆ†éƒ¨) of a tree is also a tree â€“ recursion(é€’å½’)</li>
  <li>After Arriving in this state, what happens next can be considered Random</li>
  <li>Hence, we cannot say: â€œif i reach this state, i will get X rewardâ€</li>
  <li>we can only say: â€œif i reach this state, i will get X reward on <strong>average</strong>â€</li>
</ul>

<h2 id="4-a-fundamental-concept-of-the-value-function">4. A fundamental concept of The Value Function</h2>
<ul>
  <li>At each state s, i will get a reward R</li>
  <li>overall return G, is the sum of rewards i get</li>
  <li>we want to be able to answer:
    <ul>
      <li>â€œif i am in state, s what is the sum of reward i will get in the future, on average?â€</li>
      <li>
        <table>
          <tbody>
            <tr>
              <td>we say: V(s) = E(G</td>
              <td>s)</td>
            </tr>
          </tbody>
        </table>
      </li>
    </ul>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>â€</td>
          <td>â€ means â€œgivensâ€ - anything to the left is random/ right is not random</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>note : this is called a â€œconditional expectationâ€</li>
  <li>Value Function is equal to Expected function of Total Return by state
<a href="https://postimg.cc/gwQzsXXf">&lt;img src=â€https://i.postimg.cc/bJNZz1zN/2223123.pngâ€width=â€500pxâ€ title=â€source: imgur.comâ€ /&gt;</a></li>
  <li>Every Game we play is just a series of states and rewards</li>
  <li>Letâ€™s pretend everything is deterministic for now, e.g. E(3) = 3</li>
  <li>The value of a state is just the sum of all future rewards(if they are deterministic)
    <ul>
      <li>V($s_1$) = $r_2$ + $r_3$ + $r_4$ + â€¦ + $r_N$</li>
      <li>V($s_2$) =         $r_3$ + $r_4$ + â€¦ + $r_N$</li>
      <li>Key: V($s_1$) = $r_2$ + V($s_2$)</li>
    </ul>
  </li>
  <li>Discounted Version
    <ul>
      <li>V($s_1$) = $r_2$ + É£$r_3$ + É£$r_4$ + â€¦ + $r_N$</li>
      <li>V($s_2$) =         É£$r_3$ + É£$r_4$ + â€¦ + É£$r_N$</li>
      <li>Key: V($s_1$) = $r_2$ + É£V($s_2$)</li>
      <li>(still assuming everything is deterministic)</li>
    </ul>
  </li>
  <li>In more general terms
    <ul>
      <li>Letâ€™s make s = current state, sâ€™=next state</li>
      <li>Letâ€™s make r = reward(reall means R(s,sâ€™)- the reward i get from going from s to sâ€™)</li>
      <li><strong>V(s) = E[r + É£V(sâ€™)]</strong></li>
      <li>this is the essence of the bellman equation</li>
    </ul>
  </li>
  <li>Expansion
    <ul>
      <li>V(s) = E[r + É£E(râ€™+É£V(sâ€™â€™))]</li>
      <li>V(s) = E[r + É£E(râ€™+É£E(râ€™â€˜+â€¦))]</li>
    </ul>
  </li>
</ul>
:ET