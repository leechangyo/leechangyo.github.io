I"U9<h1 id="temporal-difference-td-learning">Temporal Difference (TD) Learning</h1>
<ul>
  <li>this section is a 3rd technique for solving MDPs</li>
  <li>TD = Temporal Difference(TD) Learning</li>
  <li>Combines ideas from DP and MC</li>
  <li>Disadvantage of DP: requires full model of environment, never learns from experience</li>
  <li>MC and TD learn from experience</li>
  <li>MC can only update after completing episode, but DP uses Bootstrapping(Making an initial estimate)</li>
  <li>We will see that TD also uses Bootstrapping and is fully online, can update value during an episode
    <h2 id="1-td-learning">1. TD Learning</h2>
  </li>
  <li>Same approach as before</li>
  <li>First Predict Problem</li>
  <li>them control problem</li>
  <li>2 control methods:
    <ul>
      <li>SARSA</li>
      <li>Q-Learning</li>
    </ul>
  </li>
  <li>Model-Free Reinforcement Learning</li>
  <li>TD methods learn directly from episode of experience.</li>
  <li>no Knowledge of MDP Transition / rewards</li>
  <li>TD learns from incomplete episodes by Bootstrapping</li>
  <li>TD updates a guess towards a guess
<a href="https://postimg.cc/fkPHCsWG"><img src="https://i.postimg.cc/28kPzjsS/41231312313.png" width="500px" title="source: imgur.com" /><a></a></a></li>
  <li>MC와 다른점은, MC는 실제 에피소드가 끝나고 받게 되는 보상을 사용하여 Value Function을 업데이트 하였다</li>
  <li>TD에서는 실제 보상과 다음 step에 대한 미래추정가치를 사용해서 학습한다.</li>
  <li>이때 사용하는 보상과 Value Function의 합을 TD Target
-그리고 TD Target 과 실제 V(S)와의 차이를 TD error라고 표현한다.
<a href="https://postimg.cc/FfJFfNRh"><img src="https://i.postimg.cc/TPtK4Pv5/14141241313.png" width="500px" title="source: imgur.com" /><a></a></a></li>
  <li>MC에서의 Value function이 업데이트 되는 과정이 왼쪽(에피소드가 전체적으로 끝나서 그의 보상을 나누어 단계별로 업데이트)</li>
  <li>TD는 각 단계별로 업데이트가 되는 과정으로 오른쪽 그림</li>
  <li>TD의 장점은 에피소드 중간에서도 학습을 하게 된다는 것이다.</li>
  <li>MC에서는 에피소드가 끝날때까지 기다렸다가 업데이트가 발생하고 학습하기 떄문이다.</li>
  <li>TD는 종료 없는 연속적인 에피소드에서도 학습할 수 있다.</li>
  <li>Return $G_t$ = R(t+1)+ɣR(t+2)+…+ɣ^(t-1)$R_T) is unbiased estimate of $v_π$($S_t$)</li>
  <li>True TD Target  R(t+1)+ɣV(S(t+1)) is biased estimate of $v_π$($S_t$)</li>
  <li>TD Target is much lower variance than the return:
    <ul>
      <li>return depends on many random actions, transitions, rewards</li>
      <li>TD Target depends on one random action, transition, reward</li>
    </ul>
  </li>
  <li>V policy가 실제 G에 대해서 unbias라 할때는 TD Target도 V policy를 추종하기 unbias이다. 하지만 TD Target에 V policy를 추정하는 V(St+1)를 사용하기에  실제값이 아니라 실제값을 추정하는 값임으로 bias가 발생한다. 그리고 TD Target은 단지 하나의 step에서만 계산하기에 noise가 작게 되므로 상대적으로 variance가 낮게 나타난다.</li>
  <li>MC has high variance, zero bias
    <ul>
      <li>good convergence properties</li>
      <li>even with function approximation</li>
      <li>not very sensitive to initial value</li>
      <li>very simple to understand and use</li>
    </ul>
  </li>
  <li>TD has low variance, some bias
    <ul>
      <li>Usually more efficient than MC</li>
      <li>TD(0) converges to $v_π$(s)</li>
      <li>but not always with function approximation</li>
      <li>More sensitive to initial value</li>
    </ul>
  </li>
  <li>Compare on variance between MC and TD
<a href="https://postimg.cc/hhyS0pYJ"><img src="https://i.postimg.cc/L6Rn2WBV/41241313123.png" width="500px" title="source: imgur.com" /><a></a></a></li>
  <li>Bootstrapping이 더 학습하는데 효율적이다.</li>
  <li>MC and TD converge : V(s) -&gt; $v_π$(s) as experience -&gt; ∞ (에피소드는 무한 반복하게 되면 결국 수렴하게 되어있다.)</li>
  <li>but what about batch solution for finite experience
<a href="https://postimg.cc/RqDW0S4b"><img src="https://i.postimg.cc/QxX10CPs/41241312312312312312.png" width="300px" title="source: imgur.com" /><a></a></a></li>
  <li>e.g repeatedly sample episode k ∈ [1,K]</li>
  <li>Apply MC or TD(0) to episode k
<a href="https://postimg.cc/VrwVnPwx"><img src="https://i.postimg.cc/wvRYK9Gj/3121.png" width="500px" title="source: imgur.com" /><a></a></a></li>
  <li>For example, first episode A got reward “0” and B got reward “0”</li>
  <li>from Second Episode B got reward “1” to seven episode. and B got 0 at the last episode.</li>
  <li>then A will go B within 100 %  and then reward will be “0”</li>
  <li>in the MC methods, A will get reward “0” because the episode pass Through A is one that final reward is zero
6- in the TD methods, running another episode because of B value update to “6” then A value also going to be updated</li>
  <li>MC converges to solution with minimum mean-square
    <ul>
      <li>best fit to the observe returns :
<a href="https://postimg.cc/bSwDGrBs"><img src="https://i.postimg.cc/4yzzShvb/412312123123.png" width="300px" title="source: imgur.com" /><a></a></a></li>
      <li>in the AB example, V(A)=0</li>
    </ul>
  </li>
  <li>TD(0) converges to solution of max likelihood Markov Model
    <ul>
      <li>Solution to the MDP (S,A,P^,R^,ɣ) that best fits the data
<a href="https://postimg.cc/zytd3H2R"><img src="https://i.postimg.cc/QMsv4ppS/12412412312312.png" width="500px" title="source: imgur.com" /><a></a></a></li>
      <li>in the AB example, V(A) = 0.75</li>
    </ul>
  </li>
  <li>TD(0)방식은 max likelihood Markv 방식을 사용하여 수렴하는 알고리즘이다. MDP 기반으로 실제적인 값을 찾아가게 되기 때문에 V(A)의 값이 6/8 평균가치가 계산되어 0.75값으로 업데이트가 된다.</li>
  <li>TD exploits Markov property
    <ul>
      <li>Usually more efficient in Markov environment</li>
    </ul>
  </li>
  <li>MC does not exploit Markov property
    <ul>
      <li>Usually more effective in Non-Markov environment</li>
    </ul>
  </li>
  <li>MC 알고르짐
<a href="https://postimg.cc/RNjYTHL5"><img src="https://i.postimg.cc/cLHN07KH/12412312312.png" width="500px" title="source: imgur.com" /><a></a></a></li>
  <li>Bootstrapping을 사용하여 states에 대한 value들을 추정하여 사용하는 방법 TD
<a href="https://postimg.cc/MMVHvR56"><img src="https://i.postimg.cc/B6wK7B1K/1414131312.png" width="500px" title="source: imgur.com" /><a></a></a></li>
  <li>DP 방식
<a href="https://postimg.cc/RqGJkkPX"><img src="https://i.postimg.cc/3JQjv725/41231231231231.png" width="500px" title="source: imgur.com" /><a></a></a></li>
  <li>DP 방식에서는 모델을 통해서 이미 MDP를 알고 있고 이에 대한 value 와 reward를 통해 학습을 하기 때문에 위에와 같이 나옵니다.</li>
  <li>Bootstrapping: update involves an estimate
    <ul>
      <li>MC does not bootstrap</li>
      <li>DP bootstraps</li>
      <li>TD bootstrap</li>
    </ul>
  </li>
  <li>Sampling update samples an expectation
    <ul>
      <li>MC samples</li>
      <li>DP does not sample</li>
      <li>TD samples</li>
    </ul>
  </li>
  <li>DP와 TP에서 사용하는 Bootstrapping은 추정 값을 기반으로 하여 업데이트</li>
  <li>MC에서 사용하는 샘플링은 expectation을 샘플하여 업데이트 한다.</li>
  <li>TD도 샘플링을 한지만 DP처럼 full backup을 하지 않는다.
<a href="https://postimg.cc/RqbK6rcq"><img src="https://i.postimg.cc/WbsnyTzm/4123131231231.png" width="500px" title="source: imgur.com" /><a></a></a></li>
</ul>

<h2 id="2-td0-prediction">2. TD(0) Prediction</h2>
<ul>
  <li>Apply TD to prediction problem</li>
  <li>algorithm is called TD(0)</li>
  <li>there is also TD(1) and TD(λ)</li>
  <li>it is related to Q-Learning and approximation methods</li>
</ul>

<h3 id="mc">MC</h3>
<ul>
  <li>Recall: one Disadvantage of MC is we need to wait until the episode is finished then we calculate return</li>
  <li>also recall: Multiple ways of calculating averages</li>
  <li>General “average-finding” equation</li>
  <li>Does not require us to store all returns</li>
  <li>Constant alpha is moving average/exponential decay
<a href="https://postimg.cc/B8LQVyj7"><img src="https://i.postimg.cc/cJTgrG0W/41312312312.png" width="500px" title="source: imgur.com" /><a></a></a></li>
</ul>

<h3 id="value-function">Value function</h3>
<ul>
  <li>now recall: the definition of V</li>
  <li>We can define it recursively
<a href="https://postimg.cc/D80ZG82K"><img src="https://i.postimg.cc/15cNLq24/4123123123123124123.png" width="500px" title="source:imgur.com" /><a></a></a></li>
</ul>

<h3 id="td0">TD(0)</h3>
<ul>
  <li>can we just combine these(MC &amp; Value Function)</li>
  <li>instead of sampling the return, use the recursive definition
<a href="https://postimg.cc/94MJ5P2n"><img src="https://i.postimg.cc/HL0KM3hd/4124123124123.png" width="500px" title="source:imgur.com" /><a></a></a></li>
  <li>this is TD(0)</li>
  <li>why this is fully online? <strong>we can update V(s) as soon as we know s’</strong></li>
</ul>

<h3 id="sources-of-randomness">sources of randomness</h3>
<ul>
  <li>In Mc, randomness arises when an episode can play out in different ways(due to stochastic policy, or stochastic state transitions)</li>
  <li>in TD(0), we have another source of randomness
    <ul>
      <li>G is an exact sample in MC</li>
      <li>r + ɣV(s’) is itself an estimate of G</li>
    </ul>
  </li>
  <li>we are estimating from other estimates(bootstrapping)</li>
</ul>

<h3 id="summary">Summary</h3>
<ul>
  <li>TD(0) is advantageous in comparison to MC/DP</li>
  <li>Unlike DP, we don’t require a model of the environment, and only update V for states we visit</li>
  <li>unlike MC, we don’t need to wait for an episode to finish</li>
  <li>advantageous for very long episodes</li>
  <li>also applicable to continuous(non-episodic)tasks</li>
</ul>

<h2 id="3-td-learning-for-control">3. TD Learning for Control</h2>
<ul>
  <li>Apply TD(0) to Control</li>
  <li>we can probably guess what we’re going to do by now</li>
  <li>use Generalized policy iteration, alternate between TD(0) for prediction, policy improvement using <strong>greedy action selection</strong></li>
  <li><strong>Use Value iteration method</strong>: Update Q in-place, improve policy after every change</li>
  <li>skip the part where we do full policy evaluation</li>
</ul>

<h2 id="4-sarsa">4. SARSA</h2>
<ul>
  <li>Recall from MC: we want to use Q because it’s indexed by a, V is only indexed by s</li>
  <li>Q has the same recursive form</li>
  <li>Same limitation as MC: need lots of samples in order to converge</li>
  <li>require the 5-tuuple:(s,a,r,s’,a’)</li>
  <li>Hence the name
<a href="https://postimg.cc/dZ3DB1cp"><img src="https://i.postimg.cc/C5HDwBPh/41412131231232.png" width="500px" title="source:imgur.com" /><a></a></a></li>
  <li>TD방식과 다른점은 Value function을 쓰지 않고 Q Fucntion을 쓴다.</li>
  <li>Like MC, still requires us to know Q(s,a) for all a, in order to choose argmax</li>
  <li>problem: if we follow a deterministic policy, we would only fill in 1/IAI values on each episode</li>
  <li>Leave most of Q untouched</li>
  <li>Remedy(处理方法): Exploring starts or policy that includes exploration</li>
  <li>use epsilon-greedy</li>
</ul>

<blockquote>
  <p>Pseudocode</p>
</blockquote>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Q(s,a) = arbitrary, Q(terminal,a) = 0
for t=1..N
  s = start_state, a = epsilon_greedy_from(Q(s))
  while not game over:
    s', r = do_action(a)
    a' = episode_greedy_from(Q(s'))
    Q(s,a) = Q(s,a) + alpha + [r + gamma*Q(s',a')-Q(s,a)]
    s = s', a = q'
</code></pre></div></div>

<ul>
  <li>Interesting fact : convergence proof has never been published</li>
  <li>has been stated informally that it will converge of policy converges to greedy</li>
  <li>we can achieve this by seeing ε = 1/t</li>
  <li>Or ε = c/t or ε= c/$t^a$</li>
  <li>Hyperparameters</li>
</ul>

<h3 id="learning-rate">Learning Rate</h3>
<ul>
  <li>Recall that learning rate can also decay</li>
  <li>problem:
    <ul>
      <li>if we set α = 1/t</li>
      <li>at every iteration, only one (s,a) pair will be updated for Q(s,a)</li>
      <li>Learning rate will decay even for values we have never updated</li>
      <li>Could we just only decrease α after every episode?</li>
      <li>No</li>
      <li>Many elements of Q(s,a) are not updated during an episode</li>
    </ul>
  </li>
  <li>we take inspiration from deep learning: AdaGrad and RMSprop(adaptive learning rates)</li>
  <li>Effective Learning rate Decays more when previous gradient has been larger</li>
  <li>in other words: the more it has changed in the past, the less it will change in the future</li>
  <li>our version is simpler: keep a count of every(s,a) pair seen:
    <ul>
      <li>α(s,a) = $α_0$ / count(s,a)</li>
    </ul>
  </li>
  <li>Equivalently
    <ul>
      <li>α(s,a) = $α_0$ / (k+m*count(s,a))</li>
    </ul>
  </li>
</ul>

<h2 id="5-q-learning">5. Q-Learning</h2>
<ul>
  <li>Main Theme: Generalize policy iteration
    <ul>
      <li>Policy evaluation</li>
      <li>Policy Improvement(greedy wrt(with regard to) current value)</li>
    </ul>
  </li>
  <li>what we’ve been studying: <strong>on-policy</strong> methods</li>
  <li>we always follow the current best policy</li>
  <li>Q-Learning is an <strong>off-policy</strong> methds</li>
  <li>do any random action, and still find Q*</li>
  <li>Looks similar SARSA
<a href="https://postimg.cc/dZ3DB1cp"><img src="https://i.postimg.cc/C5HDwBPh/41412131231232.png" width="500px" title="source:imgur.com" /><a></a></a></li>
  <li>instead of choosing a’ based on argmax of Q, we update Q(s,a) directly with max over Q(s’,a’)</li>
  <li>isn’t that the same, since a’=argmax[a’]{Q(s’,a’)}?
<a href="https://postimg.cc/hQJPqywD"><img src="https://i.postimg.cc/nr2sBNGD/41241241231231.png" width="500px" title="source:imgur.com" /><a></a></a></li>
  <li>we don’t need to actually do action a’ as the next move</li>
  <li>therefore, we use Q(s’,a’) in the update for Q(s,a), even if we don’t do a’ next</li>
  <li>Doesn’t matter what policy we follow</li>
  <li>Reality: Random actions -&gt; suboptimal -&gt; takes longer for episode to finish</li>
</ul>
:ET