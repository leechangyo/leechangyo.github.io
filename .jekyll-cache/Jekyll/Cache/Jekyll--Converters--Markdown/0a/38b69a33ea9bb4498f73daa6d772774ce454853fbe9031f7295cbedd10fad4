I"N
<h1 id="montel-carlomc-methods-introduction">Montel Carlo(MC) Methods Introduction</h1>
<ul>
  <li>Last Section: Dynamic Programming</li>
  <li>would that work for self-driving cars or video games?</li>
  <li>can i just set the state of the agent?</li>
  <li>“god-mode” capabilities?</li>
  <li><strong>MC Methods learn purely from experience</strong></li>
  <li>Montel Carlo usually refers to any method with a significant random component</li>
  <li>Random Component in RL is the return</li>
  <li>With MC, instead of calculating the true expected value of G, <strong>we calculate its sample mean</strong>
<a href="https://postimg.cc/6yrZL1Rr"><img src="https://i.postimg.cc/c4jcWpLb/22222.png" width="500px" title="source: imgur.com" /></a></li>
  <li>Need to assume episode tasks only</li>
  <li>Episode must terminate before we calculate return</li>
  <li>Not “fully” online since we need to wait for entire episode to finish before updating</li>
  <li>(full online mean is update after every action)</li>
  <li>monte carlo methods is not fully online which mean it is updated after episode to finish</li>
  <li>Should Remind you of multi-armed bandit</li>
  <li><strong>Multi-Armed bandit : average reward after every action</strong></li>
  <li>MDPs: average the return</li>
  <li>One way to think of MC-MDP is every state is a separate multi-armed bandit problem</li>
  <li>Follow the same pattern</li>
  <li>Prediction Problem(Finding Value given policy)</li>
  <li>Control Problem(finding optimal policy)</li>
</ul>

<h2 id="1-monte-carlo-for-prediction-problem">1. Monte Carlo for prediction Problem</h2>
<ul>
  <li>Recall what V(s) is :
 <a href="https://postimg.cc/t1yqhpWb"><img src="https://i.postimg.cc/hGQXRGGX/4124131233.png" width="500px" title="source: imgur.com" /></a></li>
  <li>Any expected value can be approximated like
<a href="https://postimg.cc/rzYYWjxF"><img src="https://i.postimg.cc/dtqMbS0d/41241231321312.png" width="500px" title="source: imgur.com" /></a></li>
  <li>“i” is episode, “t” is steps</li>
</ul>

<h3 id="how-do-we-generate-g">How do we generate G?</h3>
<ul>
  <li>just play a bunch of episode, log the states and reward sequences
    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>s1 s2 s3 ... sT
r1 r2 r3 ... rT
</code></pre></div>    </div>
  </li>
  <li>
    <p>Calculate G from Definition:
G(t) = r(t+1)+gamma*G(t+1)</p>
  </li>
  <li>very helpful to calculate G by iterating through states in reverse order</li>
  <li>Once we have(s,G) pairs, average them for each s</li>
</ul>

<h3 id="multiple-visits-to-s">Multiple Visits to s</h3>
:ET