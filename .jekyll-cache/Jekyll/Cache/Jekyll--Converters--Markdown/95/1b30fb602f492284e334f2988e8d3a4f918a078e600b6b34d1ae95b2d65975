I"—<h1 id="temporal-difference-td-learning">Temporal Difference (TD) Learning</h1>
<ul>
  <li>this section is a 3rd technique for solving MDPs</li>
  <li>TD = Temporal Difference(TD) Learning</li>
  <li>Combines ideas from DP and MC</li>
  <li>Disadvantage of DP: requires full model of environment, never learns from experience</li>
  <li>MC and TD learn from experience</li>
  <li>MC can only update after completing episode, but DP uses Bootstrapping(Making an initial estimate)</li>
  <li>We will see that TD also uses Bootstrapping and is fully online, can update value during an episode
    <h2 id="1-td-learning">1. TD Learning</h2>
  </li>
  <li>Same approach as before</li>
  <li>First Predict Problem</li>
  <li>them control problem</li>
  <li>2 control methods:
    <ul>
      <li>SARSA</li>
      <li>Q-Learning</li>
    </ul>
  </li>
  <li>Model-Free Reinforcement Learning</li>
  <li>TD methods learn directly from episode of experience.</li>
  <li>no Knowledge of MDP Transition / rewards</li>
  <li>TD learns from incomplete episodes by Bootstrapping</li>
  <li>TD updates a guess towards a guess
<a href="https://postimg.cc/fkPHCsWG"><img src="https://i.postimg.cc/28kPzjsS/41231312313.png" width="500px" title="source: imgur.com" /><a></a></a></li>
  <li>MCì™€ ë‹¤ë¥¸ì ì€, MCëŠ” ì‹¤ì œ ì—í”¼ì†Œë“œê°€ ëë‚˜ê³  ë°›ê²Œ ë˜ëŠ” ë³´ìƒì„ ì‚¬ìš©í•˜ì—¬ Value Functionì„ ì—…ë°ì´íŠ¸ í•˜ì˜€ë‹¤</li>
  <li>TDì—ì„œëŠ” ì‹¤ì œ ë³´ìƒê³¼ ë‹¤ìŒ stepì— ëŒ€í•œ ë¯¸ë˜ì¶”ì •ê°€ì¹˜ë¥¼ ì‚¬ìš©í•´ì„œ í•™ìŠµí•œë‹¤.</li>
  <li>ì´ë•Œ ì‚¬ìš©í•˜ëŠ” ë³´ìƒê³¼ Value Functionì˜ í•©ì„ TD Target
-ê·¸ë¦¬ê³  TD Target ê³¼ ì‹¤ì œ V(S)ì™€ì˜ ì°¨ì´ë¥¼ TD errorë¼ê³  í‘œí˜„í•œë‹¤.</li>
</ul>
:ET