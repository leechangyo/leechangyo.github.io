I"''<h1 id="montel-carlomc-methods-introduction">Montel Carlo(MC) Methods Introduction</h1>
<ul>
  <li>Last Section: Dynamic Programming</li>
  <li>would that work for self-driving cars or video games?</li>
  <li>can i just set the state of the agent?</li>
  <li>“god-mode” capabilities?</li>
  <li><strong>MC Methods learn purely from experience</strong></li>
  <li>Montel Carlo usually refers to any method with a significant random component</li>
  <li>Random Component in RL is the return</li>
  <li>With MC, instead of calculating the true expected value of G, <strong>we calculate its sample mean</strong>
<a href="https://postimg.cc/6yrZL1Rr"><img src="https://i.postimg.cc/c4jcWpLb/22222.png" width="500px" title="source: imgur.com" /></a></li>
  <li>Need to assume episode tasks only</li>
  <li>Episode must terminate before we calculate return</li>
  <li>Not “fully” online since we need to wait for entire episode to finish before updating</li>
  <li>(full online mean is update after every action)</li>
  <li>monte carlo methods is not fully online which mean it is updated after episode to finish</li>
  <li>Should Remind you of multi-armed bandit</li>
  <li><strong>Multi-Armed bandit : average reward after every action</strong></li>
  <li>MDPs: average the return</li>
  <li>One way to think of MC-MDP is every state is a separate multi-armed bandit problem</li>
  <li>Follow the same pattern</li>
  <li>Prediction Problem(Finding Value given policy)</li>
  <li>Control Problem(finding optimal policy)</li>
</ul>

<h2 id="1-monte-carlo-for-prediction-problem">1. Monte Carlo for prediction Problem</h2>
<ul>
  <li>Recall what V(s) is :
 <a href="https://postimg.cc/t1yqhpWb"><img src="https://i.postimg.cc/hGQXRGGX/4124131233.png" width="500px" title="source: imgur.com" /></a></li>
  <li>Any expected value can be approximated like
<a href="https://postimg.cc/rzYYWjxF"><img src="https://i.postimg.cc/dtqMbS0d/41241231321312.png" width="500px" title="source: imgur.com" /></a></li>
  <li>“i” is episode, “t” is steps</li>
</ul>

<h3 id="how-do-we-generate-g">How do we generate G?</h3>
<ul>
  <li>just play a bunch of episode, log the states and reward sequences
    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>s1 s2 s3 ... sT
r1 r2 r3 ... rT
</code></pre></div>    </div>
  </li>
  <li>
    <p>Calculate G from Definition:
G(t) = r(t+1)+gamma*G(t+1)</p>
  </li>
  <li>very helpful to calculate G by iterating through states in reverse order</li>
  <li>Once we have(s,G) pairs, average them for each s</li>
</ul>

<h3 id="multiple-visits-to-s">Multiple Visits to s</h3>
<ul>
  <li>what if we see the same state more than once in an episode</li>
  <li>E.g. we see s at t=1 and t=3</li>
  <li>which return should we use? G(1) or G(3</li>
  <li>First-visit method :
    <ul>
      <li>Use t = 1 only</li>
    </ul>
  </li>
  <li>Every-visit method :
    <ul>
      <li>Use both t=1 and t=3 as samples</li>
    </ul>
  </li>
  <li>Surprisingly, it has been proven that both lead to same answer</li>
</ul>

<blockquote>
  <p>First-Visit MC Pseudocode</p>
</blockquote>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def first_visit_monte_carlo_prediction(π, N):
  V = random initialization
  all_return = {} # default = []
  do N times:
    states, returns = play_episode
    for s, g in zip(states, returns):
      if not seen s in this episode yet:
        all_return[s].append(g)
        V(s) = sample_mean(all_returns[s])
  return V
</code></pre></div></div>

<h3 id="sample-mean">Sample Mean</h3>
<ul>
  <li>Notice how we store all returns in a list</li>
  <li>Didn’t we discuss how that’s inefficient?</li>
  <li>Can also use previous mean to calculate current mean</li>
  <li>Can also use moving average for non-stationary problems</li>
  <li>Everything we learned before still applies
<a href="https://postimg.cc/6yrZL1Rr"><img src="https://i.postimg.cc/c4jcWpLb/22222.png" width="500px" title="source: imgur.com" /></a></li>
  <li>Rules of probability still apply</li>
  <li><a href="https://ko.wikipedia.org/wiki/%EC%A4%91%EC%8B%AC_%EA%B7%B9%ED%95%9C_%EC%A0%95%EB%A6%AC">Central limit Theorem</a></li>
  <li>Variance of estimate = Variance of RV(Random Value) / N</li>
</ul>

<blockquote>
  <p>Calculating Returns from Rewards</p>
</blockquote>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>s = grid.current_state()
states_and_rewards = [(s,0)]
while not game_over:
  a = policy(s)
  r = grid.move(a)
  s = grid.current_state()
  states_and_rewards.append((s,r))

G = 0
states_and_returns = []
for s,r in reverse(states_and_rewards):
  states_and_returns.((s,G))
  G = r + gamma*G
states_and_returns.reverse
</code></pre></div></div>

<h3 id="mc">MC</h3>
<ul>
  <li>Recall: one Disadvantage of DP is that <strong>we need to loop through all states</strong></li>
  <li>MC: only update V for visited states</li>
  <li>We don’t even need to know what all the states are, we can just discover them as we play</li>
</ul>

<h2 id="2-mc-for-windy-gridworld">2. MC for Windy Gridworld</h2>
<ul>
  <li>Windy Gridworld, different policy</li>
  <li>Policy/transitions were deterministic, MC not really needed</li>
  <li>in windy gridworld, p(s’r I s,a) not deterministic</li>
  <li>with this policy, we try to get to goal</li>
  <li>Values can be -ve, if overall wind pushes us to losing state more often</li>
</ul>

<blockquote>
  <p>code</p>
</blockquote>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># https://deeplearningcourses.com/c/artificial-intelligence-reinforcement-learning-in-python
# https://www.udemy.com/artificial-intelligence-reinforcement-learning-in-python
from __future__ import print_function, division
from builtins import range
# Note: you may need to update your version of future
# sudo pip install -U future


import numpy as np
from grid_world import standard_grid, negative_grid
from iterative_policy_evaluation import print_values, print_policy

SMALL_ENOUGH = 1e-3
GAMMA = 0.9
ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')

# NOTE: this is only policy evaluation, not optimization

def random_action(a):
  # choose given a with probability 0.5
  # choose some other a' != a with probability 0.5/3
  p = np.random.random()
  if p &lt; 0.5:
    return a
  else:
    tmp = list(ALL_POSSIBLE_ACTIONS)
    tmp.remove(a)
    return np.random.choice(tmp)

def play_game(grid, policy):
  # returns a list of states and corresponding returns

  # reset game to start at a random position
  # we need to do this, because given our current deterministic policy
  # we would never end up at certain states, but we still want to measure their value
  start_states = list(grid.actions.keys())
  start_idx = np.random.choice(len(start_states))
  grid.set_state(start_states[start_idx])
# random start

  s = grid.current_state()
  states_and_rewards = [(s, 0)] # list of tuples of (state, reward)
  while not grid.game_over():
    a = policy[s]
    a = random_action(a)
    r = grid.move(a)
    s = grid.current_state()
    states_and_rewards.append((s, r))
  # calculate the returns by working backwards from the terminal state
  G = 0
  states_and_returns = []
  first = True
  for s, r in reversed(states_and_rewards):
    # the value of the terminal state is 0 by definition
    # we should ignore the first state we encounter
    # and ignore the last G, which is meaningless since it doesn't correspond to any move
    if first:
      first = False
    else:
      states_and_returns.append((s, G))
    G = r + GAMMA*G
  states_and_returns.reverse() # we want it to be in order of state visited
  return states_and_returns


if __name__ == '__main__':
  # use the standard grid again (0 for every step) so that we can compare
  # to iterative policy evaluation
  grid = standard_grid()

  # print rewards
  print("rewards:")
  print_values(grid.rewards, grid)

  # state -&gt; action
  # found by policy_iteration_random on standard_grid
  # MC method won't get exactly this, but should be close
  # values:
  # ---------------------------
  #  0.43|  0.56|  0.72|  0.00|
  # ---------------------------
  #  0.33|  0.00|  0.21|  0.00|
  # ---------------------------
  #  0.25|  0.18|  0.11| -0.17|
  # policy:
  # ---------------------------
  #   R  |   R  |   R  |      |
  # ---------------------------
  #   U  |      |   U  |      |
  # ---------------------------
  #   U  |   L  |   U  |   L  |
  policy = {
    (2, 0): 'U',
    (1, 0): 'U',
    (0, 0): 'R',
    (0, 1): 'R',
    (0, 2): 'R',
    (1, 2): 'U',
    (2, 1): 'L',
    (2, 2): 'U',
    (2, 3): 'L',
  }

  # initialize V(s) and returns
  V = {}
  returns = {} # dictionary of state -&gt; list of returns we've received
  states = grid.all_states()
  for s in states:
    if s in grid.actions:
      returns[s] = []
    # 빈값을 넣어는다 initializize
    else:
      # terminal state or state we can't otherwise get to
      V[s] = 0

  # repeat until convergence
# 5000 iterative
  for t in range(5000):

    # generate an episode using pi
    states_and_returns = play_game(grid, policy)
    seen_states = set()
    for s, G in states_and_returns:
      # check if we have already seen s
      # called "first-visit" MC policy evaluation
      if s not in seen_states:
        returns[s].append(G)
        V[s] = np.mean(returns[s])
        seen_states.add(s)

  print("values:")
  print_values(V, grid)
  print("policy:")
  print_policy(policy, grid)
</code></pre></div></div>

<h2 id="2-mc-for-control-problem">2. MC for Control Problem</h2>
<ul>
  <li>Let’s now move on to control problem</li>
  <li>Can we use MC?</li>
  <li>Problem: we only have V(s) for a given policy, we don’t know what actions will lead to better V(s) because   we can’t do look-ahead search</li>
  <li>only play episode and get states/returns</li>
  <li>
    <h2 id="key-is-to-use-qsa"><strong>key is to use Q(s,a)</strong></h2>
    <p>-</p>
  </li>
</ul>
:ET