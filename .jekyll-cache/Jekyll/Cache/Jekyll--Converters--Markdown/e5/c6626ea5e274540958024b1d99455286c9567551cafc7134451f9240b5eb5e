I"æ"<h1 id="temporal-difference-td-learning">Temporal Difference (TD) Learning</h1>
<ul>
  <li>this section is a 3rd technique for solving MDPs</li>
  <li>TD = Temporal Difference(TD) Learning</li>
  <li>Combines ideas from DP and MC</li>
  <li>Disadvantage of DP: requires full model of environment, never learns from experience</li>
  <li>MC and TD learn from experience</li>
  <li>MC can only update after completing episode, but DP uses Bootstrapping(Making an initial estimate)</li>
  <li>We will see that TD also uses Bootstrapping and is fully online, can update value during an episode
    <h2 id="1-td-learning">1. TD Learning</h2>
  </li>
  <li>Same approach as before</li>
  <li>First Predict Problem</li>
  <li>them control problem</li>
  <li>2 control methods:
    <ul>
      <li>SARSA</li>
      <li>Q-Learning</li>
    </ul>
  </li>
  <li>Model-Free Reinforcement Learning</li>
  <li>TD methods learn directly from episode of experience.</li>
  <li>no Knowledge of MDP Transition / rewards</li>
  <li>TD learns from incomplete episodes by Bootstrapping</li>
  <li>TD updates a guess towards a guess
<a href="https://postimg.cc/fkPHCsWG"><img src="https://i.postimg.cc/28kPzjsS/41231312313.png" width="500px" title="source: imgur.com" /><a></a></a></li>
  <li>MCì™€ ë‹¤ë¥¸ì ì€, MCëŠ” ì‹¤ì œ ì—í”¼ì†Œë“œê°€ ëë‚˜ê³  ë°›ê²Œ ë˜ëŠ” ë³´ìƒì„ ì‚¬ìš©í•˜ì—¬ Value Functionì„ ì—…ë°ì´íŠ¸ í•˜ì˜€ë‹¤</li>
  <li>TDì—ì„œëŠ” ì‹¤ì œ ë³´ìƒê³¼ ë‹¤ìŒ stepì— ëŒ€í•œ ë¯¸ë˜ì¶”ì •ê°€ì¹˜ë¥¼ ì‚¬ìš©í•´ì„œ í•™ìŠµí•œë‹¤.</li>
  <li>ì´ë•Œ ì‚¬ìš©í•˜ëŠ” ë³´ìƒê³¼ Value Functionì˜ í•©ì„ TD Target
-ê·¸ë¦¬ê³  TD Target ê³¼ ì‹¤ì œ V(S)ì™€ì˜ ì°¨ì´ë¥¼ TD errorë¼ê³  í‘œí˜„í•œë‹¤.
<a href="https://postimg.cc/FfJFfNRh"><img src="https://i.postimg.cc/TPtK4Pv5/14141241313.png" width="500px" title="source: imgur.com" /><a></a></a></li>
  <li>MCì—ì„œì˜ Value functionì´ ì—…ë°ì´íŠ¸ ë˜ëŠ” ê³¼ì •ì´ ì™¼ìª½(ì—í”¼ì†Œë“œê°€ ì „ì²´ì ìœ¼ë¡œ ëë‚˜ì„œ ê·¸ì˜ ë³´ìƒì„ ë‚˜ëˆ„ì–´ ë‹¨ê³„ë³„ë¡œ ì—…ë°ì´íŠ¸)</li>
  <li>TDëŠ” ê° ë‹¨ê³„ë³„ë¡œ ì—…ë°ì´íŠ¸ê°€ ë˜ëŠ” ê³¼ì •ìœ¼ë¡œ ì˜¤ë¥¸ìª½ ê·¸ë¦¼</li>
  <li>TDì˜ ì¥ì ì€ ì—í”¼ì†Œë“œ ì¤‘ê°„ì—ì„œë„ í•™ìŠµì„ í•˜ê²Œ ëœë‹¤ëŠ” ê²ƒì´ë‹¤.</li>
  <li>MCì—ì„œëŠ” ì—í”¼ì†Œë“œê°€ ëë‚ ë•Œê¹Œì§€ ê¸°ë‹¤ë ¸ë‹¤ê°€ ì—…ë°ì´íŠ¸ê°€ ë°œìƒí•˜ê³  í•™ìŠµí•˜ê¸° ë–„ë¬¸ì´ë‹¤.</li>
  <li>TDëŠ” ì¢…ë£Œ ì—†ëŠ” ì—°ì†ì ì¸ ì—í”¼ì†Œë“œì—ì„œë„ í•™ìŠµí•  ìˆ˜ ìˆë‹¤.</li>
  <li>Return $G_t$ = R(t+1)+É£R(t+2)+â€¦+É£^(t-1)$R_T) is unbiased estimate of $v_Ï€$($S_t$)</li>
  <li>True TD Target  R(t+1)+É£V(S(t+1)) is biased estimate of $v_Ï€$($S_t$)</li>
  <li>TD Target is much lower variance than the return:
    <ul>
      <li>return depends on many random actions, transitions, rewards</li>
      <li>TD Target depends on one random action, transition, reward</li>
    </ul>
  </li>
  <li>V policyê°€ ì‹¤ì œ Gì— ëŒ€í•´ì„œ unbiasë¼ í• ë•ŒëŠ” TD Targetë„ V policyë¥¼ ì¶”ì¢…í•˜ê¸° unbiasì´ë‹¤. í•˜ì§€ë§Œ TD Targetì— V policyë¥¼ ì¶”ì •í•˜ëŠ” V(St+1)ë¥¼ ì‚¬ìš©í•˜ê¸°ì—  ì‹¤ì œê°’ì´ ì•„ë‹ˆë¼ ì‹¤ì œê°’ì„ ì¶”ì •í•˜ëŠ” ê°’ì„ìœ¼ë¡œ biasê°€ ë°œìƒí•œë‹¤. ê·¸ë¦¬ê³  TD Targetì€ ë‹¨ì§€ í•˜ë‚˜ì˜ stepì—ì„œë§Œ ê³„ì‚°í•˜ê¸°ì— noiseê°€ ì‘ê²Œ ë˜ë¯€ë¡œ ìƒëŒ€ì ìœ¼ë¡œ varianceê°€ ë‚®ê²Œ ë‚˜íƒ€ë‚œë‹¤.</li>
  <li>MC has high variance, zero bias
    <ul>
      <li>good convergence properties</li>
      <li>even with function approximation</li>
      <li>not very sensitive to initial value</li>
      <li>very simple to understand and use</li>
    </ul>
  </li>
  <li>TD has low variance, some bias
    <ul>
      <li>Usually more efficient than MC</li>
      <li>TD(0) converges to $v_Ï€$(s)</li>
      <li>but not always with function approximation</li>
      <li>More sensitive to initial value</li>
    </ul>
  </li>
  <li>Compare on variance between MC and TD
<a href="https://postimg.cc/hhyS0pYJ"><img src="https://i.postimg.cc/L6Rn2WBV/41241313123.png" width="500px" title="source: imgur.com" /><a></a></a></li>
  <li>Bootstrappingì´ ë” í•™ìŠµí•˜ëŠ”ë° íš¨ìœ¨ì ì´ë‹¤.</li>
  <li>MC and TD converge : V(s) -&gt; $v_Ï€$(s) as experience -&gt; âˆ (ì—í”¼ì†Œë“œëŠ” ë¬´í•œ ë°˜ë³µí•˜ê²Œ ë˜ë©´ ê²°êµ­ ìˆ˜ë ´í•˜ê²Œ ë˜ì–´ìˆë‹¤.)</li>
  <li>but what about batch solution for finite experience
<a href="https://postimg.cc/RqDW0S4b"><img src="https://i.postimg.cc/QxX10CPs/41241312312312312312.png" width="300px" title="source: imgur.com" /><a></a></a></li>
  <li>e.g repeatedly sample episode k âˆˆ [1,K]</li>
  <li>Apply MC or TD(0) to episode k
<a href="https://postimg.cc/VrwVnPwx"><img src="https://i.postimg.cc/wvRYK9Gj/3121.png" width="500px" title="source: imgur.com" /><a></a></a></li>
  <li>For example, first episode A got reward â€œ0â€ and B got reward â€œ0â€</li>
  <li>from Second Episode B got reward â€œ1â€ to seven episode. and B got 0 at the last episode.</li>
  <li>then A will go B within 100 %  and then reward will be â€œ0â€</li>
  <li>in the MC methods, A will get reward â€œ0â€ because the episode pass Through A is one that final reward is zero
6- in the TD methods, running another episode because of B value update to â€œ6â€ then A value also going to be updated</li>
  <li>MC converges to solution with minimum mean-square
    <ul>
      <li>best fit to the observe returns :
<a href="https://postimg.cc/bSwDGrBs"><img src="https://i.postimg.cc/4yzzShvb/412312123123.png" width="300px" title="source: imgur.com" /><a></a></a></li>
      <li>in the AB example, V(A)=0</li>
    </ul>
  </li>
  <li>TD(0) converges to solution of max likelihood Markov Model
    <ul>
      <li>Solution to the MDP (S,A,P^,R^,É£) that best fits the data
<a href="https://postimg.cc/zytd3H2R"><img src="https://i.postimg.cc/QMsv4ppS/12412412312312.png" width="500px" title="source: imgur.com" /><a></a></a></li>
      <li>in the AB example, V(A) = 0.75</li>
    </ul>
  </li>
  <li>TD(0)ë°©ì‹ì€ max likelihood Markv ë°©ì‹ì„ ì‚¬ìš©í•˜ì—¬ ìˆ˜ë ´í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì´ë‹¤. MDP ê¸°ë°˜ìœ¼ë¡œ ì‹¤ì œì ì¸ ê°’ì„ ì°¾ì•„ê°€ê²Œ ë˜ê¸° ë•Œë¬¸ì— V(A)ì˜ ê°’ì´ 6/8 í‰ê· ê°€ì¹˜ê°€ ê³„ì‚°ë˜ì–´ 0.75ê°’ìœ¼ë¡œ ì—…ë°ì´íŠ¸ê°€ ëœë‹¤.</li>
  <li>TD exploits Markov property
    <ul>
      <li>Usually more efficient in Markov environment</li>
    </ul>
  </li>
  <li>MC does not exploit Markov property
    <ul>
      <li>Usually more effective in Non-Markov environment</li>
    </ul>
  </li>
  <li>MC ì•Œê³ ë¥´ì§
<a href="https://postimg.cc/RNjYTHL5"><img src="https://i.postimg.cc/cLHN07KH/12412312312.png" width="500px" title="source: imgur.com" /><a></a></a></li>
  <li>Bootstrappingì„ ì‚¬ìš©í•˜ì—¬ statesì— ëŒ€í•œ valueë“¤ì„ ì¶”ì •í•˜ì—¬ ì‚¬ìš©í•˜ëŠ” ë°©ë²• TD
<a href="https://postimg.cc/MMVHvR56"><img src="https://i.postimg.cc/B6wK7B1K/1414131312.png" width="500px" title="source: imgur.com" /><a></a></a></li>
  <li>DP ë°©ì‹
<a href="https://postimg.cc/RqGJkkPX"><img src="https://i.postimg.cc/3JQjv725/41231231231231.png" width="500px" title="source: imgur.com" /><a></a></a></li>
  <li>DP ë°©ì‹ì—ì„œëŠ” ëª¨ë¸ì„ í†µí•´ì„œ ì´ë¯¸ MDPë¥¼ ì•Œê³  ìˆê³  ì´ì— ëŒ€í•œ value ì™€ rewardë¥¼ í†µí•´ í•™ìŠµì„ í•˜ê¸° ë•Œë¬¸ì— ìœ„ì—ì™€ ê°™ì´ ë‚˜ì˜µë‹ˆë‹¤.</li>
  <li>Bootstrapping: update involves an estimate
    <ul>
      <li>MC does not bootstrap</li>
      <li>DP bootstraps</li>
      <li>TD bootstrap</li>
    </ul>
  </li>
  <li>Sampling update samples an expectation
    <ul>
      <li>MC samples</li>
      <li>DP does not sample</li>
      <li>TD samples</li>
    </ul>
  </li>
  <li>DPì™€ TPì—ì„œ ì‚¬ìš©í•˜ëŠ” Bootstrappingì€ ì¶”ì • ê°’ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ì—¬ ì—…ë°ì´íŠ¸</li>
  <li>MCì—ì„œ ì‚¬ìš©í•˜ëŠ” ìƒ˜í”Œë§ì€ expectationì„ ìƒ˜í”Œí•˜ì—¬ ì—…ë°ì´íŠ¸ í•œë‹¤.</li>
  <li>TDë„ ìƒ˜í”Œë§ì„ í•œì§€ë§Œ DPì²˜ëŸ¼ full backupì„ í•˜ì§€ ì•ŠëŠ”ë‹¤.
<a href="https://postimg.cc/RqbK6rcq"><img src="https://i.postimg.cc/WbsnyTzm/4123131231231.png" width="500px" title="source: imgur.com" /><a></a></a></li>
</ul>

<h2 id="2-td0-prediction">2. TD(0) Prediction</h2>
<ul>
  <li>Apply TD to prediction problem</li>
  <li>algorithm is called TD(0)</li>
  <li>there is also TD(1) and TD(Î»)</li>
  <li>it is related to Q-Learning and approximation methods</li>
</ul>

<h3 id="mc">MC</h3>
<ul>
  <li>Recall: one Disadvantage of MC is we need to wait until the episode is finished then we calculate return</li>
  <li>also recall: Multiple ways of calculating averages</li>
  <li>General â€œaverage-findingâ€ equation</li>
  <li>Does not require us to store all returns</li>
  <li>Constant alpha is moving average/exponential decay
<a href="https://postimg.cc/B8LQVyj7"><img src="https://i.postimg.cc/cJTgrG0W/41312312312.png" width="500px" title="source: imgur.com" /><a></a></a></li>
</ul>

<h3 id="value-function">Value function</h3>
<ul>
  <li>now recall: the definition of V</li>
  <li>We can define it recursively
<a href="https://postimg.cc/B8LQVyj7"><img src="https://i.postimg.cc/cJTgrG0W/41312312312.png" width="500px" title="source: imgur.com" /><a></a></a></li>
</ul>
:ET