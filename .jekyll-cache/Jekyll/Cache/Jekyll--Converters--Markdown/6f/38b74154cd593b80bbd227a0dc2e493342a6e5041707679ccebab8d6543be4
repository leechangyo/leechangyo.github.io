I"%<h1 id="n-step-methods">N-step Methods</h1>
<ul>
  <li>N-step Methods : Further our understanding of TD methods</li>
  <li>we know so far： TD(0)</li>
  <li>we will learn :
    <ul>
      <li>λ = 0 gives us TD(0), λ = 1 gives us monte Carlo</li>
    </ul>
  </li>
  <li>any other λ is a trade-off(协调) between the two
<a href="https://postimg.cc/n9hf3sZq"><img src="https://i.postimg.cc/gjvckRTt/Capture.png" width="700px" title="source: imgur.com" /><a></a></a></li>
  <li>1-Step TD의 step을 증가시켜 나가면서 n까지 보게 된다면 n-step TD로 일반화 할 수 있다.</li>
  <li>만약 step이 무한대에 가깝게 되면 MC와 동일하게 될 것이다.</li>
  <li>2-Step TD에서 업데이트 방식은 척번째 보상과 두번째 보상 그리고 부전째 상태에서의 Value function의 합으로 업데이트가 된다.
```
Reference:</li>
</ul>

<p><a href="https://www.udemy.com/course/artificial-intelligence-reinforcement-learning-in-python/">Artificial Intelligence Reinforcement Learning</a></p>

<p><a href="https://www.udemy.com/deep-reinforcement-learning-in-python/">Advance AI : Deep-Reinforcement Learning</a></p>

<p><a href="https://www.udemy.com/cutting-edge-artificial-intelligence/learn/lecture/14650508#overview">Cutting-Edge Deep-Reinforcement Learning</a></p>
:ET